























 

 
 







 

bibliography.bib 

 















	paper=a4paper, 
	inner=2.5cm, 
	outer=3.8cm, 
	bindingoffset=.5cm, 
	top=1.5cm, 
	bottom=1.5cm, 
	






Deep Learning on Graphs with Applications to the Life Sciences 
Dr. Davide Bacciu
Prof. Alessio Micheli 
 
Doctor of Philosophy 
Marco Podda 
 

Computer Science 
 
http://www.university.comUniversity of Pisa 
http://department.university.comDepartment of Computer Science 
http://researchgroup.university.comComputational Intelligence and Machine Learning Group 
http://faculty.university.comFaculty Name 


pdftitle= 
pdfauthor= 
pdfkeywords= 





theoremTheorem
corollaryCorollary
lemmaLemma

definition
definitionDefinition[chapter]





















 








*.06
 
Doctoral Thesis
 


 
 

 

[t]0.4
 
Author:

 


[t]0.4
 
Supervisor: 

 






A thesis submitted in fulfillment of the requirements
 for the degree of 
 
in the



 




 















































 
The application of Deep Learning models to complex biological problems has recently revolutionized the field of life sciences, leading to advancements that could potentially change the quality and length of human life for the better. A major contribution to this success comes from the ability of deep neural networks to well approximate non-linear biological functions, as well as the flexibility to learn from structured biological data directly. This thesis presents two relevant applications where Deep Learning is used on biological graphs to learn complex tasks. The first concerns the prediction of dynamical properties of chemical reactions at the cellular level, represented as Petri graphs. Our contribution is a Deep Learning model that can learn the task solely relying on the structure of such graphs, which is orders of magnitude faster than running expensive simulations. The second application is about accelerating the drug design process by discovering novel drug candidates. We present a deep and generative framework in which novel graphs, corresponding to molecules with desired characteristics, can be obtained by combining chemically meaningful molecular fragments. Our work suggests that coupling the power of Deep Learning with the ability to handle structured data remains one of the preferred avenues to pursue towards solving well-known biological problems, as well as an effective methodology to tackle new ones.





























[type=,style=list]
















































 






Introduction
One of the long-standing goals of humanity is to understand the mechanisms that underlie biological life. The journey towards this end objective requires to solve several fascinating sub-problems: how do biological entities work at the cell level? Which processes determine a certain phenotype? How can we cure diseases? These broad questions require dedicated scientific studies, which necessarily entail analyzing large quantities of data to model complex phenomena. In this sense, the latest technological advancements in the collection, storage, and processing of biological information have brought forth an unparalleled progress. At the same time, an equal effort has been dedicated to the development of computational methodologies that learn difficult biological tasks by extracting useful knowledge from these sheer amounts of data. Among the many, Deep Learning with neural networks lecun2015naturedeeplearning has been undoubtedly one of the driving forces. As we write this thesis, a Deep Learning model called AlphaFold has made a giant leap towards realiably predicting the structure of a protein from its sequence of aminoacids -- the so-called protein folding problem senior2020alphafold. With Deep Learning, we can now predict quantum properties of chemical structures within chemical accuracy, being 300k times faster than numerical simulations gilmer2017neuralmessagepassing. A Deep Learning method called DeepVariant can reconstruct the true genome sequence from High-Throughput Sequencing data with significantly greater accuracy than previous classical methods poplin2018deepvariant. Lastly, Deep Learning has been successfully applied to counter antibiotic resistence, by finding antibiotics that are chemically different from known ones, but have the same bactericidal activity stokes2020deeplearningantibiotic. All these life sciences problems -- and most life sciences problems more broadly -- share key commonalities. Perhaps the most apparent is that each of them concerns a phenomenon that is poorly characterized, meaning that the biological function that relates the data to the observed outcomes is only partially understood. In this case, Deep Learning is helpful because it shifts the burden of finding a suitable representation of the input data from the end user to the modeling machine bengio2014representationlearning. In other words, instead of hand-crafting the knowledge required to solve a task, Deep Learning models infer task-specific knowledge from the data themselves. Another commonality regards the nature of the biological datum. Indeed, most of biological data are structured, meaning that they can be decomposed into a set of entities, and a set of relations between them. Examples of structured data in life sciences are ubiquitous: proteins can be thought of as sequences of aminoacids. Gene ontologies, which describe gene attributes across a variety of species, are organized with hierarchical data structures. Molecules are naturally represented as graphs where atoms are vertices and chemical bonds are edges. Clearly, a biological datum taken as a whole has a more informative content than its constituents taken in isolation. On one hand, this superior expressiveness allows to devise more effective models of biological processes. On the other hand, working with structured data introduces additional challenges, such as that of representing them in a computationally efficient way, while also retaining their relational nature.

Traditional Machine Learning models are designed to work with unstructured, real-valued vectors. Thus, they cannot be applied off-the-self to problems where data have a complex structure. In principle, it is possible to circumvent this complexity by flattening the relational information into vectors of structural descriptors. While useful to some extent, this approach is built upon a somewhat under-specified representation of knowledge. Moreover, it heavily relies on domain expertise in order to choose what structural information need to be retained, and what need to be discarded. Conversely, Deep Learning methodologies allow to handle structured data directly and to extract knowledge in a goal-oriented way, without losing the relational information within them. This feature well combines with the powerful nature of neural networks, which are capable of approximating any complex function with arbitrary precision under mild assumptions cybenko1998approximationuniversal,hammer2005universal. Neural approaches for the processing of recursive structured data have been studied long before the advent of Deep Learning , and were generalized to arbitrary relational data while modern Deep Learning methods were still under development micheli2009nn4g,scarselli2009gnn. The result of these research efforts culminated in three classes of neural networks for structured data: Recurrent Neural Networks for sequence data, Recursive Neural Networks for tree/hierarchical data, and Deep Graph Networks for general graph data. Recently, with the availability of large data sources and the increase of computational power, these models have been constantly rediscovered and improved, attracting the attention of researchers coming from very different domains. Along with their increased usage, these models have demonstrated strong performances and great success when applied on very difficult biological tasks .

Objective and Contributions
The high-level objective of this thesis is to provide a broad perspective on the kinds of biological tasks that can be tackled with Deep Learning. We do so by focusing on two classes of problems: predictive and generative. Predictive problems deal with modeling an unknown input-output relationship of interest. Generative problems deal with learning the data distribution to sample novel instances. For each of the two classes of problems, we present two original contributions: one concerning methodological aspects, and another concerning an application to real-world biological problems. Following, we discuss our contributions in detail.

Methodological Contribution on Predictive Deep Learning on Graphs
We provide a systematic and fair evaluation of Deep Graph Networks for graph classification tasks. In the literature, the evaluation of these models is often biased by poor experimental setups and unfair comparisons. We develop a unified framework, under which these models can be evaluated coherently, and their results properly compared. This work lets us also shed a light on what Deep Graph Networks learn differently from structure-agnostic baselines, as well as on the relation between structural features of the graphs (specifically, the vertex degree) and model performances.

Applied Contribution in Computational Biology
We address a newly formulated predictive problem in the context of biological pathways. A biochemical pathway is a dynamical system in which molecules interact with each other through chemical reactions. Each molecule can take different roles in a biochemical pathway: it can be a reactant, a product of some chemical reaction, and it can act as inhibitor or promoter of other reactions. The state of the system  corresponds to some function being performed or not (for example, cell replication). Biochemical pathways can be conveniently represented as Petri networks, allowing researchers to study dynamical properties of the system such as reachability of steady states, causality between species, and robustness. Specifically, we focus on the property of robustness, which can be described informally as the resilience of the pathway to maintain its function against external perturbations. Robustness is usually assessed with extensive numerical simulations, which are usually expensive in terms of computational time. We present an application of Deep Graph Networks to predict robustness indicators on pathways represented as Petri nets, without resorting to costly simulations. The assumption at the basis of this work is that the structure of the pathway (and not other factors such as kinetic and chemical constants) is sufficient to be effective in this task. We show experimentally for the first time that this is indeed possible to a good extent. Furthermore, we study how different architectural choices of the Deep Graph Network influence performances.

Methodological Contribution on Generative Deep Learning on Graphs
Graph generation is a challenging problem with applications in various research fields. We first propose a general-purpose recurrent model to generate arbitrary unlabelled graphs, whose structure resembles that of the training sample. Despite its simplicity, we experimentally demonstrate its effectiveness on a wide range of graph datasets. The experimental results show that the model outperforms canonical baselines from graph theory, and perform comparably to the current state of the art on unlabelled graph generation.

Applied Contribution in Computational Chemistry
We study the molecular generation problem in the context of drug design. Currently, deep generative approaches on this task belong to two broad categories, differing in how molecules are represented. One approach is based on encoding molecular graphs as strings of text, and learning a character-based language model on such strings. Another, more expressive, approach works directly with the molecular graph. The work addresses two limitations of the former generative framework, namely the generation of invalid or duplicate molecules. To improve validity rates, we develop a language model for small molecular substructures called fragments, loosely inspired by the well-known paradigm of Fragment-Based Drug Design. The proposed architecture generates molecules fragment by fragment, instead of atom by atom. To improve uniqueness rates, we present a frequency-based masking strategy that helps generate molecules with infrequent fragments. We show experimentally that our model largely outperforms other language model-based competitors, reaching performances comparable to the state of the art at the task. Moreover, we show how the generated molecules display molecular properties similar to those in the training sample, even in absence of explicit task-specific supervision.

Structure of the Thesis
This thesis is divided in four parts. In Part , we present the necessary introductory notions to facilitate the understanding of the following content. In the first part of Chapter , we provide a brief overview of the concepts of Machine Learning that are functional to our discussion. The rest of the chapter is spent to discuss about the foundations of Neural Networks and Deep Learning for supervised and unsupervised tasks. In Chapter , we present the class of Deep Learning models for the processing of structured data, namely Recurrent Neural Networks for sequence data, Recursive Neural Networks for tree/hierarchical data, and Deep Graph Networks for graph data. In Part , we present our first two contributions in the context of predictive Deep Learning and its applications to computational biology. Specifically, in Chapter , we present a fair and rigorous evaluation of existing Deep Graph Networks; while in Chapter , we propose a novel application of Deep Graph Networks to the problem of predicting the dynamical properties of biological pathways. Part  of the thesis presents our two original contributions in the context deep generative learning of graphs, and its applications to computational chemistry. In Chapter , we discuss a general-purpose  model capable of generating unattributed graphs by sequentially generating its edges. In Chapter , we introduce a novel generative model of molecules in the context of drug discovery. Lastly, Part  concerns concluding remarks, opportuninities and future works, which are discussed in Chapter .

Published Works
Several publications have contributed to the development of this thesis. Section  is based on the journal article:

1em
bacciu2020dgn,
1em

where we re-elaborated the concepts behind Deep Graph Networks in a systematic introductory review. Chapter  draws from our conference paper:

1em
errica2020fairevaluation,
1em

which presented a fair and rigorous comparison of several state-of-the-art Deep Graph Networks. Chapter  is a compendium of the work developed in two conference papers:

1em
bove2020prediction

podda2020esannbiochemical.
1em

The former work first formulated the predictive problem, and has been later been extended for post-proceedings journal publication (in review). Chapter  derives from our work on the conference paper:

1em
bacciu2019graphesann,
1em

which was later extended to a journal article in:

1em
bacciu2020graphgeneration.
1em

Lastly, Chapter  concerns work published in the following conference paper:

1em
podda2020aistats.
1em
Preliminaries
Machine Learning and Neural Networks
In this chapter, we provide an overview of Machine Learning and neural networks. We start with a short introduction to Machine Learning, laying out the foundations on how learning from data is possible, as well as more practical aspects about the selection and evaluation of machine learning models. In Section , we briefly review the core concepts about nn, such as the procedure by which they are trained, the loss functions they optimize, and how they are regularized.

Machine Learning
Many real-world phenomena are not clearly understood, or cannot be characterized in terms of simple mathematical equations. However, one can usually collect quantitative and/or qualitative measurements about them, and observe their effects in the environment where they manifest. ml is a branch of Artificial Intelligence that studies algorithms and provides tools to learn these unknown processes from data. According to Mitchell, learning is defined as improving at some task from experience mitchell1997ml. To start off, let us first briefly describe the key components of a learning system in detail, introducing other useful notation, definitions and concepts along the way:


    the experience, in the context of ml, refers to
    the data which is available to the learner. Data is provided in the form
    of examples, (also called observations or data points). Each example is a set of qualitative or quantitative measurements about some phenomenon of interest;
    the task refers to some function  (called target function) that the learner needs to estimate from data. ml tasks are multiple, and of potentially very different nature. Three well-known examples of tasks are are classification, where  is a function that assigns a categorical label to a data point; regression, where  associates a desired numerical quantity to a given example; and density estimation, where  is the probability density (or mass, for the discrete case) function from which the examples are drawn from;
    the performance is a function which returns a quantitative measurement of how well the task is being learned. For example, in a classification task, one can measure improvement measuring the accuracy of the learner, the proportion of correctly classified examples out of the total number of available examples. Clearly, higher accuracies indicate that a task is being learned.


Machine Learning can be broadly divided in three main areas: supervised, unsupervised, and reinforcement learning. In supervised learning, the learner is given a set of input-output pairs, and the goal is to to learn the relationship between the inputs and the outputs. Classification and regression fall into the supervised paradigm. In unsupervised learning, data only consists of inputs, and the goal is to learn some property of the distribution that generates the data. Typical unsupervised tasks include clustering and density estimation. Reinforcement learning sutton1998rl is about learning to act in an environment, where the actions performed yield rewards or penalties. This work exclusively deals with supervised and unsupervised learning.

Regardless of the learning paradigm, the central issue in ml is generalization, that is, the learned function should be such that it works correctly on previously unseen examples, not used during the learning process. As it turns out, generalization is possible if the learning process is carefully crafted.

Learning and Generalization
Informally, learning can be thought of as finding a good approximation of the target function in some space of candidate functions. The search process is often referred to as training. During training, several candidates are evaluated until one that best approximates the target function is found. A program that implements this learning process is called learning algorithm.

The training process is driven by the data. Given a task, the learning algorithm has access to a dataset , a set of  iid samples drawn from some data domain  according to a fixed but unknown distribution . The nature of  depends on the learning paradigm. In the supervised case, we have , where  is called input space and  output space. The output space  is tightly coupled to the task to be learned; for example, in classification tasks,  is a discrete set and its elements are called labels; in regression tasks, , and its elements are called targets. In unsupervised learning,  is just the input space . For this reason, data used in unsupervised tasks is often called unlabelled. For the moment, we assume that the input space  is the set of -dimensional real-valued vectors . We call elements   feature vectors, and their elements features. Given a vector , we use the notation  to indicate its -th feature. Later on, we shall generalize the notion of input space to more complex spaces than just vectors, to apply ml to more structured data.

The space of candidate functions explored by the learning algorithm is called hypotheses space. Formally, a hypotheses space is a set of functions  whose members are called hypotheses. The hypotheses space is used indirectly by the learning algorithm, which rather operates in parameter space . The elements of the parameter space are called parameters, and each parameter  uniquely identifies a hypotheses . Usually, the parameters  correspond to a set of real-valued vectors.

Besides the dataset, the learning algorithm is also given a loss function . The role of the loss function is to provide a non-negative value to measure the error committed in approximating the unknown function  with a hypothesis . The objective of the learning algorithm is thus to find the optimal hypothesis  whose error approximating  is the lowest. This can be achieved by solving the following optimization problem:

where  is called risk functional, or generalization error. However, the above objective function is intractable, since the learning algorithm only has access to the specific portion of  represented by the dataset , randomly sampled from . Therefore, a tractable optimization problem is used instead:


where  is called empirical risk functional, or training error, and corresponds to selecting the hypothesis whose average loss computed on the dataset is the lowest. We call the hypothesis  given as output by the learning algorithm a model. The learning principle corresponding to this optimization problem is called erm.

Gradient-Based Optimization
There exist several methods to optimize the learning objective; in this thesis, we focus on gradient-based methods, which use the information provided by the gradient of the loss function (or an approximation thereof) to minimize the training error. Clearly, to apply gradient-based methods, the loss function has to be differentiable. By far, the most widely used gradient-based method in modern ml is sgd ruder2016overviewsgd. Briefly, sgd reduces the value of the loss function by moving the parameters in the direction of its negative gradient. The process requires to initialize the parameters to some randomly chosen value , and to iterate over the dataset several times. At each iteration , the parameters are updated according to the following update rule:

where  is a per-iteration learning rate that controls the magnitude of the update, and:

is an estimate of the true gradient of the loss function, averaged over a mini-batch of  data points. sgd has several desirable properties as regards its convergence: specifically, given a convex loss function, if the learning rate is decreased at an appropriate rate, sgd converges to its global minimum almost surely as ; if the loss function is not convex, under the same assumptions sgd converges to one local minimum almost surely. Over the years, different variants of sgd have been developed; a program that implements a specific variant is called optimizer.

Overfitting
One desired property of erm is that the empirical risk is guaranteed to approximate arbitrarily well the true risk as , provided that the hypotheses space has enough capacity, or complexity. The capacity of a hypotheses space is the scope of functions it is able to learn: the higher it is, the more complex functions can be learned. However, if the capacity of the hypotheses space is unconstrained, one might incur in overfitting, a phenomenon where the learned model perfectly predicts the examples in the dataset (achieves very low training error), but is unable to generalize to unseen examples (has high generalization error). Intuitively, an overly-expressive hypotheses space is likely to contain hypotheses so complex, that are able to fit not only the true relationships in the data, but also the sampling noise in the dataset. Such hypotheses would be then wrongly chosen as the best hypotheses according to the erm principle. Overfitting can be observed by dividing the dataset in two disjoint partitions: a training set which is used for learning, and a test set, which is held out from the training procedure and used to get an estimate of the true generalization error. To detect overfitting, one should monitor if the generalization error (estimated in the test set) diverges from the training error (computed on the training set) during the learning process.

Regularization
Besides using a separate test set for detection, overfitting can be prevented using regularization techniques. The general idea of regularization is to limit, directly or indirectly, the complexity of the hypoteses space used by the learning algorithm. A principled form of regularization of the learning process derives from the field of slt vapnik2000slt, which studies, among other problems, the relationship between the training error and the generalization error.  One important result of slt is the so-called generalization bound, which states that, if the complexity  of a hypotheses space is known , the following inequality:


holds with probability of at least  if . In other words, the generalization bound tells us that the generalization error is bounded by above by the training error and a confidence term , which depends on the number of examples  and the complexity of . These two terms are tightly related: the confidence term can be decreased by getting more data (increasing ) if possible, or by restricting the complexity of the hypothesis space using regularization. This second choice, however, leads to an increase of the training error. The bound induces a simple principle to learn effectively while avoiding overfitting, called srm vapnik2000slt, which requires minimizing both the training error and the confidence term.

Model Evaluation
With the goal of generalization in mind, a learning algorithm needs to be evaluated on unseen data. The name model evaluation, or model assessment, refers to the task of getting a proper estimate of generalization capability of the model. In model evaluation, one is not necessarily interested to evaluating the generalization error; in most practical cases, another performance metric is used. For example, in classification tasks, the accuracy of the model is evaluated instead. The estimation of the performance metric is performed on a test set; there exist different estimators, depending on how the dataset is split into training and test partitions. The simplest is the hold-out strategy, which splits the data into one training set and one test set, according to some predefined proportion. The parameters of the model are found using the training set, and the performance estimate is obtained from the test set. According to the field of statistics, an estimator can be decomposed in two related quantities: bias and variance hastie2009elements. Informally, bias is related to how close (or how far) the estimation is to the true value; variance is related to how much the estimation depends on the specific dataset on which it is obtained. Good estimators trade-off between the two. For small datasets, the hold-out estimator has high variance, since it is obtained on a single test set and might over-estimate the true value just by chance. To reduce the variance of the estimation, -fold cv arlot2010cv can be used instead. -fold cv consists in splitting the data in  disjoint partitions and repeat the evaluation  times. Each repetition is a hold-out estimation where one partition in turn acts as test set, and the remaining  form the training set. The final estimator is the average of the  estimates. In practical cases,  or  are often used.

Model Selection
Learning algorithms are such that finding good values of the parameters is usually not enough. In fact, the learning process is also influenced by other settings, not directly optimizable by gradient descent in parameter space. These extra settings are called hyper-parameters. Some examples are the learning rate  and the number of iterations of sgd; other hyper-parameters are specific to the particular learning algorithm used. The process of jointly choosing the parameters and the hyper-parameters of a model is called model selection, or hyper-parameter tuning. Model selection requires a set of hyper-parameter configurations to be evaluated. A configuration is any assignment of the hyper-parameter values. Given a configuration, a model is instantiated with the corresponding hyper-parameters, trained on the training set and evaluated on some held-out dataset. Usually, the hyper-parameters set is specified as a grid, where each hyper-parameter is associated to a discrete set of possible choices. When this is the case, model selection is referred to as grid search, and the algorithm is simply an exhaustive evaluation of all possible hyper-parameter configurations. Other strategies for defining the set of hyper-parameters and the model selection algorithm are also possible bergstra2009randomsearch,bergstra2012hyperopt. Model selection requires a separate set of data than the test set. In fact, if done on the test set, the set of hyper-parameters found would be tailored on that specific test set, and the corresponding performance would be an over-optimistic (biased) estimate of the true performance. In general, the test set cannot be used to take decisions about the learning process, regardless of whether the decision concerns the parameters or hyper-parameters of the model. This problem is solved by using one or more parts of the training set as a validation set, where the effect of the different hyper-parameters on the performances is estimated. In this sense, model selection can be viewed as a nested assessment inside the outer model evaluation task, and can be tackled using the same kinds of estimators such as -fold cv. Later on in this thesis, we shall see some strategies of how model selection and evaluation are performed jointly in practical settings.

Neural Networks
This work revolves around feed-forward nn haykin2009nnets,goodfellow2016dl, a learning algorithm inspired by the mechanisms through which the neurons in our brain learn rosenblatt1958perceptron. In brief, biological neurons acquire electrical signal from neurons they are connected to, and send it over to other neurons if the strength of such signal exceeds a given threshold. Learning in this context can be intended as the process that adjusts the strength of the neural connections according to the signal provided in input, such that a specific behaviour is obtained. For this reason, the neural approach to ml is often called connectionist, borrowing this terminology from the computational neuroscience field. Mathematically speaking, given an input signal , the general function performed by a neuron is the following:

where  are called weights (or parameters),  is called bias term,  is called activation function, and  is the output signal. An example of neuron is shown in Figure . We can generalize a neuron to output a vector  as follows:

where  is now a weights matrix,   is a bias vector, and the activation function is applied element-wise. Such construction is called a layer, and it is shown in Figure . nn compose multiple layers sequentially to implement complex functions as follows:

where  for  are called hidden layers and  is called output layer. In general, the -th hidden layer of the network computes an activation  as follows:

where ,  is the output of the previous layer (with ), and . In other words, a neural network applies a series of parameterized transformations to an input vector to compute some desired output. The network parameters  are optimized with sgd so that the end-to-end function  computed by the network is a good approximation of the target function . Notice that, to be able to use sgd, all the operations performed by the layers must be differentiable.




















*

The number of layers and the dimension of their parameter matrices constitute the architecture of the network. Network architecture, and the nature of the activation function of the hidden layers, play a major role in determining the kinds of functions a nn can learn. Specifically, nn with at least one hidden layer with non-linear activation function (also called mlp, such as the one shown in Figure ) are universal approximators, meaning that they can approximate arbitrarily well any computable function cybenko1998approximationuniversal. However, no guarantees as to which kinds of architectures work best can be given, according to the no free lungh theorem wolpert1997freelunchtheorem; thus, network architecture is task-dependent and must be optimized with model selection. As regards the activation functions, modern neural networks are commonly trained with three non-linear activation functions (or variations thereof)for which universal approximation capabilities have been proved:

    the sigmoid function, defined as:
    
    the hyperbolic tangent function, defined as:
    
    the relu function glorot2011relu, defined as:
    
    Notice that, even though the relu function is not differentiable at , it can be used in practical settings with some adaptation.

These three activation functions, and their derivaties, are shown in Figure .





















*
One distinctive advantage of nn with respect to other learning algorithms is automatic feature engineering: in fact, the hidden layers of a nn extract features from the data without an explicit guidance from the end user. This means that the network itself decides which representation of the data is relevant to solve a given task by tuning the parameters during learning; this decision is driven by the high-level goal of approximating the input-output relationship of interest. This ability is crucial in tasks where knowledge is difficult to represent, or where there is not enough domain expertise to manually devise features, and has been the key of the success of nn in ml.

Training
Learning in a nn requires several passes through the training set. Each pass is called an epoch, and consists of two phases. In the first phase, called forward propagation, training examples are fed to the network, which produces an output for each data point. Subsequently, the error between the output and the target is calculated by the loss function. The second phase consists in propagating the error back to the hidden layers to calculate the gradient of the loss function at each layer. This is done via the well-known backpropagation algorithm rumelhart1986backprop, which is basically an application of the chain rule of derivation for function composition. Once the gradient of the loss function is available at each layer, the parameters are modified using the sgd update rule. Training nn requires particular care to ensure the convergence of the sgd algorithm. In particular, the initial values of the weight matrices should be initialized with small random values around 0 for simmetry breaking, and the learning rate must be , to prevent too large update steps which would make the objective function diverge from the local minima lecun1998backprop.

Output Layers and Loss Functions
The output layer of a nn is chosen according to the task it should learn. In turn, each task is associated to a specific loss function that must be minimized by the optimizer, in order to find a set of parameters that generalize. nn are usually trained with mle hastie2009elements. Specifically, one assumes that the unknown function  is a distribution , whose parameters are given by the neural network. The idea is to make this distribution match the distribution of the data observed through a dataset . This corresponds to minimizing the dissimilarity between the distribution parameterized by the network and the empirical distribution of the data. This dissimilarity can be quantified by the kld between the empirical distribution and the distribution parameterized by the neural network:

where  is the empirical distribution. The kld is a proper loss function, since its output is always , and it is 0 if and only if . Thus, we can minimize it under the erm principle. Since  does not depend on the parameters of , to minimize the kld it is sufficient to minimize the term , the negative log-likelihood of the distribution parameterized by the network:

where  are the parameters of the network as usual. Below, we detail about three of the most common output layers, and the loss functions by which the corresponding networks can be trained. Following, we assume a dataset  defined as above, and a neural network with parameters  and  layers for simplicity.

Linear Output Layer
A linear output layer is used to solve regression tasks, tasks where the output space is continuous. It is defined as:


Notice that a linear output layer does not have an activation function; equivalently, one can think that the activation function is the identity. The model assumed for the true conditional is the following:

In other words, the network outputs the mean of a Gaussian distribution with unit variance. Applying the mle, minimizing the kld is equivalent to minimizing the mse loss function:

where  is the output of the network for a particular data point .
Notice that minimizing the mse corresponds to minimizing the Euclidean distance between the target  and the prediction of the network . A network with only one single linear output layer trained to minimize the mse loss function is commonly known in ml literature as the Linear Regression model.

Logistic Output Layer
A logistic output layer is used to solve binary classification tasks, tasks where the output space is the discrete set . The label 0 is usually called the negative class, while the label 1 is called positive. The layer is defined as follows:


Since the codomain of the sigmoid function is the real-valued interval , it is suitable to express output probabilities. To apply mle to a binary classification task, the following model for the true conditional is assumed:

In other words, the neural network outputs the parameter of a Bernoulli distribution for each data point . Using mle, minimizing the kld is equivalent to minimizing the bce loss function:

where  as usual. Training a single logistic output layer with the bce loss function is equivalent to training a Logistic Regression model.

Softmax Output Layer
The softmax output layer is used in multi-class classification tasks, tasks where the output space is a discrete set  of  mutually exclusive classes. In this cases, the targets  are expressed as one-hot vectors . In practice, each position of the one-hot vector corresponds to a specific class, and its entries are defined as follows:
*
    y_i=
    
        1 & if y = c_i

        0 & otherwise.
    

In words, a one-hot vector encodes the target as a discrete probability distribution over the possible classes, where all the mass is put on the class corresponding to the label . The layer is defined as:


where the softmax activation function is defined element-wise over a generic vector  as:

In practice, a softmax layer outputs a score for each possible class, and the vector of scores is normalized to be a probability distribution by the softmax function. The model for the true conditional assumed in this case is the following:

In other words, the conditional is a Multinoulli (categorical) distribution for the  classes, and the network outputs its parameters (the probabilities for each class). Minimizing the kld in this setting is equivalent to minimizing the ce loss function:

where  is the output distribution as given the network. Notice that the bce loss function is just a special case of ce where . A nn with one single softmax output trained with ce is known in the ml literature as the Softmax Regression model.

Regularization
As we have briefly described in Section , regularizing a ml model requires to somehow limit the complexity of the hypotheses space, such that the learning algorithm is more likely to select hypotheses that do not overfit the training data. For nn, one straightforward approach to limit complexity is to reduce the number of hidden layers, and the number of units in each layer. Other strategies are discussed below.

Penalized Loss Function
One very general regularization technique, which is not restricted to nn, is to impose a preference bias to the possible values the weights might take, such that configurations that generalize achieve a lower training error than configurations that overfit. Focusing again on the supervised case, and assuming a dataset  as usual, this can be achieved by augmenting the training ojective with a penalty term as follows:

where  indicates a generic -norm. The norm of the parameters implement the preference criteria used to avoid overfitting, whose influence on the training objective is controlled by a hyper-parameter . Depending on the value of , we distinguish:

    L1 regularization, where . The bias of L1 regularization is to push the values of some weights to zero. This corresponds to training a nn with less parameters (parameters set to 0 do not contribute to the loss function);
    L2 regularization, where . The bias induced of L2 regularization is to penalize large values of the weights. Intuitively, restricting the range of the values that the weights can take limits the type of functions the network can approximate.

Notice that these are not the only penalties possible, although they are the most used in practical settings hastie2009elements.

Early Stopping
Early stopping prechelt1998earlystopping is another general regularization scheme, which is widely adopted in nn training. In short, it requires monitoring a performance metric (which can be the value of the loss function or other task-specific metrics) on the validation set, in order to stop the learning process as soon as overfitting is detected. Once learning has stopped, the parameters that yielded the best score according to the metric are chosen by the learning algorithm. Intuitively, early stopping implicitly biases the hypotheses space by limiting the number of training iterations, which in turn limits the number of hypotheses evaluated during training.

Dropout
Dropout srivastava2014dropout is a regularization technique that is specific to nn. The idea behind dropout is to use a very expressive network for training, whose capacity is constrained stochastically at each iteration of the learning procedure. At each pass through a batch of training data, dropout turns off some of the units in a layer by multiplying their output with a binary vector mask, whose entries are samples from a Bernoulli distribution with hyper-parameter , called dropout rate. This has a double effect: first, a smaller number of parameters is used to compute a prediction (because some of them are made ineffective by the binary mask); second, the parameters used by the network are different at each pass (because of the stochasticity of the mask). According to this second implication, dropout implements a dynamic form of model ensembling, a ml technique in which one combines several low-capacity model to obtain a stronger predictor.

Auto-Encoders
An ae baldi2012autoenc is a nn architecture for unsupervised learning. ae are trained to reconstruct their inputs by jointly learning two mappings, one from the input space to a latent space, and another from the latent space back to the input space. During training, the latent space learns general features about the input that help achieve a good reconstruction. Given a -dimensional input vector , an autoencoder computes a function  as follows:
*
    h &= (x)

    r &= (h),

where  is an -dimensional latent code,  is a reconstruction of the input,  is an encoding nn or encoder with parameters ,  is a decoding nn or decoder, with parameters , and . Despite being an unsupervised model, ae can be trained in a supervised way, by implicitly using a dataset of the form , one where the input itself is the target. The loss function of an ae is called reconstruction loss, and measures the error between the input  and the reconstructed output , as shown in Figure . For continuous inputs, the reconstruction is simply the mse loss:

while for discrete inputs the ce can be used instead.






*
If the latent space of the ae is not constrained in any way, it learns to just copy its input to the output: in fact, one can always have  everywhere by choosing  as the identity function. This is not very useful in practice, as a network structured in this way would not learn anything useful about the data; thus, many forms of structuring or constraining the latent space have been studied. Perhaps the simplest form of ae is the undercomplete ae, one where . In this case, the latent space acts like a bottleneck that is forced to retain relevant features that allow to reconstruct the input, while discarding irrelevant ones. A useful byproduct of training an undercomplete ae is that the latent space acts as a manifold, a lower-dimensional subspace with Euclidean properties where data points are projected onto. One limitation of this kind of architecture is that one has to choose the capacity of the encoder and decoder very carefully. As an extreme case, consider that an over-capacitated ae with a 1-D latent space could learn to map each data point to the set of integers. Clearly, this mapping would be uninformative of the true data distribution.

Regularized Auto-Encoders
Regularized ae generally use , but impose constraints on the representations learned by the latent space through penalties on the loss function. Specifically, a regularized ae optimizes the following loss function:


where different choices of  define different variants. For example, sparse AEs
are trained in such a way that the latent space produces sparse representations, meaning that only certain units are active for certain data inputs. This can be accomplished by constraining the mean activation of the units to be small. Specifically, in a sparse ae, if  is a desired average activation of the outputs  of a hidden layer, the penalty is formulated as:

If  is chosen to be small, the penalty constrains most hidden units to have zero activation. Similarly, contractive ae rifai2011contractiveautoenc regularize the objective function by imposing a penalty on the gradients of the hidden units with respect to the input. More in detail, the penalty term of a contractive ae is the following:

which intuitively corresponds to penalizing hidden activations that have large variation for small variations in the input. Thus, the local structure of the latent space is forced to be smooth. Differently from the other variants, denoising ae vincent2010denoisingautoenc achieve regularization acting on the training procedure, rather than via a penalty term. In a denoising ae, the input  is corrupted before being passed to the network (usually through Gaussian noise addition). Thus, the network uses the corrupted version  during forward propagation, while the loss is calculated on the original input. By being trained to remove the noise from the input, the network is forced to learn meaningful patterns. The forward propagation phase of a denoising ae is shown in Figure .





*

Deep Learning
In Section , we have talked about the representational power of nn. However, the way this representational power is attained is greatly influenced by the depth of the network, its number of layers. When a nn is deep, the hidden features are organized during learning in a hierarchy, in which simpler features of the early layers are progressively combined into very sophisticated features in subsequent layers. Although shallow networks (networks with very few but large hidden layers) have their same approximating capabilities, deep networks combine features in a more computationally efficient way bengio2009deeparch. This data-driven representational bias has proven effective in many practical domains, such as computer vision krizhevsky2017imagenet and nlp vaswani2017transformer, establishing the success of deep nn in many ml tasks. The term dl goodfellow2016dl, coined around 2006, is used to refer to nn architectures composed of a large number of hidden layers, usually . The representational power of deep networks comes in hand with a number of computational challenges that arise from their depth. Below, we summarize the most well-known:

    the loss functions used for training deep networks are extremely complicated and non-convex, since the hidden layers have non-linear activation functions. This means that gradient-based methods may get stuck in bad local minima;
    very deep networks suffer from vanishing or exploding gradient issues, which can prevent the network from learning, or make the optimization numerically unstable, respectively;
    training deep nn requires large datasets and an extensive amount of computational power.

These three major issues have been the focus of basic research on deep networks in the last years. As regards the first challenge, these problems have been tackled by better characterizing the properties of the optimization problem nn try to solve goodfellow2015optimization,janocha2017lossfunction. In parallel, improvements have been achieved by devising more effective optimizers kingma2015adam,ruder2016overviewsgd and effective activation functions glorot2011relu. As regards the second challenge, several mechanisms to prevent the two undesirable phenomena from happening have been proposed and applied with success; the most known are gradient clipping zhang2020gradientclipping to prevent gradient explosion, batch normalization ioffe2015batchnorm and residual connections he2016resnets to prevent gradient vanishing. As for the third challenge, major contributions came from the advent of the big data revolution, which ensured large and progressively more curated data sources, and the use of gpu vectorization, automatic differentiation and computational graphs abadi2016tensorflow,paszke2017pytorch to speed up the training and inference processes of deep networks.

Convolutional Neural Networks
A cnn lecun1995convolutionalnn is a kind of nn born in the field of computer vision. The development of cnn started long before the deep learning renaissance in 2006, but they only recently became popular, after demonstrating their effectiveness in several image recognition tasks krizhevsky2017imagenet. Given their historical importance in the landscape of dl, we shall present them briefly even though they are not part of this thesis. The standard hidden layer of a cnn is called convolutional layer, and it is able to process 2-dimensional data such as images. It does so with parameterized filters which are applied to an input through a convolution operation. The output of a filter is usually called feature map. More in detail, a 2-D convolutional filter computes computes a feature map where each entry is defined as follows:

In the equation above,  is a 2-D matrix (for example, an image with width  and height ),  is a learnable filter, or kernel, with width  and height , and  is a convolutional operator. In practice, the filter is slid on the input row-wise. The values in the feature map have larger magnitude if parts of the input match the filter, or more intuitively, if the pattern of the filter is detected (to some extent) in the input image. In turn, this implies that each feature of the feature map shares the same weights (those of the kernel). This approach is called weight sharing, and it implements translational invariance, features are detected regardless of their position in the input. Another essential layer of cnn is a pooling layer. It works by dividing the input in non-overlapping regions, and computing an aggregation function (usually a max) for each region. Pooling serves a double purpose: firstly, it reduces the number of weights needed in subsequent layers, thus maintaining computational tractability as the depth of the network grows; secondly, it acts as a regularizer, as it discards the specific information of the region where it is applied, picking up only one representative pattern. Convolutional and pooling layers are applied sequentially to the input data, followed by a standard hidden layer activation function, usually a ReLU. The three layers (Convolution-Pooling-ReLU) in cascade are the standard building block of convolutional architectures. The last layers of a cnn usually consist of a standard mlp (or a single output layer), which uses the final representation computed by the convolutional blocks as input, and computes the corresponding task-dependent output. Even though they were born within the computer vision field, cnn have been generalized to 1-D inputs (such as sound waves in speech recognition tasks) and 3-D inputs (such as video frames in object tracking tasks).

Deep Generative Models
Typical unsupervised tasks require to learn the probability distribution of the data, or some aspect of it, such as, for example, how the data is clustered together. Broadly speaking, the two operations that approximating the data distribution allows are:

    inference, that is, computing the density of arbitrary data points;
    sampling, that is, generating new data by drawing samples from the distribution.

dgm goodfellow2016dl are essentially deep architectures that learn how to do inference and sampling, or just sampling, from the data distribution. Based on this definition, we can further distinguish between:

    prescribed dgm, which can jointly learn inference as well as sampling;
    implicit dgm, which only learn sampling.

Below, we provide a brief overview on the topic.

Prescribed Deep Generative Models
Prescribed models are trained under the mle principle, by minimizing the kld between the empirical data distribution  and a model  of the true data distribution parameterized by a deep nn with parameters :

Notice that the setup is slightly different from Section : there, the neural network was estimating the parameters of a conditional distribution; here, it is approximating the distribution itself. Similarly as we have seen, the term  does not depend on the parameters of the model; thus, the following quantity can be minimized equivalently:

In practice, applying mle on a (generally unlabelled) dataset , prescribed dgm learn the data distribution by minimizing the negative log-likelihood of the observed data using sgd:

Two prescribed dgm of relevance to this thesis is are described in the following. Besides those mentioned above, other prescribed dgm include for example Sigmoid Belief Networks neal1992sigmoidbeliefnet, and Flow-based models rezende2015normalizingflows.

Autoregressive Models
An ar model is a prescribed dgm where the data distribution is decomposed in a product of conditionals according the chain rule of probability:

where  are random variables, and . Basically, each conditional is approximated by a deep nn  that takes as input  (which for now we assume to be a fixed-size vector for simplicity) and outputs a distribution over . Inference in autoregressive models is achieved by forward propagating the input through each network in the order established by the decomposition, and summing up the intermediate probabilities. To generate a data point according to the learned distribution, it is sufficient to sample each conditional in the order given by the chain rule decomposition. Given their sequentiality, autoregressive models are often implemented with Recurrent Neural Networks (see Section ); but in principle, any neural network that takes two inputs (the current input of the conditional  and a summarization of ) can be used. Another family of autoregressive models use neural networks with masking techniques to constrain their output to follow a given chain rule order. For example, the model by  uses autoencoders, while  uses convolutional layers.

Variational Auto-Encoders
A vae kingma2014vae is a prescribed dgm which originates from the family of latent variable models. In latent variable models, a set of latent variables , also called explaining factors, is incorporated to the data distribution by marginalization as follows:

In the above formula,  is a decoding distribution and  is a prior over the latent variables, usually chosen to be a tractable distribution such as a Gaussian. The generative process expressed by a latent variable model consists in sampling from the prior a specification of the data, provided by the latent variables, which is used to condition the decoding distribution. Since the latent variables are not known in general, vae introduce an encoding distribution  to produce latent variables given a data point. Instead of the data log-likelihood, vae work with a related quantity, called elbo of the true log-likelihood, defined as follows:


and such that . By maximizing the elbo, the true log-likelihood of the data can be recovered. Intuitively, this can be achieved by maximizing the expected log-likelihood of the decoding distribution  under the encoding distribution (the first term), while making the encoding distribution  close to the prior distribution  at the same time (as specified by the kld term). In practice,  is chosen to be a standard Gaussian  with unit covariance matrix . The encoder distribution  is also a Gaussian distribution, whose mean and standard deviation are given by two different neural networks (which share some parameters but not the output layers). Specifically,
 with , such that , . The fact that both the prior and the encoder distributions are Gaussian gives the kld term of the elbo in closed form. The decoding distribution  is implemented as another deep nn  with parameters . The loss function minimized by a vae is thus the following:

with . The leftmost term is a likelihood-based reconstruction loss, and the rightmost term regularizes the encoding distribution by making it similar to the prior. The forward propagation of a vae is specified as follows: first, the input  is mapped to the mean and variance of the encoding distribution by the encoder network. The two parameters are used to sample a latent vector . This is turn is given to the decoder network, which outputs a reconstruction . One major issue with this formulation is that this model cannot be trained with sgd, since the gradient of a stochastic operation (the sampling from the encoder distribution) is not defined. Thus, the sampling process is reparameterized as , with . This way, the stochastic operation is independent of the input, and gradient backpropagation through the encoder becomes deterministic. The reparameterized model is shown in Figure .





*
The vae has several interesting properties: firstly, the latent space of a trained vae is approximately normally distributed with 0 mean and unit variance. This means that it is compact and smooth around the mean, which in turn enables the user to seamlessly interpolate between latent representations. Secondly, it allows two generative modalities: an unconstrained one, which can be achieved by discarding the encoder network and starting the generative process by sampling from the prior ; and a conditional one, which is obtained by running an input  through the entire network. This last modality is generative in the sense that the network outputs a variation (not an identical copy) of the input, due to the stochasticity induced by sampling the learned encoding distribution.

Implicit Deep Generative Models
Implicit models mohamed2016implicitgan learn a stochastic procedure that generates data similar to that of the training distribution. The most salient characteristic of implicit dgm is that they do not minimize the negative log-likelihood of the training set. Instead, an implicit model consists of a deterministic generator function  parameterized by weights , which maps latent variables (obtained by some prior ) into data samples:

The mapping function  (which is usually a deep nn) induces a density over the data, but does not give its explicit form: one can observe samples from the density, but cannot compute their probability. This implies that mle is not applicable anymore, since the density are inaccessible.

Generative Adversarial Networks
gan goodfellow2014gan solve the problem above by introducting a function , parameterized by weights , called discriminator. The discriminator is basically a binary classifier that is given either samples from the dataset or from the generator, and its purpose is to classify their origin correctly. The whole process translates into the following min-max objective function:

where in practice  is trained to improve the samples it generates, so that  classifies them wrongly as if they were real; conversely,  is trained to improve at better distinguishing real from samples generated by . The whole architecture is trained to reach a Nash equilibrium, so that both  and  cannot improve further. During training, the parameters of the discriminator and the generator are updated in turns. First, the discriminator is trained to minimize the following loss function:

where . Notice that this is a simple binary classification task, where the positive label indicates that a sample is real, and a negative label indicates that it is generated. Then, the generator is trained to minimize the following loss:

and the overall loss term is simply the sum of the two losses:

Figure  shows a schematic representation of a gan.





*
Notice that, in order to train the architecture of the model, the samples taken from the generator of a gan must differentiable.
Deep Learning in Structured Domains 
A structured domain is a data domain whose elements are formed by a set of atomic entities, and the relations between them. Structured data is common in several fields, such as biology, chemistry, finance, social networks, and many more. Typical examples are sequences such as time-series data, or graphs representing molecular structures. One distinctive characteristic of structured data is that it has variable size, meaning that the number of entities composing the datum is not fixed in general. This constitutes a serious limitation for traditional ml models, which are designed to work with flat data, collections of fixed-size vectors. In principle, they can be adapted to work with variable-sized data by incorporating the structure of the data to the input vectors as additional features. While useful to some extent, this approach requires to decide which features are needed to solve a task. This, in turn, requires a level of domain expertise that is not always available for many interesting problems. In contrast, nn (and Deep Learning models more so) are able to learn which features are useful to solve a task adaptively from data, without the need of feature engineering. Thus, the general idea is to provide the structured data directly as an input to the network, which automatically learns the needed features and the task, guided by the learning process. In this chapter, we present a class of nn that are able to handle variable-sized inputs for learning in structured domains.

Graphs
The elements of structured domains can be described in a compact and convenient notation using the general formalism of graphs bondy1976graph. Informally, a graph is a collection of vertices (the entities) connected through a collection of edges (the relations). In literature, vertices are sometimes called nodes, while edges are also referred to as arcs or links. Formally, a graph with  vertices is a pair

where  is its set of vertices, and  is its set of edges. In a graph,   specifies the graph structure, that is, the way vertices are interconnected. Notice that the pair  is unordered: in this case, the graph is called undirected. Figure  shows a visual representation of an undirected graph.
Given an edge ,  and  are called its endpoints, and are said to be adjacent. Alternatively, we say that  is incident to  and . Edges of the form  that connect a vertex to itself are called self-loops.




















*
Graphs where it is possible to have more than one edge between a pair of vertices are called multigraphs. In this work, we restrict ourselves to the case where there is at most one possible edge between two vertices.

Directed Graphs
A directed graph is one where he edges are ordered pairs of vertices, or equivalently one where . A directed edge is written as , meaning that it goes from vertex  to vertex . An example of directed graph is shown in Figure . Given a directed graph  and one of its vertices , the set of all vertices from which an edge reaches  is called predecessors set, and is defined as . The cardinality of the predecessors set is called the in-degree of the vertex, and we indicate it as . Analogously, the set of all vertices reached by an edge from  is called the successors set, and is defined as . Its cardinality is called the out-degree of the vertex, and indicated as . The neighborhood (or adjacency set) of a vertex  is the union of the predecessors and successors sets: . Alternatively, one can view the neighborhood as a function  from vertices to sets of vertices. The cardinality of the neighborhood is called the degree of the vertex, indicated as .  In this work, we consider all graphs directed unless otherwise specified. Undirected graphs are thus implicitly transformed into directed graphs with the same vertices, where the set of edges contains the edges  and  if and only if  is an edge of the undirected graph.

Bipartite Graphs
A graph  is called bipartite if we can split  in two disjoint subsets  and , such that  if and only if either  and , or  and . Figure  shows an example of bipartite graph, where
 and .

Walks, Paths, and Cycles
Let  be a graph. A walk of length  is any sequence of  vertices , where each pair of consecutive vertices is adjacent, , . A path of length  from vertex  to vertex  is a walk such that  and , where each vertex appears exactly once in the sequence. If, given two vertices  such that , there exists a path between them, we say they are connected, or that  is reachable from . Otherwise, we say they are disconnected, or that  is unreachable from . A shortest path from a node  to a node  is the path, among all paths from  to , with the smallest length. We indicate it with the notation . A graph is called connected if every vertex is connected to any other vertex (ignoring the direction of the edges); otherwise it is called disconnected. A cycle, or loop, of length  is a walk where , and all the other vertices appear once in the sequence. Graphs that do not contain cycles are called acyclic.

Trees and Sequences
A graph  is called a tree if its set of edges defines a partial order over the set of vertices, implying that it is also connected and acyclic. The vertices of a tree are called nodes. Given an edge , we call  the parent of  and  the child of . The set of children of a node  is indicated with the notation . In a tree, every node has exactly one parent, with the exception of a node called root or supersource, which has no parent node. A tree is positional if we can distinguish among the children of a node, if there exist a consistent ordering between them. Trees have a recursive structure: every node  is itself the root of a tree, called sub-tree of  rooted at v, and indicated as . If  contains only ,  is called a leaf. Trees encode hierarchical relationships among nodes; an example of tree with five nodes is shown in Figure .

A graph  with  vertices is called a sequential graph, or sequence of length , if its set of edges defines a total order over the set of vertices, which allows us to represent the set of vertices in an ordered fashion as . In a sequence, the vertices are usually called elements. A sequence can be viewed as a special case of tree with only one leaf. Sequences are useful to encode sequential relationships among elements; Figure  shows an example of a sequence of four elements.














*

Subgraphs and Induced Subgraphs A subgraph
 of a graph  is any graph for which  and  . If , or equivalently, if  contains only vertices that are endpoints in , the resulting subgraph is called induced subgraph.

Attributed Graphs 
Real-world instances of graphs usually carry out other information besides structure, generally attached to their vertices or edges. As an example, consider the graph representation of a molecule, in which vertices are usually annotated with an atom type, and edges are annotated with a chemical bond type. Given a graph  with  vertices and  edges, we define the associated graph with additional information content, and we call it an attributed graph, as a triplet  where  is a mapping from the space of vertices to a space of -dimensional vertex features, and , is a mapping from the space of edges to a space of -dimensional edge features. The values of these features can be either discrete (in which case the features are called labels and encoded as one-hot vectors) or continuous vectors. In most cases, we omit to define  and  explicitly, and provide the vertex and edge features directly as sets,  for the vertex features, and  for the edge features. In this case, an attributed graph is a tuple:

If some ordering of the vertices and edges is assumed, we can represent equivalently  as a matrix  where the -th row contains the vertex features of the -th vertex; analogously, we can define  as a matrix of edge features . In this case, an attributed graph is a triple:


Isomorphisms, Automorphisms, and Canonization 
An isomorphism between two graphs  and  is a bijection  such that  if and only if . Intuitively, graph isomorphism formalizes the notion of structural equivalence between graphs, in the sense that two isomorphic graphs are structurally equivalent, regardless of the information they contain. Figure  shows two isomorphic graphs and their corresponding  bijection. An automorphism  is an isomorphism between  and itself. Since  is essentially a permutation of the vertex set, it follows that a graph always has at most  possible automorphisms. Intuitively, and similarly to graph isomorphism, graph automorphisms convey the notion that the structure of a graph is invariant to permutation of the vertices and edges. An automorphism  on an example graph is shown
in Figure . Related to isomorphisms and automorphisms is the problem of graph canonization, where a canonical ordering (or form) of the graph vertices is sought, such that every graph  isomorph to a given graph  has the same canonical form. As we shall see, (approximate) graph canonization plays a role in the usage of graph within practical contexts; conversely, many techniques described in this work try to avoid representing graphs in canonical form, in favor of permutation-invariant representations.















*

Graphs Matrices 
One compact way to represent the structure of a graph is through its  adjacency matrix. Given a graph  with  vertices and  edges, the entries  of its corresponding  adjacency matrix  are defined as follows:







Note that the diagonal entries  of the adjacency matrix specify the presence  (or absence) of self-loops. Another interesting property of the adjacency matrix is that  it is symmetric for undirected graphs, which implies  . Adjacency matrices make some calculations of graph properties particularly convenient: for example, the in-degree and out-degree of a vertex 
can be obtained by performing row-wise and column-wise sums on :



Adjacency matrices are also useful to understand concepts such as graph automorphisms: in fact, an automorphism of  corresponds to a permutation of the columns or rows of the adjacency matrix (but not both). Other useful matrices to represent properties of graphs are the Laplacian matrix , and the symmetric normalized Laplacian matrix . In both definitions, the matrix   is the degree matrix, where all entries are zero except the diagonal entries, for which . These matrices provide information about the graph connectivity through their eigenvalues and eigenvectors.

The Adaptive Processing of Structured Data
The processing of structured data for learning purposes is carried out by a structural transduction, namely a function  where  and  are structured domains. When the structural transduction is implemented by a (deep) nn, it is adaptive, it is learned from data. A structural transduction can be decomposed as , where:

     is called encoding function or state transition function that is applied separately to each element of the structure. The output of the encoding function is a structure isomorphic to that in input, where the elements are now state vectors. Intuitively, a state vector encodes the information of the element and of the elements it depends on;
     is called output function, which computes an output from the state vectors.

The output function of the structural transduction is task-dependent. Considering a supervised setting and a generic graph dataset  consisting of  training pairs, we distinguish two learning problems:

    in structure-to-structure tasks, the dataset is composed of pairs of attributed graphs , where  is an input graph,  is an output graph, and the two underlying unattributed graphs  and  are isomorphic under a bijection . The task is to predict the target associated to an output graph vertex, given the corresponding features of its isomorphic vertex. The objective function minimized in these tasks is the following:
    
    in structure-to-element tasks, the dataset has the form  , where  is an input graph and  is an output vector. The task is to predict a single output vector (or scalar) from the structure and the features of . The  objective function minimized in these tasks is the following:
    
    To learn structure-to-element tasks, the output function must compress the states of each element of the structure into a global output vector representing the entire structure, which is compared to the target . To do so, there are several strategies; in general, one could pick a single state vector as a representative for the whole structure, or compute a summary of the entire structure using all the available state vectors. The function that implements the latter strategy is usually termed readout.


As anticipated, one important issue that structural transductions need to address is how to deal with variable-sized inputs. The solution is to apply the same state transition function (that is, with the same adaptive parameters) locally to every element in the structure, rather than to apply it one time to the overall structure. This process is similar to the localized processing of images performed by cnn, which works by considering a single pixel at a time, and combining it with some finite set of nearby pixels. This local property of the structural transduction is often referred to as stationarity. An interesting byproduct of using stationary transductions is that they require a smaller number of parameters with respect to non-stationary ones, since the network weights are shared across the structure. At the same time, using stationary transductions also requires additional mechanisms to learn from the global structure of the datum (such as readouts in the case of structure-to-element tasks), rather than only locally.

In the following sections, we present three specific nn architectures that implement transductions over structured data: recurrent neural networks, which process data represented as sequences; recursive neural networks, which process hierarchical data such as trees; and deep graph networks, which process general graphs.

Recurrent Neural Networks
A rnn is a nn architecture able to process sequences. Let  be an attributed sequence of length  with elements , and let  be its element features. Here, we slightly abuse the notation  in favor of , since sequence elements are ordered. In supervised settings, we also assume an isomorphic attributed sequence  with targets . The state transition function of a rnn, applied locally to each sequence element, has the following general form:
*
    ht =
    
        0 & if t = 0

        (xt, ht-1) & otherwise,
    

where  is a state vector, also known as hidden state, and  is a zero vector. The calculation of the hidden state performed by the state transition function  is recursive: to compute a hidden state for the -th element of the sequence, the hidden state of the previous element must be known in advance. Thus, the state computation is a sequential process, where the input sequence is traversed in order one element at a time, and the hidden state is updated as a function of the current sequence element and the hidden state at the previous step. To avoid infinite recursion, the hidden state is initialized with the zero vector . As the sequence is traversed, the hidden state maintains a memory of the past elements of the sequence. The presence of a memory mechanism makes rnn very powerful: in fact, it has been proved that finite-size rnn can compute any function computable with a Turing machine siegelmann1995rnnturing. As with cnn, the development of rnn started in the early '90s, and they have recently been rediscovered within the dl framework after their success, especially in nlp-related tasks.

Training
Given a sequence  with features , the original implementation of the state transition function of a rnn is defined as follows :
*
    ht &= Wxt + Uht-1, t=1, , m.

The above is also called recurrent layer. The weight matrices  and , are shared among the sequence elements according to the stationarity property. For this reason, it is often said that the network is unrolled over the sequence. In structure-to-structure tasks, once the states of the elements are calculated, an element-wise output is computed as:
*
    ot = g(ht), t=1, , m,

where  can be any neural network such as one simple output layer or a more complex downstream network. Similarly, in structure-to-element tasks, a single output is computed from the last hidden state of the sequence:
*
    o = g(hm).

Figure  shows a rnn in compact form, as well as unrolled over a sequence of length  for a structure-to-structure task. The error of the network during training is computed by comparing the output of the network for each sequence element  to the isomorphic sequence element  in the target sequence with the loss function , which is summed up over all the elements in the sequence. Notice that it is possible to stack multiple recurrent layers and create deep rnn by feeding the hidden state produced the recurrent layer to a subsequent recurrent layer, other than to the next step of the recurrence. In this cases, the output is computed after the last recurrent layer.















*
rnn can be also adapted to learn structure-to-structure distributions of the kind , where the underlying unattributed graphs  and  are not isomorphic, when the lengths of the input and target sequence do not match. The usual way to proceed in this case is to use two rnn: one acts as an encoder, computing a fixed-size representation of the input  (for example, its last hidden state as seen above); the other acts as a decoder of the target sequence , conditioned on the input representation. The conditioning is achieved by initializing the hidden state of the decoder rnn with the encoding of the input computed by the encoder rnn. These types of architectures are called s2s models.

rnn are usually trained with bptt werbos1988backpropthroughtime, a variant of vanilla backpropagation that propagates the gradient both from the output layer to the recurrent layer, and backwards along the sequence elements. One bptt update requires  computation, where  is the sequence length and  is the size of the mini-batch given to the optimizer. This can become computationally inconvenient for long sequences, and can lead to instabilities like gradient vanishing. Thus, in practical settings, faster bptt variants are often used, such as truncated bptt jaeger2002truncatedbptt.

Gated Recurrent Neural Networks
Vanilla rnn struggle to learn with long sequences. This issue has been documented several times in the literature (see bengio1994learninglongtermdependenciesdifficult), and is mostly due to the gradient vanishing or exploding problems. While gradient exploding can be dealt with gradient clipping, gradient vanishing is more hard to tackle. Several workarounds have been proposed to overcome such limitation; the most adopted in practical settings exploits a form of information gating. Specifically, gating mechanisms in rnn are used to control the information flow inside the recurrent layer. In particular, it might be useful for the network to forget useless information, or to reset the hidden state when some kind of knowledge has already been processed. Gated mechanisms fulfill this purpose adaptively, driven by data. The most used rnn variant that implements gating mechanisms is the lstm hochreiter1997lstm.
An lstm is composed of a cell , an input gate , a forget gate , and an output gate . Assuming an input sequence element , the hidden state  of a lstm is computed as follows:
*
    ft &= W_1xt + U_1ht-1

    it &= W_2xt + U_2ht-1

    gt &= W_3xt + U_3ht-1

    c_(t) &= W_4xt + U_4ht-1

    ct &= ft ct-1 + it c_(t)

    ht &= gt (ct),

where  is the Hadamard (element-wise) product between matrices. Notice that the weight matrices  and  with  are all different. In short, the input gate controls how much of the input is kept, the forget gate controls how much information about previous elements is kept, and the output gate controls how much of the two should be used to compute the hidden state. While powerful, a single lstm requires eight weight matrices; thus, it is computationally expensive to train. The gru cho2014gru gating mechanism is a lightweight alternative to lstm which uses less parameters, though it is slightly less powerful gruber2020gruspecificlstm. A gru uses two gates, an update gate  and a reset gate , and computes the hidden state as follows:
*
    ut &= W_1xt + U_1ht-1

    rt &= W_2xt + U_2ht-1

    h_(t) &= W_3xt + U_3(rt ht-1)

    ht &= (- ut) ht-1 + ut h_(t),

where  is a vector of all ones. In practice, the reset gate controls how much information from previous sequence elements should be kept, and the hidden state is computed as a convex combination of this quantity and the previous hidden state, controlled by the update gate.

Recurrent Neural Networks as Autoregressive Models
Besides being used as supervised models, rnn can be also used as generative models of sequences. Specifically, given a dataset  of sequences, they can be trained to learn a model  of the underlying distribution  as follows:

where  indicates the sequence length,  is a special start of sequence symbol , and  as usual. Once the network is trained, a sequence can be generated one element at a time. The process is initialized by feeding the start of sequence token  and the initial hidden state , and proceeds repeating the following instructions until an end of sequence token  is predicted by the network:

    update the current state  with the rnn and predict an output , which corresponds to a conditional distribution over the possible sequence elements. Sample a sequence element  according to this distribution;
    feed the sampled element  and the updated state  to the network and repeat.

We call this process autoregressive sampling mode. The process is depicted in Figure .





*
During training, the sampling process breaks the differentiability of the model. Hence, one resorts to reparameterization jang2017gumbel or differentiates the output  instead of the hard sample bengio2013straighttrough. Another option during training is teacher forcing williams1989teacherforcing: in this case, the knowledge of the elements of the sequence during training is exploited, by feeding the ground truth sequence element (instead of the sampled value) as the input for the next sequence. Both strategies have advantages and disadvantages: teacher forcing learns faster initially, but does not expose the network to its own errors, thus it can be less precise at generation time. Often, a combination of the two is used.





*

Recursive Neural Networks 
A recnn sperduti1997generalizedneuron,frasconi1998general is a nn architecture that can adaptively process hierarchical data. Using trees as an example, let  be a tree with  nodes , and  be its set node of features. The state transition function of a recnn, applied locally to each node , is the following:
*
    hv=
    
        0 & if ch(v) = 

        (xv, ht_v) & otherwise,
    

where  is the hidden state of the sub-tree rooted at . As with rnn, the state transition function  is recursive, but this time the recursion is defined over the tree structure. Specifically, to compute the hidden state of a node, the hidden state of all its children must be known in advance. The state computation starts at the leaves of the tree (where the state is initialized beforehand to make the recursion well-defined), and proceeds bottom-up until the root node is reached.
The development of recnn started in the middle '90s, with the introduction of the notion of generalized recursive neuron sperduti1997generalizedneuron and the development of a general framework for learning with tree-structured data frasconi1998general, which was later extended to more expressive classes of structures such as dag and dpag. Since then, they have applied fruitfully in several fields, including among others cheminformatics micheli2007introductionrecnncheminformatics, sentiment analysis socher2013recnnsentiment and scene parsing socher2011parsingscenes. Interestingly, recnn are also backed up by strong theoretical results, which support the generality of the structural transduction and characterize the kind of functions they can learn. Specifically, universal approximation theorems showing that recnn can approximate arbitrarily well any function from labeled trees to real values hammer1999recnn, and from labelled dpag to real values hammer2005universal have been proved.

Training
Using the binary tree of Figure  and a structure-to-element task as an example, one possible implementation of the state transition function of a recnn is the following:
*
    hv &= Wxv + U_lhl(v) + U_rhr(v), v t.

In the above formula,  can be any hidden activation function, and , , and , are weight matrices shared across the structure. Notice that the two weight matrices on the node children are positional, meaning that they are applied to a certain node according to its position. In the example case of a binary tree, the two functions  and  select the left and right child of a node , respectively, if they exist. Figure  shows the unfolded recnn over the tree, where the final output of the entire structure is obtained using the hidden state of the root node  as:

where  is the output of the network and  can be any downstream network such as a simple output layer, or a more complex neural network. For structure-to-structure tasks, the output is calculated node-wise as follows:

Notably, the order by which the hidden states need to be calculated (the numbers at the left of the hidden states in the figure) must be respected to ensure that the recursive process is well-defined. The states of nodes with the same ordering can be calculated in parallel according to the tree structure, which makes recnn more efficient than rnn when compared on structures with the same number of elements.





*
More in general, recnn are analogous to rnn as to how they can be trained with mle, and as what kinds of conditional distributions they can learn (even though in practical cases the structure-to-element scenario is more common).

Deep Graph Networks 
The rnn and recnn models presented in Sections  and Section  share the idea that the state transition function is applied locally and recursively on the structure to compute the state vectors. Extending it to arbitrary graphs (which can have cycles) would require to apply the state transition function recursively to the neighbors of a vertex. However, this approach is not applicable to general graphs. In fact, the presence of cycles creates mutual dependencies, which are difficult to model recursively and may lead to infinite loops when computing the states of vertices in parallel. While this issue can be overcome by resorting to canonization techniques that provide an ordering between the vertices, it is not feasible in many practical cases. dgn are a class of nn which can process arbitrary graphs, even in presence of cycles. The solution adopted by dgn to the problem of modelling mutual dependencies is to update the state of the vertices according to an iterative scheme. Specifically, the hidden state of a vertex is updated as a function of the hidden state of the same vertex at the previous iteration. Given an attributed graph  with vertex features , the state transition function computed by a dgn, applied locally to each vertex of , has the following form:
*
    v=
    
        xv & if = 0

        ^()(xv, v& otherwise,
    

where  is now the state of vertex  at iteration , and . Notice how the value of the state vector does not depend on the value of the neighboring state vectors, but to the same state vector at the previous iteration. Following, we slightly change terminology and refer to the vertices of a graph as nodes, in accordance to the terminology currently used in the literature. For the same reason, we shall use the following terminology as regards the supervised tasks that can be learned with dgn:

    structure-to-structure tasks shall now be termed node classification tasks if the targets are discrete node labels, or node regression tasks if the targets associated to the nodes are continuous vectors or scalars. We further distinguish among inductive node classification (respectively, regression) tasks, if the prediction concerns unseen graphs; and transductive node classification (respectively, regression) tasks, if the structure of the graph is fixed (, the dataset is composed of one single graph), and the task is to predict from a subset of nodes for whose target is not known. The transductive setting is often referred to as semi-supervised node classification (respectively, regression);
    structure-to-element tasks shall now be termed graph classification tasks if the target associated to the graph is a discrete label, or graph regression tasks if the targets are continuous vectors (or scalars).


Contextual Processing of Graph Information
Besides solving the problem of mutual dependencies in the state computations, the iterative scheme has another important purpose, that of propagating the local information of a node to the other nodes of the graph. This process is known under several names, such as context diffusion. Informally, the context of a node is the set of nodes that directly or indirectly contribute to determine its hidden state; for a more formal characterization of the context, see micheli2009nn4g. Context diffusion in a graph is obtained through message passing, by repeatedly applying the following procedures:

    each node constructs a message vector using its hidden state, which is sent to the immediate neighbors according to the graph structure;
    each node receives messages from its neighbors, which are used to update its current hidden state through the state transition function.

Message passing is bootstrapped by initializing the hidden state of the nodes appropriately, so that an initial message can be created. Usually, this initial message is the vector of node features. Using the example graph of Figure  as reference, we now explain how the context flows through the nodes as message passing is iterated. At iteration , the vertex  receives a single message from its only neighbor, . The incoming message was constructed using information about the state of  at , which in turn was obtained through the state of neighbors of  at  (including  itself). Thus, the context of  at iteration  includes  as well as the neighbors of . It is clear that, for this particular case, at iteration  the context of  would include all the nodes in the graph. Clearly, by iterating message passing, the nodes are able to acquire information from other nodes farther away in the graph.





*
In the literature, we distinguish three different approaches by which iterative context diffusion is implemented in practice, which we describe in the following.

Recursive Approaches
In the recursive approach to context diffusion, message passing is formulated as a dynamical system. In this case, the state transition function is recursive, meaning that . Practically speaking, the mutual dependencies between hidden states are modeled with a single recurrent layer, which is run indefinitely until convergence. Some well-known representatives of this paradigm are the Graph Neural Network scarselli2009gnn, the Graph Echo State Network gallicchio2010graphesn, and the more recent Fast and Deep Graph Neural Network gallicchio2020fastdeepgnn. To ensure convergence, these approaches impose contractive dynamics on the state transition function. While the Graph Neural Network enforces such constraints in the (supervised) loss function, the other two inherit convergence from the contractivity of (untrained) reservoir dynamics. Another example is the Gated Graph Neural Network li2016gatedgnn, where, differently from scarselli2009gnn, the number of iterations is fixed regardless of whether convergence is reached or not. Another approach based on collective inference, which adopts the same strategy but does not rely on any particular convergence criteria, has been introduced in macskassy2007classificationnetworkdata.

Feed-Forward Approaches
The feed-forward approach is based on stacking multiple layers to compose the local context learned at each message passing iteration. As a result, the mutual dependencies between the hidden states are handled separately via differently parameterized layers, without the need of constraints to ensure the convergence of the state transition function. In practice, the state transition function is no more recursive, but changes at every layer. Thus, in the feed-forward case, the symbol  indicates the layer that handles the corresponding message passing iteration. The effectiveness of the compositionality induced by the introduction of layers has been demonstrated in micheli2009nn4g, where it is shown formally that the context of a node increases as a function of the network depth, up to including all the other nodes in the graph. Feed-forward approaches are nowadays the main paradigm to design dgn, due to their simplicity, efficiency, and performance on many different tasks. However, deep networks for graphs suffer from the same gradient-related problems as other deep nn, especially when associated with an end-to-end learning process running through the whole architecture bengio1994learninglongtermdependenciesdifficult,li2018deeperinsightgraphconvsemisupervised. For the rest of this thesis, all the dgn used shall be feed-forward.

Constructive Approaches
Constructive approaches are a special case of feed-forward models, in which training is performed layer-wise. The major benefit of constructive architectures is that deep networks do not incur the vanishing/exploding gradient problem by design. In supervised scenarios, the constructive technique can learn the number of layers needed to solve a task fahlman1990cascor,marquez2018deepcascade,bianucci2000cascorchemistry. In other words, constructive dgn can determine automatically how much context is most beneficial to perform well, according to the specific task at hand. Another feature of constructive models is that they solve a problem in a divide-et-impera fashion, rather than using end-to-end training, by incrementally splitting the task into manageable sub-tasks. Each layer solves its own sub-problem, and subsequent layers use their results to improve further on their own, addressing the global task progressively. Among the constructive approaches, we mention the Neural Network for Graphs micheli2009nn4g, which was the first to propose a feed-forward architecture for graphs. Among recent models, another related approach which tackles the problem from a probabilistic point of view is the Contextual Graph Markov Model bacciu2018contgraphmarkov.

Building Blocks of Deep Graph Networks
dgn are built from several architectural components, which we cover in detail in this section. In short, a dgn can be decomposed into a collection of layers that process the graph structure, and a downstream predictor (either a classifier or a regressor) that computes a task-dependent output. The whole network is trained in an end-to-end fashion. In this section, we focus on the former components, the ones whose role is to carry out the processing of an input graph.

Graph Convolutional Layers
A gcl is essentially a neural network layer that performs message passing. The term convolutional is used to remark that the local processing performed by the state transition function is a generalization of the convolutional layer for images to graph domains with variable-size neighborhoods. Given an attributed graph  with  nodes, and its node attributes , one general formulation of a gcl is the following:

    
    v = U vAT(uu (v),  v g,

where  is the hidden state of the node at layer ,  is the hidden state of the node at the previous layer , and by convention . Notice that the neighborhood function  is also implicitly passed as input of the layer, so that the connectivity of each node is known. We can identify three key functions inside a gcl:

     is a transform function that applies some transformation to the hidden states of neighbors of node  at layer . This can be any function, either fixed or adaptive (implemented by a neural network);
     is an aggregation function that maps a multiset  of transformed neighbors of  to a unique neighborhood state vector. In practice,  is a permutation invariant function, meaning that its output does not change upon reordering of the arguments. For this reason, the computation of the neighborhood state vector is often referred to as neighborhood aggregation;
     is an update function that takes the hidden state of a node at layer  and the aggregated vector, and combines them to produce the new hidden state of the node at layer . Similarly to ,  can also be fixed or adaptive.

The usage of a permutation invariant function to compute the state of the neighbors is crucial, as it allows to acquire information from nearby nodes in a non-positional fashion, which is often the case with real-world graphs. From this general formulation, several implementations can be realized. As an example, we report the well-known formulation in kipf2017semisupervisedgcn, corresponding to the gcn model:

    
    v =  w _u (v) luvu v g,

where  is the entry of the symmetric normalized graph Laplacian  related to nodes  and , and:

    
    T(u&= tv= luvu
    
    Atvu (v) &= nv= _u (v) tv
    
    U(vnv&= v =  w nv

In this case, the aggregation function is the sum function. Other examples of permutation invariant functions used in practical contexts are the mean, the max, or other general functions which work on multisets zaheer2017deepsets. Notice that a gcl can be applied simultaneously to all the nodes in the graph, corresponding to visiting the graph nodes in parallel, with no predefined ordering. This contrasts with rnn and recnn, where parallelism in the state calculations is not possible or limited, respectively.

The generic gcl can be rewritten in matrix form as some variation of the following:

where  is a generic activation function,  is the adjacency matrix of the graph,  are the hidden states computed at layer  where by convention  is the matrix of node features, and  is the matrix of trainable layer-wise weights. Here, the adjacency matrix substitutes the neighborhood function , and the node adjacencies are inferred by its rows and columns. With this formulation, the gcl can be vectorized, which allows to run the state computation in fast hardware such as gpu.

Handling Edges
In certain tasks, including information about the edge features to the message passing algorithm can be beneficial to performances. Here, we describe how this can be achieved, focusing on the case where the edge features are discrete values out of a set of  possible choices. Specifically, given a graph , we assume a set of edge features of the form , with . To account for different edge types, two modifications to the message passing algorithm are required. One is to replace the standard neighborhood function in the aggregation function with the following edge-aware neighborhood function of a node :

which selects only neighbors of  with edge type . The other modification requires to change the update function for handling the different edge types. Taking again the gcn implementation as an example, Eq.  is modified as follows:
*
    v = _c C w_c _u _c(v) luvhv^(v g,

where the weight matrices  are now edge-specific, so that the contributions of the different edge types are weighted adaptively micheli2009nn4g,schlichtkrull2018relationaldatagcn. In practice, the above procedure corresponds to performing  different aggregations weighted separately to compute the state of the node. Other approaches to include edge information in the message passing scheme require to extend the transform function, such that the edges between the processed node and its neighbors are included in the transformation (for example, by concatenating the edge feature to the hidden state vector).

Node Attention
Attention mechanisms bahdanau2015attention are a widely used tool in Deep Learning to get importance scores out of arbitrary sets of items. Thus, they are naturally applicable within the dgn framework to measure the contribution of the different nodes during neighborhood aggregation. Specifically, to introduce attention mechanisms in the neighborhood aggregation, we weigh the contribution of the transformed nodes in the neighborhood by a scalar , called attention score as follows:

The attention scores are derived from attention coefficients , which are essentially similarity scores between the neighbor and the current node, calculated as follows:

where  is an arbitrary function (generally a neural network). Different attention mechanisms are defined based on how  is implemented. Finally, the coefficients are normalized into attention scores by a softmax function, effectively defining a probability distribution among them. The attention mechanism can be generalized to multi-head attention, where multiple attention scores for each node are calculated and concatenated together to obtain an attention vector, rather than a score. Figure  shows an example of attention computed on an example graph. We remark that node attention is unrelated to weighting the connection between nodes, which is an operation that involves the edge features. Here, similarity between nodes is calculated relying solely on the hidden states of the involved node and its neighbors.






*

Node Sampling
Node sampling is a technique used when learning on large graphs to ensure computational efficiency. When the number of nodes in a graph is large, and nodes are densely connected among themselves, computing neighborhood aggregation may become very expensive or even intractable. The most straightforward method to address this issue is to randomly sample a predefined number of nodes to aggregate, rather than using the whole neighborhood. This basic strategy can be refined by using more sophisticated techniques such as importance sampling gallicchio2020fastdeepgnn, or even extended to sampling a bounded number of nodes which are not necessarily in the immediate neighborhood of the current node hamilton2017graphsage. The latter requires to add fictitious edges between the current node and nodes at farther distances, in order to treat them as standard neighbors. This way, global information about the graph can be incorporated more directly, as compared to message passing.






*

Readout Layers
As we have seen, the application of  dgn layers to a graph  yields  hidden states per node, each one composed with a progressively broad context. In node classification or regression tasks, these are combined by a hidden state readout function to obtain a unique hidden state to use as input of the output function, which emits a prediction for every node. Specifically, a hidden state readout function  computes a node representation (or node embeddings)  for each node as follows:

Notice that, when aggregating hidden states, one can exploit the fact that the number of layers is fixed beforehand in feed-forward dgn architectures, and that the hidden states are ordered depth-wise. Thus, the aggregation need not to be permutation-invariant. Usual choices of  include concatenation, weighted average (where the mixing weights can also be learned), rnns, or just selecting the hidden state at the last layer. The node representations are then fed to an output layer or downstream network, which computes node-wise outputs:

where  and  can be any arbitrarily complex neural network as usual. A visual example of the process for a single node is shown in Figure .





*
In graph classification or regression tasks, the node representations computed by a node readout function are aggregated once more by a graph readout function, to compute a graph representation (or graph embedding) , a vector representing the entire graph. Differently from the hidden state readout, the readout function must necessarily be permutation-invariant, since there are no guarantees about the number of graph nodes. Specifically, a graph readout function  computes the embedding of graph  as follows:

Typical readouts for dgn include simple functions such sum, mean, max, or more complex aggregators such as deep sets models zaheer2017deepsets. Finally, the graph embedding is fed to an output layer or a downstream network to compute the associated output:






*
An graph readout applied to an example graph is shown in Figure .

Graph Pooling Layers
Similarly to the layer used by cnn for computer vision, pooling is also applicable to dgn for graph classification (or regression) tasks. In dgn architectures, pooling is usually placed after a graph convolutional layer, and serves a three-fold purpose: it is used to detect communities in the graph, clusters of nodes with very higher connectivity among themselves than the rest of the graph; to augment the information content of the hidden states with this knowledge; and to reduce the number of nodes (and consequently, the number of parameters) needed by the network in later stages of computation. An example of a graph pooling layer is shown in Figure , where nearby nodes are pooled into a single node in the reduced graph according to some strategy. Graph pooling methods are developed according to two strategies: adaptive and tolopogical. Adaptive methods pool nodes in a differentiable manner, so that the optimal clustering of the nodes for the task at hand is learned by the end-to-end network. One example of adaptive pooling is DiffPool, developed in ying2018diffpool. Given a graph  with  nodes, and assuming the  gcl have been applied, DiffPool computes two matrices:
*
    z&= DGN_e(g^n h

    s&= DGN_p(g^n k,

where  and  is a stack of graph convolutional layers. The matrix  computes a soft-assignment to each node to one of  clusters with a softmax output function. These two matrices are then combined with the current hidden states to produce a novel adjacency matrix and its corresponding matrix of hidden states as follows:
*
    g &= sz^k h

     &= ss^k k

where  and . Thus, after applying the DiffPool layer, the size of the graph is reduced progressively from  to  nodes.






*
Topological pooling, on the other hand,  uses not-differentiable methods which leverage the global structure of the graph, and the communities beneath it. These methods work by grouping nodes according to well known graph theory tools, such as spectral clustering (see vonluxburg2007tutorialspectralclustering, dhillon2007weightedgraphcuts).

Regularization
dgn are trained with regular losses, such as ce for classification and mse for regression. Besides standard regularization techniques, the objective function is often regularized through unsupervised loss functions, which impose priors on which kinds of
structures the network should preferably learn. The regularized objective function for supervised tasks  has the form:

were  is a regularization function weighted by a regularization coefficient , that is applied at each layer  to the set of hidden states of the nodes, represented as a matrix . An example of regularization widely employed in practical settings is the link prediction unsupervised loss, defined as:

    (g) = _u,v v - u_2,

where the summation ranges over all possible combinations of nodes. Basically, when this loss is minimized, it biases the network towards producing node representations that are more similar for nodes connected by an edge. Notably, this loss can be also used in isolation to tackle link prediction tasks, tasks where the downstream network must predict unseen links between the nodes.
The Evaluation of Deep Graph Networks and Applications to Computational Biology
The Evaluation of Deep Graph Networks 
Over the years, dgn have yielded strong performances on several predictive tasks, becoming the learning tool for graph-related problems. Given their appeal, several dgn architectures have been developed recently. These architectures require a thorough evaluation to understand which one is better suited for a certain task. The evaluation requires both an extensive model selection phase, to select appropriate hyper-parameters, as well as a model evaluation phase to obtain an estimation of the generalization ability of the network. In the literature, the evaluation of dgn is carried out on a variety of benchmark datasets, generally from the chemistry and social sciences domains, where graphs are used to represent molecules and social networks, respectively. However, as pointed out by researchers lipton2018troubling, the papers that introduce novel architectures often adopt not reproducible or unfair experimental setups, which make the comparisons among models unreliable. In this section, we present three contributions related to address this important issue. Specifically, we:

    provide a rigorous evaluation of existing dgn models in the context of graph classification, using a standardized and reproducible experimental environment. Specifically, we perform a large number of experiments within a rigorous model selection and assessment framework, in which all models are compared using the same node features and data splits;
    investigate if and to what extent current dgn models can effectively exploit graph structure on the evaluation benchmarks. To this end, we also evaluate two domain-specific structure-agnostic baselines, whose purpose is to disentangle the contribution of structural information from node features;
    study the effect of node degrees as features in social datasets. We show that adding node degrees to the node features can be beneficial, and it has implications as to how many convolutional layers are needed to obtain good performances.


Datasets
All  graph  datasets  are  publicly  available kersting2016benchmark  and  represent  a  relevant subset of those most frequently used in literature to compare dgn. Some collect molecular graphs, while others contain social graphs. Specifically, we use the following chemical datasets:

    DD dobson2003dd is a graph dataset in which nodes are amino acids, and there is an edge between two nodes if they are they are neighbors in the amino-acid sequence or in 3D space. The task is a binary classification one, where the objective is to determine whether a graph represents an enzyme or non-enzyme;
    PROTEINS borgwardt2005proteins is a subset of DD where the largest graphs have been removed;
    NCI1 wale2008nci1 is a dataset made of chemical compounds screened for ability to suppress or inhibit the growth of a panel of human tumor cell lines. The task is a binary classification one, where the objective is to determine if a chemical compound acts as suppressor or inhibitor;
    ENZYMES is a dataset of enzymes taken from the BRENDA enzyme database schomburg2004enzymes. In this case, the task is to correctly assign each enzyme to one out of 6 Enzyme Commission (EC) numbers.

As regards datasets containing social graphs, we use the following:

    IMDB-BINARY and IMDB-MULTI yanardag2015imdbredditcollab are movie collaboration datasets. Each graph is an ego-network where nodes are actors or actresses, and edges connect two actors/actresses which star in the same movie. Each graph has been extracted from a pre-specified genre of movies, and the task is to classify the genre graph the ego-network is derived from.
    REDDIT-5K yanardag2015imdbredditcollab is a dataset where each graph represents an online thread on the Reddit platform, and nodes correspond to users. Two nodes are connected by an edge if at least one of the two users commented each other on the thread. The task is to classify each graph to a corresponding community (a sub-reddit);
    COLLAB yanardag2015imdbredditcollab is a dataset where each graph is an ego-network of different researchers from some research field. There is an edge between two authors if they coauthored a scientific article. The task is to classify each ego-network to the corresponding field of research.

All node features are discrete (we shall refer to them as node labels equivalently). The only exception is the ENZYMES dataset, which also has an additional 18 continuous features. The social datasets do not have node features. In this case, we use either an uninformative label for all nodes, or the node degree. Specifically, social datasets are used to understand whether the models are able to learn structural features on their own or not. The statistics of the datasets, which include number of graphs, number of target classes, average number of nodes per graph, average number of nodes per graph, and number of node labels (if any) are reported in Table .


























Architectures
In total, we choose five different dgn architectures. The high-level structure of the dgn comprises an input layer, a stack of one or more gcl, a readout layer and a final MLP classifier, which maps the graph embedding to the dataset-dependent output. Following, we describe in detail the kinds of gcl used by the dgn, and the specific hyper-parameters optimized for each of them.

Graph Isomorphism Network The gin convolutional layer by  is implemented as follows:

where  is a trainable parameter, and MLP is a neural network with 2 hidden layers, each one consisting of a linear transformation, followed by a BatchNorm layer and a relu non-linearity. The first layer of the dgn is not a gcl, but a graph readout that sums the node features, and passes them through an MLP identical to the one just described. The graph embedding obtained by this layer is then summed to the graph embeddings computed by the gcl. For this architecture, we choose the following hyper-parameters: number of layers, size of the hidden state at each layer, and type of readout function (for the first non-convolutional layer as well).

GraphSAGE
The Graph SAmple and aggreGatE (GraphSAGE) layer by  is implemented as follows:

Notice that the original formulation also implements a form of node sampling, since it is applied to large graphs. Here, since the size of the graph we deal with is manageable, we do not implement node sampling.

GraphSAGE + DiffPool
In order to have a representative dgn with pooling, we tested a variant of GraphSAGE, where a DiffPool layer is added to the network after every GraphSAGE convolution. Internally, the DiffPool layer is implemented as in Section , where  and  are a stack of 3 GraphSAGE layers respectively. Thus, one DiffPool layer requires to compute 6 graph convolutions. The number of clusters  is deterministic, and is obtained as , where  is the maximum number of nodes among the graphs in the dataset, and  is a coarsening factor which is 0.1 if only one DiffPool layer is applied, and 0.25 otherwise.

ECC
The Edge-Conditioned Convolutional layer by  is implemented as follows:

where MLP is a neural network that computes a weight between the hidden state of the current node  and its generic neighbor , using the edge feature vector as input, composed of one hidden layer with ReLU non-linearities and a subsequent linear output layer. Each convolutional layer of the corresponding dgn is composed of a stack of three ECC layers.

DGCNN
The graph convolutional layer of the Deep Graph Convolutional Neural Network (DGCNN) zhang2018dgcnn, is the following:

where  is the inverse degree matrix. After all the convolutional layers are applied, the computation is followed by SortPool layer, which performs graph pooling, and a final 1-D cnn.

Baselines
We compare the proposed architectures with two structure-agnostic baselines, one for molecular graphs, and one for social graphs. For molecular graphs, with the exception of graphs in the ENZYMES dataset, we use an mlp-based model ralaivola2005graphkernels, applied as follows to a generic graph :

This architecture corresponds to summing the node features together (which are one-hot encoded vectors representing the atom types), and applying an mlp with 2 hidden layers with relu non-linearities on top of them. In practice, the network counts the atom occurrences for each atom type, and computes non-linear transformation of this sum. Notice that the graph connectivity is not taken into account by the model.
For the social graphs, and for the graphs in the ENZYMES dataset (which uses 18 additional node features with respect to the other molecular datasets), we use the following architecture, applied to a generic graph  as follows:

where  is a linear layer plus a relu non-linearity, and  is a two hidden layer mlp with relu non linearities zaheer2017deepsets. Notice that even in this case we do not take into account the graph connectivity. The role of these baselines is to provide feedback on the effectiveness of dgn on a specific dataset. Specifically, if a dgn performs similarly to a structure-agnostic baseline, one can draw two possible conclusions: either the task does not need structural information to be effectively solved, or the dgn is not exploiting graph structure adequately. While the former can be verified through domain-specific human expertise, the second is more difficult to assess, as multiple factors come into play such as the size of the training dataset, the structural inductive bias imposed by the architecture and the selected hyper-parameters. Nevertheless, a significant improvement with respect to a baseline is a strong indicator that graph structure has been exploited.

Experimental Setup
In all our experiments, we use classification accuracy (the percentage of correctly classified graphs out of the total number of predictions) as performance metric.
Our evaluation pipeline consists in an outer -fold CV for model assessment, and an inner holdout technique with a 90/10 training/validation split for model selection. After each model selection, we train the winning model three times on the whole training fold, holding out a random fraction (10) of the data to perform early stopping. We do this in order to contrast the effect of unfavorable random weight initialization on test performances. The final score for each test fold is obtained as the average of these three runs. Importantly, all data splits have been precomputed, so that models are selected and evaluated on the same data partitions; this guarantees consistency in the evaluation. For the same reason, we also stratify all the class labels, so that the classes proportions are preserved inside each -fold split, as well as in the internal holdout splits. With respect to the general dgn architecture, we optimize learning rate and early stopping patience for every considered dgn. All models are trained with the Adam kingma2015adam optimizer with learning rate decay.

Hyper-Parameters
For all the dgn architectures, we tune the size of the hidden state of the convolutional layers and the number of layers. We keep the number of configurations roughly equal across all the tested models. The baselines are composed of a single layer, hence we only tune the hidden size. Besides architecture-specific hyper-parameters, we also tune others that are shared across all models, related to the training procedure. Specifically, for each model under evaluation, we optimize learning rate and learning rate decay. For GIN, we also optimize batch size. For the two baselines, we also optimize the L2 regularization factor. To be consistent with the literature, we implement early stopping with patience, where training stops if a number epochs have passed without improvement on the validation set. A high patience can favor model selection by making it less sensitive to fluctuations in the validation score at the cost of additional computation. The patience hyper-parameter is optimized for all the considered models. A table showing the complete grid of hyper-parameters we used for each dgn under evaluation is reported in Appendix .

Computational Considerations
The experiments required an extensive computational effort. For all models, the sizes of the hyper-parameter grids range from 32 to 72 possible configurations, depending on the number of hyper-parameters to choose from. The total number of single training runs to complete model assessment exceeds 47000. Such a large number requires an extensive use of parallelism, both in CPU and GPU, to conduct the experiments in a reasonable amount of time. In some cases (e.g. ECC in social datasets), training on a single hyper-parameter configuration required more than 72 hours; consequently, the sequential exploration of one single grid would last months. For this reason, we limit the time to complete a single training to 72 hours.

Results
The experimental results are reported in Table  (for the chemical datasets) and Table  (for the social datasets). We notice an interesting trend on the DD, PROTEINS and ENZYMES datasets, where none of the dgn are able to improve over the baseline. Conversely, on the NCI1 dataset, the baseline is clearly outperformed: this result suggests that on this dataset, these dgn architectures are suited to exploit the structural information of the training graphs. This result is reinforced from empirical evidence: in fact, we observed in preliminary trials (not reported here) that an overly-parameterized baseline with 10000 hidden units and no regularization is not able to overfit the NCI1 training data completely, reaching around 67 training accuracy, while a model such as GIN can easily overfit ( accuracy) the training data. This indicates that, at least for the NCI1 dataset, the structural information hugely affects the ability to fit the training set. On social datasets, GIN seems to be the most performant model, reaching the best accuracy in three out of five datasets. However, in both chemical and social scenarios, the standard deviations are so large that all judgements about which model is better are speculative. Following, we summarize other relevant findings of our study.




















The Importance of Baselines
Our results confirm that structure-agnostic baselines are an essential tool to evaluate dgn under a clear perspective, as well as to extract useful insights on whether structure has been exploited. As an example, consider how none of the dgn surpasses the baseline on DD, PROTEINS and ENZYMES; based on this result, we argue that the state-of-the-art dgn we analyzed are not able to fully exploit the structure on such datasets yet. This contrasts with the current literature of chemistry, where structural properties of the molecular graph are strongly believed to correlate with molecular properties vanrossum1965chemical. For this reason, we suggest not to over-emphasize small performance gains on these datasets. Currently, it is more likely that small fluctuations in performances are likely to be caused by other factors, such as random initializations, rather than a successful exploitation of the structure. In conclusion, we warmly recommend dgn practitioners to include baseline comparisons in future works, in order to better characterize the extent of their contributions.


































The Effect of Node Degree
Based on our results, adding the node degree to the input features almost always results in a performance improvement, sometimes very strong, on social datasets. For example, adding the degree information to the baseline improves performances of  across all datasets, up to being competitive with the examined dgn. In particular, the baseline achieves the best performance on IMDB-MULTI, and performs very close to the best model (GIN) on IMDB-BINARY. In contrast, the addition of degree information is less impacting for most dgn. This result is somewhat expected, since they are supposed to automatically extract the degree information from the structure. One notable exception to this trend is DGCNN, which explicitly needs the addition of node degrees to perform well across all datasets. We observe that the ranking of the models, after the addition of the degrees, drastically changes; this raises the question about the impact of other structural features (such as clustering coefficient) on performances, which we leave to future works. In a further experiment, we reason about whether the degree has an influence on the number of layers that are necessary to solve the task. We investigate the matter by computing the median number of layers of the winning model in each of the 10 different folds. These results are shown in Table . Given the benefit given by the addition of the node degree feature, we hypothesize that models such as DGCNN learn features correlated to the node degrees in the very first layers; this learned information helps to perform well in the tasks using fewer convolutional layers.
























Comparison with Published Results
Figure  compares the average values of our test results (shown with a square marker) to those reported in the literature (shown with a triangle marker). In addition, we plot the average of our validation results across the 10 different model selections (shown with a circle marker). From the plot, it is clear that our results are in most cases different from published results, and the gap between the two estimates is usually consistent. Moreover, and differently from results in the literature, our average validation accuracies are consistently similar to the test accuracies, which indicates that our estimates are less biased in general. We emphasize that our results are i) obtained within the rigorous model selection and evaluation framework; ii) fair in terms of how the data was split, and which features have been used for all competitors; iii) reproducible .








Conclusions
In this section, we showed how a rigorous empirical evaluation of dgn can help to design better experiments, and to draw more informed conclusions as regards the potential impact of novel architectures. This has been possible by introducing a clear and reproducible environment for benchmarking current and future dgn architectures, as well as with reliable and reproducible results to which dgn practitioners can test novel architectures. This work will hopefully prove useful to researchers and practitioners that want to compare dgn in a more rigorous way.

Prediction of Dynamical Properties of Biochemical Pathways using Deep Graph Networks

In this chapter, we present an application of Deep Learning techniques on graphs to a life sciences problem related to computational biology. Specifically, we apply Deep Graph Networks to process biochemical pathways, dynamical systems that model the complex interactions between molecules at the biochemical level. Biochemical pathways can be represented as a particular form of bipartite graphs known as Petri networks, which allow to study several properties of such systems. Here, we focus on the property of concentration robustness. To be measured, concentration robustness requires to perform time-expensive simulations. Here, we opt for an approximate but reliable solution, which is orders of magnitude faster to compute. Through processing of the Petri network associated to the biochemical pathway with Deep Graph Networks, we show experimentally that it is possible to build a model that predicts concentration robustness rapidly and with a satisfactory level of accuracy.

Introduction and Motivation
In order to understand the mechanisms underlying the functioning of living cells, it is necessary to analyze their activities at the biochemical level. Biochemical pathways (or pathways, in short) are complex dynamical systems in which molecules interact with each other through chemical reactions. In these reactions, molecules can take the role of reactant, product, promoter or inhibitor. The dynamics of a pathway are determined by the variation over time of the concentration of its molecules. To study these dynamics, two methodologies are traditionally employed. One consists in modelling the pathway as a system of ode, derived from the application of chemical kinetics laws such as the law of mass action. In cases where pathways involve molecules available in small concentrations, which make the dynamics of reactions sensitive to random events, stochastic modelling and simulation approaches are preferred. These are usually variants of the well-known Gillespie's simulation algorithm gillespie1977exact. The use of these modelling tools allows to investigate dynamical properties of biochemical pathways such as the reachability of steady states, the occurrence of oscillatory behaviors, causalities between molecular species, and robustness. However, quantitatively measuring these properties often requires to execute a large number of numerical or stochastic simulation, which in turn are time-consuming and computationally intensive.

Given their nature, one widely used formalism to represent biochemical pathways is that of graphs. Many different graphical notations of pathways exist in the literature (see, e.g., karp1994representations,reddy1993petri,le2009systems), most of which represent molecules as nodes, and reactions as multi-edges or as additional nodes. Using graphs to represent pathways is convenient for three main reasons. Firstly, they provide a quite natural visual representation of the reactions occurring in the pathway. Secondly, they enable the study of the pathway dynamics through methods such as network and structural analysis. Thirdly, graphs can easily be transformed into ode or stochastic models, to apply standard numerical simulation techniques.

In this study, we investigate whether predicting dynamical properties of biochemical pathways from the structure of their associated graphs is possible; and if so, to what extent. In other words, our main assumption is that the dynamics of the biological system modeled by the pathway can be correlated to the structural properties of the graph by which it is represented. If the assumption is correct, the positive implications are two-fold: on one hand, a good predictive model of desired biochemical properties could, in principle, replace numerical or stochastic simulations whenever time and computational budgets are limited. On the other hand, it could allow to predict the properties even in cases where the quantitative information is not available, for example whenever numerical or stochastic simulation methods cannot be applied.

The main idea behind this work is to use of Deep Graph Networks to learn structural features of pathways represented as Petri networks (or Petri nets, in short), which are used to predict a property of interest. Here, we focus on the assessment of the dynamical property of robustness, defined as the the ability of a pathway to preserve its dynamics despite the perturbation of some parameters or initial conditions. More specifically, given a pathway and a pair of molecular species (called input and output species), the robustness measures how much the concentration of the output species at the steady state is influenced by perturbations of the initial concentration of the input species. This is a notion of concentration robustness kitano2004biological which is to some extent correlated with the notion of global sensitivity zi2011sensitivity. Robustness makes up for a perfect candidate to test our approach, as its assessment is time-consuming and computationally intensive, requiring a huge number of simulations to explore the parameters space.

The initial part of this work focuses on the creation of a dataset suited to train the dgn. We start from collecting 706 curated pathway models in SBML format from the BioModels  database li2010biomodels, which were initially converted into Petri nets. For every pathway in this initial dataset, the robustness of every possible pair of input and output species has been computed through ode-based simulations. Then, these robustness values have been transformed into binary indicators of whether robustness holds for a given pathway and input/output species. Lastly, for each pathway and for each input/output species in that pathway, the induced subgraphs containing the input and output nodes (as well as other nodes that influence the pathway dynamics) have been extracted. To summarize, the final dataset obtained with this preparatory phase consists of a set of subgraphs, each associated to a pair of input/output molecular species, and their respective robustness indicator. The predictive task is thus one of binary classification: specifically, given a subgraph and two nodes corresponding to the input and output species, the model should correctly classify them as robust or not.

We model the task with a dgn to learn structural features from the subgraphs and compute a graph embedding that is passed to a mlp classifier. The performances of the model are assessed according to a rigorous framework similar to the one developed in . Our experimental results show that we are indeed able to predict robustness with reasonable accuracy. We also conduct a follow-up investigation of how the architectural choices, such as type of graph convolutional layer and number of layer, impact performances. The analysis suggests that the depth of the dgn, in terms of number of layers, plays an important role in capturing the right features that correlate the subgraph structure to the robustness, and that deep dgn perform better at this task.

To our knowledge, this is the first work that addresses the problem of predicting dynamical, properties of pathways on a large scale using Deep Learning. In contrast, other approaches in the literature mainly focus on inferring the parameters of a single pathway, or the relationships between its species. We believe this work has great potential in helping understand the functioning of living cells, by serving as a fast, and computationally friendlier, alternative to performing expensive simulations in the assessment of pathway properties.

Background
In this section, we provide the necessary formal background to understand the modeling of biochemical pathways with Petri nets, and the dynamical property of concentration robustness.

Pathway Petri Nets
Biochemical pathways are essentially sets of chemical reactions of the form:





where  are molecules (reactants and products, respectively),  are stoichiometric coefficients expressing the multiplicities of reactants and products involved in the reaction, and  is the kinetic constant, used to compute the reaction rate according to standard chemical kinetic laws such as the law of mass action. Besides reactants and products, the reactions of a biochemical pathway often include in their description other molecules, called modifiers. These are not consumed nor produced by the reaction, but act either as promoters or as inhibitors, meaning that they can increase or decrease the reaction rate, respectively. Although these molecules are not listed among reactants and products, they do have a role in the kinetic formula, which no longer follows the mass action principle in this case. For example, in the SBML language hucka2018sbml, a standard XML-based modeling language for biochemical pathways, reactions can be associated with a number of modifiers, whose concentration is used in the kinetic formula of the reaction. In Figure  we show a table describing the set of reactions describing a biochemical pathway (first column), some of which include a modifier (second column), namely  for the third reaction, and  for the sixth. Each reaction is associated with its kinetic formula (third column), that, for simplicity, we reference through an alias of the form  with . Using the kinetic formulas of the two reactions with modifiers as an example, it is clear that  acts as a promoter (meaning that the reaction rate is proportional to the concentration of ) and that  acts as inhibitor (meaning that the reaction rate is inversely proportional to the concentration of ). Kinetic formulas can then be used to construct a system of ode as shown in Figure .






































A common way to represent biochemical pathways is through Petri nets. The formalism of Petri nets have been originally proposed for the description and analysis of concurrent systems peterson1977petri, but has been later adopted to model other kinds of systems, such as biological ones. Several variants of Petri nets have been proposed in the literature. In this work, we consider a version of continuous Petri nets gilbert2007petri with promotion and inhibition edges and general kinetic functions. We call this biologically inspired variant ppn. A ppn is essentially a bipartite graph with two types of nodes and three types of labelled edges. According to standard Petri nets terminology, the two types of nodes are called places and transitions. The semantics of a ppn in a continuous setting are described by a system of ODEs, with one equation for each place. In the case of pathways, such system corresponds exactly to the one obtained from the chemical reactions shown in Figure . The state of a ppn (called marking) is then defined as an assignment of positive real values to the variables of the ODEs. We denote with  the set of all possible markings.

More formally, a ppn can be defined as a tuple  where:

     and  are finite, non empty disjoint sets of places and transitions, respectively;
     is a set of standard directed edges;
     is the set of promotion edges;
     is the set of inhibition edges;
     weights every standard edge by non-negative integer values;
    ,  is a function that assigns, to each transition, a function that computes a kinetic formula to every possible marking ;
     is the initial marking.

A visual representation of the ppn corresponding to the pathway in Figure  is shown in Figure . The sets of places  and transitions  of a pathway Petri net represent molecular species and reactants, and are displayed as circles and rectangles, respectively. In the figure, places are labeled with the name of the corresponding molecule. The directed edges, depicted as standard arrows, connect reactants to reactions and reactions to products. The weights of the edges (omitted if equal to one) correspond to the stoichiometric coefficients of reactant/product pairs. The sets of promotion and inhibition edges,  and , connect molecules to the reactions they promote or inhibit, respectively, and they are displayed as dotted or T-shaped arrows, respectively. The kinetic formulas of reactions (or rather, their aliases defined as in Figure ), are shown inside the rectangles of the corresponding transitions. As explained previously, molecules connected through promotion edges give a positive contribution to the value of the kinetic formula, while molecules connected through inhibition edges give a negative (inversely proportional) contribution. Finally, the initial marking  is not shown in the figure, and it has to be described separately.





*

In this work, we use a variant of ppn where all the information unrelated to the structure of the pathway is discarded. Specifically, we ignore information about:

    kinetic formulas;
    multiplicities of reactants and products (edge labels);
    the initial marking .

Basically, for our purposes, a ppn is a tuple  where the irrelevant components have been omitted. We rewrite this object, according to the graph notation introduced in Section , into a pathway graph . To do so, we first define the following nodes and edges subsets:

     is the set of molecules;
     is the set of reactions;
     is the set of standard edges;
     is the set of promoter edges;
     is the set of inhibitor edges,

where  and . Then, we simply set  and .
Using the biochemical pathway of Figure  as reference, its associated pathway graph is shown in Figure .





*
More explicitly, the set of nodes of the pathway graph contains both molecules and reactions. The set of edges of the graph contains an edge (of any type) from a molecule to a reaction if and only if a perturbation in the concentration of the molecule determines a change in the reaction rate (that could in principle be computed through the omitted kinetic formula). Similarly, it contains an edge from a reaction to a molecule if and only if a perturbation in the reaction rate determines a change in the dynamics of the concentration of that molecule. This is intuitive for those molecular species that are products, as the dynamics of the product accumulation is determined by the reaction rate. By construction, a pathway graph is bipartite.

Concentration Robustness
Robustness is defined as the ability of a system to maintain its functionalities again external and internal perturbations kitano2004biological, a property observed in many biological systems. A general notion of robustness has been formalized by Kitano in . This formalization considers a specific functionality of a system and the viability of such functionality, defined as the ability of the system (a cell) to carry it out. This could be expressed, for instance, in terms of the synthesis/degradation rate or concentration level of some target substance, in terms of cell growth rate, or in terms of  other suitable quantitative indicators. More specifically, according to Kitano's definition, the robustness  of a system , with respect to a specific functionality  and against a set of perturbations  is expressed as:



In the above definition,  is the probability a perturbation , and  evaluates the functionality  of the system  under the perturbation . More precisely, function  gives the viability of  under perturbation , in relation to the viability of  under normal conditions. In the absence of perturbations, , meaning that the functionality  is assumed to be carried out in an optimal way, or equivalently, that perturbations have irrelevant or no influence; conversely,  if perturbations cause the system to fail completely in performing , and  in the case of relevant perturbations.

An improvement to Kitano's formulation of robustness has been proposed in rizk2009general, where functionalities to be maintained are described as linear temporal logic (LTL) formulas. In this formulation, the impact of perturbations is quantified through a notion of violation degree, which measures the distance between the dynamics of the perturbed system and the LTL formula. Many more specific definitions exist, differing either in the class of biological systems they apply to, or in the way the functionality to be maintained is expressed larhlimi2011robustness.

In the case of biochemical pathways, a common formulation of robustness can be expressed in terms of maintenance of the concentration levels of some species. This definition can be reduced to both general formulations in kitano2007towards and . In particular, the absolute concentration robustness proposed in  is based on the comparison of the concentration level of given species at the steady state, against perturbations (either in the kinetic parameters or in the initial concentrations) of some other species.

A generalization of absolute concentration robustness, called -robustness, has been proposed in nasti2018formalizing, where concentration intervals are introduced both for the perturbed molecules (input species) and for the molecules whose concentration is maintained (output species). Informally speaking, a biochemical pathway is -robust with respect to a given set of initial concentration intervals, if the concentration of a chosen output molecule at the steady state lies in the interval  for some . A relative version of -robustness can be obtained simply by dividing  by . This notion of -robustness is related to the notion of global sensitivity zi2011sensitivity, which typically measures the average effect of a set of perturbations. Hereafter, we use the term robustness to specifically refer to -robustness for brevity.

The assessment of robustness usually requires to perform exhaustive numerical simulations in parameter space rizk2009general,iooss2015review (where by parameters we intend mainly the kinetic parameters, or the initial concentrations). In some particular cases, one can exploit the biological network structure to avoid performing simulations altogether shinar2010structural. Moreover, in cases where the dynamics of the network are monotonic, the number of such simulations can be significantly reduced gori2019towards.

Methods
Here, we provide details about how the raw biological pathways have been converted into the dataset of graphs on which the dgn has been trained.

Subgraphs Extraction
As explained in Section , concentration robustness is defined in terms of pathway, and a pair of input and output molecules. However, for a fixed choice of input and output, not all the nodes in a pathway graph contribute to the assessment, but only a specific subset corresponding to an induced subgraph. In other words, given an input/output pair, this subgraph corresponds to the portion of the pathway that is relevant for the assessment of the robustness. The idea is to extract these subgraphs for each pathway graph and for each possible input/output pair of molecules. In order to do so, let us first introduce a helper data structure which we call enriched pathway graph. Given a pathway graph , its enriched version  is defined as follows: initially, , . Then, for every standard edge  where  and , we update the standard edges of  with a reverse standard edge , setting . Note that we do not reverse neither standard edges from reactions to molecules, nor promotion and inhibition edges. The enriched pathway graph obtained from the pathway graph of Figure  is shown in Figure , where the additional edges are drawn in solid black. Such graph represents influence relationships between molecules and reactions. The reversed edges encode the fact that a perturbation in the reaction rates determines a variation in the reactants consumption. Hence, the enriched pathway graph essentially corresponds to the influence graph that could be computed from the Jacobian matrix containing the partial derivatives of the system of ODEs associated to the pathway fages2008influencegraph.





*
In practice, the nodes defining the subgraphs are identified through . More specifically, given , , and a pair of nodes , we define the subgraph induced by  in  as the graph  where  and . In practice,  is a subgraph of  where the node set contains , , as well as nodes in every possible oriented path from  to  in . Notice that  is a subgraph of , although its node set is computed on the basis of the paths in . Figure  shows some examples of induced subgraphs extracted from the graph in Figure . Notice how, in the subgraph of Figure , the node D (below node ) is included in the subgraph because, even though there is no oriented path between them in the pathway graph, there is one in the enriched graph (see Figure ).



















*

Robustness Computation
The robustness values used to label the induced subgraphs are computed following the relative -robustness approach. The dynamics of each biochemical pathway has been simulated by applying a numerical solver (the libRoadRunner Python library by ) to its ODEs representation. Reference initial concentrations of involved molecules have been obtained from the original SBML model of each pathway. Moreover, 100 simulations have been performed for each molecule, by perturbing its initial concentration in the range . The termination of each simulation has been set to the achievement of the steady state, with a timeout of 250 simulated time units.  Given a subgraph , we computed the width  of the range of concentrations reached by the output molecules  by varying the input  (as per definition of -robustness). A relative robustness  has then been obtained by dividing  by the concentration reached by the output when the initial concentration of the input is the reference one (no perturbation). The final robustness value  has been computed by comparing  (a relative representation of the output range) with  (a relative representation of the initial input range, that is ) as follows:




Data Preprocessing
Our initial data collection consisted of 706 SBML models of biochemical pathways, downloaded from the BioModels database . Specifically, the data correspond to the complete set of manually curated models present in the database at the time we started the construction of the dataset . We removed empty models (not containing any node) and discarded duplicates, reducing the number of SBML models to 484. These models were first transformed into ppn representations and saved in DOT format . For the translation of the SMBL models into ppn, we developed a Python script that, for each reaction in the SMBL model extracts reactants, products and modifiers. Furthermore, it also checks the kinetic formula in order to determine whether each modifier is either promoter or an inhibitor. With the conversion of each ppn into a pathway graph, we obtained a collection  of pathway graphs. Each of these pathway graphs has been transformed into a set containing one induced subgraph (whose size does not exceed 100 nodes, for computational reasons) for every possible input/output combination of pathway graph nodes representing molecules, according to the procedure detailed in Section . For each induced subgraph, the associated robustness has been calculated as explained in Section . The result of this preprocessing is a training dataset:

where  are robustness indicators used as the labels for the associated binary classification task. The total number of subgraphs in the preprocessed dataset is 44928.

Subgraph Features
We encoded information such as node type, edge type, and input/output pair of the induced subgraphs in their node and feature vectors. Specifically, for each subgraph node, we associate a binary 3-dimensional feature vector which encodes whether the node is a molecule or a reaction, whether the node is an input node or not, and whether a node. Given an induced subgraph  and one of its nodes , we define its 3-dimensional vector of node features  component by component as follows:
*
    xv, 1 = 
        1  & if v p

        0  & if v p,
    
    
    xv, 2 = 
        1  & if v = v_I

        0  & otherwise,
    
    
    xv, 3 = 
        1  & if v = v_O

        0  & otherwise,
    

where the notation  indicates the -th component of the node feature vector .
Similarly, for each subgraph edge, we encode its edge type as a one-hot vector out of the three possibilities (standard, promoter or inhibitor). Given an edge , we define its 3-dimensional vector of edge features  component by component as follows:
*
    eu, v, 1 = (u,v) p, 
    eu, v, 2 = (u,v) p, 
    eu, v, 3 = (u,v) p,

where  is the indicator function, and the notation  identifies the -th component of the edge feature vector . In practice, the edge features encode the edge type as a one-hot vector. Figure  shows the induced subgraph of Figure  with the corresponding feature vectors in place of its nodes and edges.





*

Experiments
In this section, we provide all the necessary details concerning our experimental procedures. In particular, we describe the architecture of the dgn in detail, and discuss the assessment protocol by which we evaluated the proposed model on the predictive task.

The proposed dgn architecture for robustness prediction, shown at a high level in Figure , consists in a series of  graph convolutional layers, followed by a node readout that aggregates the hidden states at each layer into node representations by concatenation, a graph readout that aggregates the node representations into a graph representation, and a downstream mlp classifier which uses the graph representation to compute a probability of whether the graph is robust or not.





*
Some hyper-parameters of the network are fixed before-hand: the downstream MLP used for classification has two hidden layers of size 128 and 64, respectively, interleaved by ReLU non-linearities. As output layer, since our interest is to compute robustness probabilities, we used a sigmoid function that maps its input to the  range. All models are trained with mle using sgd, by minimizing the bce loss function as usual in binary classification tasks. For the minimization, we used the Adam optimizer, with a learning rate of 0.001 and scheduled learning rate annealing with a shrinking factor of 0.6 every 50 epochs. To prevent overfitting, we use two strategies: the hidden layers are regularized via Dropout, with drop probability of 0.1; and we used early stopping with a maximum number of epochs of 500, and patience parameter of 100 epochs.

Since this is the first time dgn are applied to this task, we also report a baseline, which is simply a model that always predict the most frequent class (robust, in this case). Its purpose is to serve as reference point to understand whether the task is being learned to some extent or not.

Our experiments are divided in two sequential phases. The experimental protocol is detailed at a high level as follows:

    in a first phase, which we abbreviate as E1, we performed model assessment on a smaller dataset of induced subgraphs. The rationale of this phase is to get a sense of which architectural choices, with respect to the type of gcl, are most promising to carry out the full-fledged evaluation on the entire dataset;
    in the second phase, termed E2, we perform a final evaluation of the architecture on the full dataset, fixing some architectural components guided by the results obtained in E1.

In both experiments, the evaluation procedure consists of an external 5-fold cv for model evaluation, with an internal hold-out split of 90 training and 10 validation for model selection. After a set of hyper-parameters is selected by the inner model selection, the winning configuration is trained and evaluated 3 times in the corresponding test fold, to mitigate the effect of random initialization. The score of the model for that fold is given by the average of these 3 trials. Following, we detail about the hyper-parameters and the evaluation metrics used in both experiments.

E1 Setup
As explained before, the first evaluation of the model is carried out a subset of induced subgraphs, speficially those with number of nodes . This resulted in a dataset with a total of 7036 induced subgraphs, which we term . In this phase, we evaluate the model selecting among the following hyper-parameters:

    number of gcl layers , choosing between 1 and 8;
    hidden state size , which once chosen remains fixed across all the  layers, choosing between 128 and 64;
    type of gcl;
    whether the gcl handles the contribution of the different edge types or not;
    type of graph readout function, choosing between sum, mean and max.

Specifically to the third point, we evaluate the following gcl variants:

    Graph Convolutional Network (GCN), as described in Section ;
    Graph Isomorphism Network (GIN), as described in Section ;
    Weifeiler Lehman Graph Convolution (WLGCN), the gcl layer presented in morris2019weisfeilerlehmangoneural, whose implementation is the following:
    
    where  is a learned scalar that weighs the connections between the current node and its neighbors.

For each considered gcl, we evaluate a vanilla variant, which does not take into account the different edge types, as well as an edge-aware variant as described in Section , using three different weight matrices (one for each edge type) for the neighborhood aggregation. More specifically, given an induced subgraph  and one of its nodes , the edge-aware neighborhood function used to select neighbors of a certain type is the following:

where .

Evaluation Metrics As performances metric, we used accuracy on the predictions, defined as usual as the number of correct predictions out of the total number of predictions. We remark that the model outputs probabilities; hence, we round the prediction to the nearest integer (which is either 0 or 1) to compute a hard prediction, which is used in the accuracy calculations. Formally, if: 
     is the number of true positives, the number of correctly predicted robust subgraphs;
     is the number of true negatives, the number of correctly predicted not robust subgraphs;
     is the number of false positive, the number of subgraphs wrongly predicted as robust;
     is the number of false negatives, the number of subgraphs wrongly predicted as not robust,

the accuracy is defined as:


E2 Setup
In the second experiment, we fix the gcl (and the fact that it handles edge types or not) to the one that obtains the best evaluation score in E1.
The other hyper-parameters evaluated are the same as in E1, though the selection is now performed on the full dataset .

Evaluation Metrics In this experiments, there is a high imbalance in favor of the positive class () with respect to the negative class in the dataset, which is not so dramatic for the small dataset ( in favor of the positive class). When the disproportion is so high, in fact, the accuracy can be misleading. Thus, besides accuracy, we evaluate the performances of the model using other metrics related to the accuracy such as:

    sensitivity, also known as True Positive Rate (TPR), defined as , which intuitively measures how good is the classifier to detect the positive (robust) class;
    specificity, defined as , which intuitively measures how good is the classifier to detect the negative (not robust) class.

Another metric we evaluated, which is not related to the accuracy, is the Area Under the Receiver Operating Characteristics curve (AUROC), which quantifies the ability of the classifier to discriminate between negative and positive examples. The AUROC is computed as the integral of a ROC curve, which measures how the TPR and the False Positive Rate  vary as one moves a threshold parameter . It is drawn as a curve plot where the x-axis represents the false positive rate, and the y-axis represents the true positive rate, both with values between 0 and 1. A point of the curve has thus coordinates (, for a fixed value of , with point (0; 0) being associated with  and point (1; 1) being associated with . The point (0; 1) is the optimum of the curve, since the false positive rate is minimized and the true positive rate is maximized. An example of ROC curve is shown in Figure .







To illustrate the behavior of the curve, it is useful to consider three extreme cases:

for a perfect classifier, which makes no errors, the curve goes from point , to point , to point ;
for a completely random classifier, which whatever the threshold always produces the same number of false positives and true positives, the curve created is the line segment bounded by the points  and ;
no reasonable classifier is assumed to produce a ROC curve whose points are located under the ,  segment.

An AUROC value of 0.5 describes a completely random classifier (since it is equal to the area of the triangle described by the extreme case 2), while an AUROC of 1 describes a perfect classifier (since it is equal to the area of the square described by the extreme case 1). Good classifiers have an AUROC score generally equal or higher than 0.8. Besides being a metric independent of class proportions, the AUROC also offers a probabilistic interpretation of its value, since it is equals to the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example. For an extensive survey of ROC curves, see for example .

Results
Here, we present the results obtained on the two experiments, E1 and E2. Subsequently, we present an analysis of the performances obtained by the model on two use cases: one on synthetic data, and one on a real-world pathway.

Results on the E1 Experiment
Table  shows the average accuracy on the 5 test folds obtained by each of the examined architectures on the E1 experiment. Even though these results are obtained on a dataset approximately 15 the size of the original one (consisting of 7036 induced subgraphs), we can already make some interesting observations about the task and the models:

    the task can be learned, as evidenced by the gap between the baseline and all the examined models, which is approximately 13 on average. This result verifies our initial assumption that it is indeed possible to predict robustness using only pathway structure;
    the comparison between the various gcl highlights the fact that there is no clear winner between the three tested architectures, which obtain very similar results when assessed on the same experimental conditions (all models with/withour edge handling capabilities);
    as expected, adding edge handling to the gcl is beneficial to improve performances. On average, the models obtain a 1.8 improvement in accuracy when edge handling is used.

























In a second experiment, we assess whether depth (intendended as number of dgn layers) is a good inductive for this task.To do so, we perform a study the validation scores, to see how they relate to number of dgn layers. Specifically, for each model, and for each number of layers from 1 to 8, we compute the average validation score obtained by all hyper-parameters configurations with an identical number of layers. Note that, although validation accuracy is in general an over-estimate of the true accuracy, the relative difference in performance as the number of layers change stays proportional independently of which specific data is used. In other words, we are not interested in the score by itself, but rather to the trend (increase or decrease) in performance in relation to the number of dgn layers. Figure  clearly shows that, in all cases, increasing the number of layers improves accuracy, up to a certain depth (around 4-5 layers on average) where performances start to plateau. This provides evidence that depth is a good inductive bias for dgn on this task, up to some extent.








Results on the E2 Experiments
For the second round of experiments, we fix the gcl to the gcn model with edge-handling capabilities, since it is the model that obtains the highest accuracy in E1.
The results of our experiments are reported in Table , where we average the computed metrics across the 5 test folds as usual.

























In this case, we stratify the performances by number of nodes, in order to also analyze the model performances in relation to the size of the input subgraph. From the results, one can immediately see, by looking at the last row, how the model accurately predicts robustness better than all models tested in the E1 phase by a large margin: this is probably caused by the largest size of the dataset, which usually results in major improvements with any ml model, and in particular with dl models. Specifically, we report an overall accuracy of , as well as an AUROC of . The model shows very high sensitivity () but a low specificity in comparison (); this indicates that it is harder for the model to predict induced subgraphs that are not robust. This effect is a probable consequence of the class misproportion between negative and positive examples, which is around 86 in favor of the positive class for this data sample. One result that is consistent across all measurements are the the very narrow standard deviations of the estimates, which indicate stable predictions regardless of the specific folds on which they are computed. To display this trend visually, we plot in Figure  the ROC curves obtained on the 5 test folds. Their similarity strongly indicates that the model performances are consistent across different test samples.





The results of Table  show a good performance of the model under the several stratifications tested. In particular, it performs better when dealing on subgraphs with 21-80 nodes, reaching an average in that strata AUROC of over 0.955. To better visualize this trend, we plot in Figure  the rolling accuracy of the model, using a window size of 20, and averaging across the 5 test folds as usual. The plot clearly shows the improvement in accuracy as the number of nodes increases. Interestingly, when the size of the graph exceeds 80 nodes, the model performances start to decrease. This might be a consequence of the smaller sample sizes of subgraph with 81-100 nodes which occur in the dataset 3 times less on average than subgraphs with 21-80 nodes. The same trend can be noticed for smaller subgraphs, with up to 20 nodes. Finally, Figure  shows the confusion matrix of the predictions computed by the model, where the entries are the averages computed across the five test folds. In the plot, we can visualize the good performances of the model as regards the number of correctly predicted subgraphs (on the diagonal) with respect to the cases where the model makes wrong predictions. Looking at the anti-diagonal the confusion matrix, we can also see that the model has a higher rate of false positives than false negative. Again, this is expected behaviour due to class misproportion in the dataset.















Lastly, we emphasize the fact that, once trained, the time needed to obtain a prediction from the model is in the order of milliseconds, while performing numerical simulations can be orders of magnitude slower. For this specific case, the simulation of most of the considered models (as detailed in Section ) requires a amount of time in the order of minutes, while bigger models can take many hours to produce an output value. While training and evaluating the model is expensive, and can take hours, it needs to be performed only once. To conclude, we believe that, once perfected, methods inspired by our approach have the potential of enabling faster advances in understanding the functioning of cells through pathway modelling.

Case Studies
Following, we present two application cases of the model: the first is on the synthetic pathway shown in Figure , and serves as an explanation tool of why the model performs sub-par in cases of smaller subgraphs; the second is an example of how the model can correctly infer the robustness relations in a real-world pathway.

Application on Synthetic Data
The lower prediction accuracy in the case of small graphs (1-10 nodes) can be put into perspective, in light of the fact the fact we trained the model with subgraphs in which kinetic, stoichiometric and initial concentration parameters have been omitted (as explained in Section ). In general, the smaller the graph, the higher the influence on its dynamics these parameters exert. As an example, let us consider the synthetic example of biochemical pathway introduced in Figure  and the corresponding pathway graph of Figure . Let us now consider the following kinetic and initial concentration (marking) parameters:
*
  k1 = 1.0 
  k3 = 0.01 
  k5 = 0.01 
  k7 = 0.3  
  k2 = 5.0  
  k4 = 0.1  
  k6 = 5.0 

  m_0(A) = 50
  m_0(B) = 50
  m_0(C) = 100
  m_0(D) = 100

  m_0(E) = 0
  m_0(F) = 0
  m_0(G) = 100
  m_0(H) = 0.

We used these parameters to simulate the ODEs of Figure , calculating the corresponding robustness by varying the initial concentration of each molecule in the interval . The robustness values obtained with the simulations are displayed in Table . Analogously, in Table  we list the average and standard deviations obtained by the 5 different models evaluated in Section  (one trained on the respective CV fold), when tasked to predict the robustness probabilities of some input/output pairs of interest.




































We remark that values in these two tables are not directly comparable: those in Table  are exact robustness values, while those in Table  are probabilities of the robustness values to be greater than 0.5. In this specific case, the prediction turns out to be accurate in the case of input/output pairs corresponding to big induced subgraphs. This happens in the cases of the input/output pairs  and , whose induced subgraphs are among the largest ones. In contrast, the prediction is incorrect for : in this case, the models predicts a small robustness probability, while the simulations give 0.99. We observe that the robustness value of this input/output combination is sensitive to the perturbation of parameters that have been discarded when constructing the dataset. In particular, if the initial (omitted) concentration of  was  instead of , the robustness value of the pair  would become  rather than . Another case when the model prediction is wrong is that of the pair The prediction turns out to be wrong also in the case of input . In this case, the probability given by the network is under , which contrasts to a value of  obtained by the ODEs simulation. Even in this case, we notice that the parameters that have been omitted in the dataset might have a strong influence on the robustness, such as kinetic formulas and the multiplicity of the edge directed to node . Finally, in the case of the pair , the prediction gives a small probability of robustness and indeed the actual measured value is borderline (). More in general, we observe that smaller subgraphs are less frequent than medium-sized subgraphs in the dataset. Thus, it is possible that the model has learned to be more accurate on the latter subgraphs (to maximize the accuracy), at the expense of making more errors when predicting the former.

Application on a model of the EGF Pathway
As a second use case, we consider the SBML model of the Epidermal Growth Factor (EGF) pathway proposed by Sivakumar et al. in sivakumar2011systems. This model corresponds to model BIOMD0000000394 of the BioModels database, and is shown in Figure  as represented by the CellDesigner tool funahashi2003celldesigner. We adopt this visual representation style for this case study for readability, but the pathway can can be trivially translated into a ppn.





The pathway describes, in a simplified way, the transduction of the EGF signal, and the consequent activation of the mitogenesis and cell differentiation processes (modelled as an abstract species in the pathway). When the EGF signal protein is available in the cell environment, it can be perceived by the receptor protein EGFR (where R stands for Receptor), which then initiates a cascade of reactions inside the cell, ultimately leading to the activation of mitogenesis and differentiation. The initial steps of the pathway give rise to a rather big complex, involving an activated EGFR dimer and a number of other proteins (the big box in the upper-left corner of Figure ). The formation of such complex is described in a very simplified way in this pathway model, by concentrating everything in only two reactions. The big complex then promotes a cascade of reactions inside the cell, which are modelled more in detail.

Considering the Mitogenesis Differentiation (abstract) species as output (in pink, bottom left corner of Figure ), with respect to EGF and EGFR as input (in green and yellow, respectively, top left corner of Figure ), numerical simulations assign a very high robustness () in both cases. This is actually expected in a signal transduction pathway, since it behaves as an amplifier that must be able to activate the target cell process despite of perturbations in the signal and receptor concentrations. On the other hand, if we look at the robustness of the first portion of the pathway up to the creation of the big complex, we can then observe a different behavior. Specifically, if we consider the big complex as output and EGF as input, we still obtain a very high robustness (). However, when the input is EGFR, we obtain a robustness value of only . Again, this is not surprising, since EGF is modelled in the pathway as a promoter of the first reaction (, it is not consumed), while EGFR is a reactant and it will be included in the big complex.

In this case, the model correctly captures the different roles of EGF and EGFR. Indeed, using EGF as input and the big complex as output, the model gives  as probability of robustness, whereas it gives  when the input is with EGFR. The model captured also captures the robustness of the whole model, namely the previous case where either EGF or EGFR were considered as input, and the abstract Mitogenesis Differentiation as output. Precisely, it gives probability  with EGF as input and  with EGFR as input.

Conclusion and Further Work
This work shows experimentally, and for the first time, that we can correlate structural properties of biological pathways to their dynamical properties. We consider this result very promising and deserving of further investigation. Indeed, the promise of this approach if that of creating a novel class of learning methods for pathways, which could help to better study the functioning of living cells.

We consider this approach impacting for two main reasons. Firstly, predicting dynamical properties of pathways with Deep Learning methods is faster than performing numerical or stochastic simulations. While the training process is expensive in terms of time required, it needs to be performed only once, and can be amortized nicely as more and larger subgraphs are used. Secondly, our approach works considering only a minimal amount of information regarding the pathway: its structure, an input and output node, and the types of nodes and relations between them. Hence, it is applicable even in cases where other information, such as kinetic constants, is not available.

One research direction to pursue in the immediate future concerns evaluating this architecture on datasets where the information we omitted in the present study has been added. For example, it would be interesting to assess how the addition of edge labels (multiplicities of reactants/products), or kinetic formulas, would affect performances. These additional information could be useful, for example, to help the preditions of the model in case of smaller subgraphs. Moreover, one could think to extend this approach to the assessment of other related dynamical properties of pathways, such as other notions of robustness, or even completely different ones such as monotonicity, or oscillatory and bistability properties.

We also believe this work opens up fascinating research scenarios to pursue in the medium term. One important aspect that needs further investigation is model explainability. Even though their predictive result are very often excellent, neural networks are sometimes seen as black-box since the exact function (in this case, from pathway structure to robustness) they approximate is difficult to characterize. One possible research direction is to investigate generative models, to uncover the underlying structure of pathway space and help design better predictive models, as well as provide better explanations to the decisions they take.

Deep Generative Learning on Graphs and Applications to Computational Chemistry
Deep Generative Learning on Graphs 

In this chapter, we discuss the graph generation problem. The term graph generation is purposedly kept broad, to include a variety of methodologies which learn processes that generate graphs from a set of training examples. This research field originates from generative models of graphs for the theoretical understanding of graph properties, which have been studied thoroughly since the '50s in the field of graph theory. The first known generative model is the  Erdos-Renyi (ER) model . The ER model studies random graphs, where the connectivity among graph nodes is modeled independently. This model is useful to study theoretical properties of graphs, such as how the global connectivity of the graph evolves as its size grows. Another historical model of graphs is the  Watts-Strogatz (WS)  model . The WS model concerns small world graphs, graphs where it is possible to reach any other node in the graph with very short paths, regardless of their ize. This property arises in several real-world graphs such as social and electrical networks. Lastly, the Barabasi-Albert (BA) model , which models the so-called preferential attachment property, where the connectivity potential of a node is directly proportional to its number of neighbors. While being useful to the study and understanding of graph properties, these models usually fail to generalize to real-world graph distributions, because they can only model one or a limited number of graph properties, and that their parameters cannot be learned from data in general. Other methods such as stochastic block models airoldi2008mixedstochasticblock, exponential random graphs robins2007exponentialrandomgraphs, and Kronecker graphs leskovec2010kronecker can approximate more complex graph distributions such as graphs with communities, but are still limited to specific kinds of graphs and scale poorly to large datasets.
These limitations clearly highlight that, in order to generate graphs for practical applications, a more powerful class of learning models is needed. In this chapter, we review a variety of generative models of graphs based on Deep Learning approaches. The advantages of such models are the possibility to approximate complex distributions efficiently and effectively, and the flexibility to use different generative paradigms to adapt to the task at hand.

The Challenges of Graph Generation
The problem of generating graph structures from arbitrary distributions is arguably harder than their predictive modeling. Some of the challenges that need to be addressed when designing deep generative models of graphs are related to the complexity of graph spaces. In particular, we report the following:

    size of graphs spaces. Graphs spaces are combinatorial, and thus very large. For example, the size of the space of undirected graphs with  nodes is , which becomes very large even for graphs with a moderate number of nodes. Thus, trivial approaches such as exhaustive enumeration are generally intractable. Moreover, real-world graph distributions are usually defined over attributed graphs with variable size, which makes the search space even larger;
    discreteness of graphs spaces. Graphs are discrete objects. This contrasts with the nature of neural networks, which can backpropagate only through continuous and differentiable objects. Hence, the learning process must be accomodated to work with discrete structures;
    sparsity of graphs spaces. For most generative tasks, only very small subsets of graph space contain graphs with non-zero probability. Thus, any generative method should be designed to focus on regions where interesting graphs are contained, to avoid efficiency issues;
    complex dependencies. Most real-world graph have hard structural constraints that are very difficult to enforce on a generative model (which is inherently stochastic). For example, cycle graphs are such that if only one edge is missplaced, the graph is not a cycle anymore;
    non-unique representations. In general, graphs are invariant under node permutation. Thus, the same graph can be potentially represented by up to  possible adjacency matrices, depending on the node permutation. This poses constraints on the kind of graph representations a generative model can learn. For example, generative methods that map graphs into a latent space must take into account that different node permutations of the same graph must map to the same latent vector. While invariance to node permutation can be addressed by assuming an order of the graph nodes, this introduces the necessity to maintain order consistency among different graphs.


Generative Tasks
The family of dgm of graphs is flexible enough to model different kinds of generative tasks. Loosely following the taxonomy proposed by guo2020systematicreviewgenerativegraphs, we distinguish two main tasks related to graph generation:

    unconditional generation, where the task is to explicitly learn a distribution  over graphs, or some parametrized function that produces samples from it. Here, the term unconditional refers to the fact that the generation starts with drawing a vector  from some easy to sample prior distribution , which is usually assumed to be an isotropic Gaussian or a uniform distribution;
    conditional generation, where the aim is to learn a conditional distribution  with , or the corresponding parameterized sampling mechanism. The purpose of the conditioning vector is to drive the generative process towards producing a graph with desired characteristics. For example, one might want to generate a graph whose structure resembles that of a graph given as input to the dgm. In this case,  is the representation of the conditioning graph.

Hereafter, we consider the task of unconditional generation, where we assume access to a dataset of graphs . Notice from the notation that, at least for the moment, we focus on unattributed graphs. Broadly speaking, defining a dgm of graphs requires to specify two components: a graph decoder, which takes care of generating a graph, and an end-to-end generative framework used to optimize the model parameters. As regards the latter, common generative frameworks include vae, gan, and flow-based models rezende2015normalizingflows. Here, we only describe approaches based on the first two.

Graph Decoders
The graph decoder is the architectural component that outputs some conditional distribution, which can be sampled to generate a graph. If the framwork in which the decoder is placed allows for inference (such as the vae), the conditional is also learned with maximum likelihood; otherwise, (such as with gan) it is used only for sampling, and its parameters are optimized with adversarial training. Ideally, graph decoders should generate permutation invariant graphs; however, this is rarely the case. The major hurdle to devise permutation invariant graph decoders is their computational cost; even though some methods do exist ermon2020permutationinvariantgraphgeneration, they are still too inefficient to be deployed in real world scenarios. Thus, in the following, we assume non-invariance.
One particular caveat that needs to be addressed during the training phase of a graph decoder is maintaining the differentiability of the architecture while still generating hard graph samples. This is critical especially in gan-like architectures, where the discriminator must be trained with actual graphs. As with sequence generation with rnn, the same techniques (straight-trough gradient estimation, reparameterization, or even reinforcement learning-based techniques ) can be used for this purpose. There are two main paradigms to implement adaptive graph decoders, which we detail in the following.

One-shot Decoders
This class of graph decoders outputs a dense probabilistic adjacency matrix , where  is the maximum number of nodes allowed. The probabilistic matrix is sampled entry by entry to produce an actual adjacency matrix. The entries of the matrix are modeled as independent Bernoulli variables, which indicate the presence or absence of an edge. In some cases, the elements in the adjacency matrix  are modeled as independent Bernoulli variables that specify if a node belongs to the graph or not. In practice, the entries of the matrix are produced by a neural network with sigmoid outputs that predicts an  vector. Thus, the adjacency matrix can be sampled in parallel (hence the term one-shot). Two possible approaches to specify a one-shot decoder are:


    graph-based decoders, which require a graph representation . In this case, the decoder models the conditional as follows:
    
    where  is a neural network that predicts the matrix from the graph representation;
    node-based decoders, which require a matrix of node representations . In this case, the decoder models the conditional as follows:
    
    where  and  range over the matrix rows. In this case,  is a neural network that takes as input pair of node representations, and applies a sigmoid function to their dot product. The idea is that nodes that are close in representation space should be more likely to be connected.

One-shot approaches are usually fast to train and to take samples from. However, they are too simplistic, in that they assume the edges are generated independently (which is usually not the case for real-world graphs). Furthermore, the maximum number of nodes must be pre-specified, which makes them unable to generalize to larger graphs.

Autoregressive Decoders
Autoregressive decoders assume that graphs are generated by some sequential process that involves its set of nodes. Specifically, the generative process is the following:

where  are sequences that generate graphs one component at a time, and the order of generation is given directly or indirectly by a nodes permutation drawn from a prior . The idea is to decompose the generating sequence autoregressively as follows:

However, this requires to integrate over all  possible node permutations, which becomes intractable for moderately large graphs. An approximate solution to this issue is to assume some ordering and create the sequences before training, as a preprocessing step. Once the sequence are fixed, the chain-rule decomposition becomes tractable:

Depending on the nature of the graph generating sequence, we distinguish four possible approaches to develop autoregressive graph decoders:

    node-based approaches decompose the graph as a sequence of actions performed on an initially empty graph. These action correspond to decisions such as whether to add a node to the existing graph, and which nodes it must be connected to. In this case,  is a vector that represents the current state of the graph. For all these models, one has two options as to how to implement the autoregressive network. One approach is to use a hierarchy of rnn: one keeps track of the state of the nodes added to the graph, and the other is responsible to connect newly added nodes to the current graph, given the state of the first you2018graphrnn. The other choice is to update the state of the current graph with a dgn, which is passed to the networks responsible of deciding which action to perform li2018learningdeepgmg;
    edge-based approaches decompose the graph as a sequence of edges. To produce an ordered sequence of edges, one must first order its nodes, then label the nodes with progressive integers, and then sort its set of edges in lexicographic order. In this case,  represents the state of the graph indirectly, by keeping memory of the edges of the sequence already generated. These approaches are mostly implemented with rnn goyal2020graphgen,bacciu2019edgegraphgenrnn;
    rule-based approaches can be applied in cases where the graph generation can be decomposed in a sequence of production rules over some known grammar (molecules or computer programs) kusner2017grammarvae,dai2018sdvae. In this case, the model generates a sequence of production rules to construct a desired graph;
    motif-based approaches decompose the graph as a sequence (or even a tree) of motifs, very small and manageable subgraphs, which are combined together adaptively. Here, the challenge is mainly how to decompose the graphs into sequences of motifs,

A special kind of sequential decomposition for graphs is the smiles linearization applied to molecules. We shall define the SMILES more precisely in Section ; for the moment, it is sufficient to say that the SMILES encoding of a molecule is a string of ASCII characters that represents its structure. When a domain-specific linearization techniques such as SMILES are not available, the sequences representing the graph generative process are constructed based on some node ordering strategy. One general strategy to do so is to choose one node at random, then visit the graph nodes with a depth-first or breadth-first traversal. The order by which the nodes are visited is used to determine the order of the elements in the sequences. Clearly, this approach is not optimal since it heavily depends on the starting node, and may produce very different sequences for different starting node choices. However, it has been shown to work empirically you2018graphrnn,li2018learningdeepgmg,bacciu2019edgegraphgenrnn,goyal2020graphgen. The pros and cons of autoregressive decoders are orthogonal to those of one-shot decoders: briefly, they allow to generate variable-sized graphs seamlessly, and they can model dependencies between nodes and edges by means of the autoregressive property. However, both training and sampling processes are slower in terms of computational time, because the graphs are reconstructed one sequence element at a time and not in parallel.

Performance Evaluation
The desired end result of training a generative model is that the structure of samples generated by the network should resemble that of graphs in the training set, without being identical. This is a very different setting with respect to predictive tasks, since the samples of a generative model do not exist until they are generated. Hence, one does not have access to held-out ground truth values to measure generalization. Furthermore, the model can very easily obtain deceptive-looking performances by learning to replicate training graphs exactly, or repeating the same graph typology over and over. Thus, one critical aspect of using generative models of graphs is how to assess performances. Below, we define two broad classes of metrics that allow the evaluation of graph models, assuming the availability of a training sample  , and a collection   of samples generated by the model.

Quantitative Metrics
Quantitative metrics measure the rate at which the generative model produces diverse and heterogeneous graphs, without taking into account structural similarity. The three main quantitative matrics considered in the literature are:

    novelty measures the ratio of generated samples that are not training samples. A high novelty indicates that the model has not learned to replicate training graphs. Formally, it is measured as ;
    uniqueness measures the ratio of unique graphs with respect to the total number of graphs generated. A low uniqueness rate might indicate that the model has overfit one specific typology of graph. To calculate uniqueness, one first checks every graph for isomorphism with every other graph in the generated sample, removing them. If the resulting set of unique graphs is indicated by , uniqueness can be simply calculated as .
    validity measures the ratio of generated graphs that respect some validity constraint, out of the total number of graphs generated. To calculate validity, one needs to check wheter every generated graph satisfies some structural constraint or not (the presence of a cycle). If  is the collection of graphs that satisfy the structural constraints, validity is calculated as . This metric is particularly useful in molecular generation tasks, since chemically invalid molecules are useless. When assessing validity is required, novelty and uniqueness are usually conditioned on validity first, meaning that the all the invalid graphs are removed from  before calculating these two metrics.


Qualitative Metrics
Quantitative metrics give only one side of the spectrum relatively to how a generative model is performing. For example, assessing novelty alone might be misleading, since a high novelty rate can also be associated to underfitting (meaning that the model generates graphs very different from the training sample, which are trivially novel). Thus, a proper evaluation of generative models must also include a series of metrics that consider the structural properties of the generated graphs. We call such metrics qualitative. The framework under which qualitative metrics are assessed consists of comparing the empirical distribution of a certain graph property in the training sample, to the empirical distribution on the generated sample. Given a generic graph with  nodes, coming from one of the two samples indifferently, a relevant subset of such properties includes:

    node degree distribution, that is, an -dimensional vector where each position contains the degree of the corresponding node. Notice the the length of the vector may differ across different graphs, since their number of nodes may change;
    clustering coefficient distribution. The clustering coefficient of a node  is defined as the ratio between the number of actual connections between neighbors of  out of the total number of possible connections. In other words, it is a relative measure of how many closed triangles (fully connected graphs with three nodes) the node is part of. Similarly to the node degree distribution, it consists of an -dimensional vector where each position contains the clustering coefficient of the corresponding node;
    number of nodes of the graph, which is a single integer;
    number of edges of the graph, which is again a single integer;
    average orbit counts. Orbits are subgraph with 4 nodes. Counting orbits in a graph can be viewed as a generalization of the clustering coefficient to 4-node subgraphs instead of triangles. In practice, it consists in a -dimensional vector, where  is the number of orbits considered;
    nspdk costa2010nspdk, which measures the similarity between two graphs by counting the number of matching induced subgraphs between them. The subgraphs are derived node-wise, by considering neighborhoods of a node comprising nodes at increasing path lengths. Differently from the other qualitative metrics, the nspdk provides a global measure of similarity between graphs, since it is based on multiple subgraph matchings. In practice, it is a vector of length  (, respectively), where each position measures the similarity of the graph with another graph in the sample.

Once the graph properties are calculated for each graph in the sample, there are two options to measure the distance between the empirical distribution of the training sample versus the generated sample, based on their respective number of elements:

    if , one can compute their empirical kld as follows:
    
    where  is one of the properties mentioned above;
    if , one can either concatenate all the values of the property for each each in the sample, and then fit a histogram with an equal number of bins to make their length match in order to apply the empirical kld. Another, more general, approach to compare distribution when the two samples have different lengths is to compute their mmd gretton2012mmdkernel. Intuitively, the mmd measures the distance between two distributions as the sum of the distances of between their matching moments. The computation of these distances can be generalized to an infinite space of moments by applying a kernel trick smola2008kernels.


A Model for Edge-Based Graph Generation
Introduction
In this section, we present an original contribution. Specifically, we introduce a novel generative model for graphs, capable of generating unattributed graphs coming from very different graph distributions. We transform graphs into sequences of ordered edges, from which we extract two sequences derived from the edge endpoints. We use a model composed of two rnn to learn the probability distribution of such sequences: the first is an autoregressive network which generates a specification of the graph to produce, which is completed into a graph by the second network. We experiment extensively with the proposed model, comparing its performances with a pool of baselines, one of which is a dgm of graphs that holds state-of-the-art performances at the generative task. The experimental framework has been designed to evaluate the proposed model on concerning both quantitative and qualitative aspects, as discussed in Section . Our experiments demonstrate that, under our evaluation framework, the proposed model is able to perform at, and sometimes surpass, the state-of-the-art in the task of generating graphs coming from very different distributions. Furthermore, we study the effect of changing the order of the edge sequence by experimenting with different node orderings. We show that the chosen node ordering strategy  is more effective for learning complex dependencies than the alternatives, and produces graphs of better quality.

Methods
In this section, we present the methodologies used to develop the model. In particular, we formally introduce the concept of ordered edge sequences, we develop the model,and  we show how it is trained and how graph generation is achieved.

Ordered Edge Sequences
Let  be a fully connected unattributed graph with  nodes and  edges. We assume  is undirected for simplicity, without loss of generality. Let  be a bijective node labelling function which assigns a unique positive integer (which we call node ID) to each node in the graph; thus,  defines a total order over the nodes of . The ordered edge sequence  of graph  is the sequence of pairs:

where , and such that it is ordered lexicographically according to the IDs assigned to the nodes,  if and only if , or  and . Given a generic pair , we call  its starting node and  its ending node. Finally, let us define the starting sequence , the sequence corresponding of starting nodes ordered as in , and analogously, the ending sequence , corresponding to the ending nodes ordered as in . For conciseness, let us omit the dependece of  on the graph , and of the starting and ending sequences from  whenever they are clear from the context. Clearly, the choice of the labelling function  is critical in determining the ordered sequence of a graph. Given the graph , we choose to implement  with the following algorithm:

    first, select a node  at random from its set of nodes , and set its node ID as ;
    then, traverse the graph in breadth-first order. Let  be the ordered sequence of nodes visited during the traversal, excluding . Assign node ID .

Assuming graph  has the structure shown in Figure , and that node  is chosen as the root node for the visit, an example of how the graph nodes are labelled by  is shown in Figure . Notice that  is trivially bijective, since it assigns a different integer to each node. Once the nodes are labelled, the ordered edge sequence of  is , with  and . Notice that the graph can be readily reconstructed from its orderd edge sequence by first applying the inverse function  to each element of its pairs to obtain , which in turn gives  since we assumed that  is fully connected.
















*

Model
Our goal is to model , the probability of ordered edge sequences, using a dataset . Our key observation is that any ordered edge sequence  is uniquely defined by its starting and ending sequences. Therefore, instead of working on the ordered edge sequence directly, we work on their starting and ending sequences. Specifically, we model the probability of sampling  from  as follows:

Intuitively, the generative process specified by the distribution is the following:

    first, one samples  from a prior probability , which is modeled autoregressively;
    once  is available, it is used to condition the prediction of the ending sequence .

Once both  and  are available, the ordered edge sequence (and thus, the corresponding graph) an be reconstructed simply pairing their elements. Here, we make an important remark. Notice that the the probability of  is modeled autoregressively, while the conditional  is not. This means that to generate a sequence, the only stochastic part in the model relates to , while predicting  is deterministic once  is known. One possible interpretation of  is that it acts as a soft prior of the structure of the graph: in fact, it specifies a subset of relevant nodes for the generation and their degree, expressed as the number of times they appear in the starting sequence. The conditional  uses this information to complete the graph based on the information provided by the prior.

Training
The model specified above is implemented as two rnn in cascade, which we refer to as  and  respectively. The two sequences (starting and ending) are first encoded as sequences of -dimensional one-hot vectors, where  is the largest node ID assigned by  to any node in the dataset, and  is added for the start of sequence and end of sequence tokens. We indicate the one-hot encoded starting sequence with the notation , where , and analogously, we use  for the one-hot encoded ending sequence, with . Notice that an end of sequence token is added to : when it is predicted, the autoregressive process that yields  is interrupted. Our dataset is has thus the following form: . Focusing on a single training pair , we explain the feed-forward phase of the network. Firstly, the sequence  is processed by . Given an element , the per-element output of  is obtained as follows:
*
    hi &= GRU_(hi-1, E^si-1)

    oi &= (V_hi + b_).

In the formula above,  is an embedding matrix whose entries are learnable, and the dot product selects the column of  corresponding to the embedding of the node ID assigned to . Furthermore,  is a hidden state,  is a multi-layer GRU network,  and  are the weights of the softmax output layer, and  is an output distribution over all the possible node IDs. The process is initialized by setting  and . We use teacher forcing, meaning that, at each step, the input of the network is the ground truth value taken from , rather than the output predicted by the network. Lastly, the output  is compared to the ground truth value  using the ce loss function. For the whole starting sequence, the computed loss is the following:

After the starting sequence  has been processed by , the control flow passes onto . The function computed by  is similar to the one implemented by the first rnn. Specifically, it is the following:
*
    h'i &= GRU_(h'i-1, E^si)

    o'i &= (V_h'i + b'_),

where , the second network is initialized with the last hidden state of the first. Moreover,  is a hidden state,  is a multi-layer GRU network,  and  are the weights of the softmax output layer,  is an output distribution over all the possible node IDs, and  is the same embedding matrix used by .  Lastly, the output  is compared to the ground truth value  using the ce loss function, similarly as before. For the whole ending sequence, the computed loss is the following:

The whole network is trained with mle by minimizing the following objective function:

where . Figure  shows the architecture during training.






*

Generation
To generate an ordered edge sequence, one first generates a starting sequence  from the first network in autoregressive sampling mode. The generative process is started by passing the  token to the network. The categorical distribution corresponding to all the possible Node IDs is sampled at each step, and the resulting node ID is used as input for the next step. The autoregressive sampling is interrupted once the  token is sampled. At this point, the sampled starting sequence  is used as input for the second network, which predicts the ending sequence. This time, the output distribution is not sampled, but the token with the highest probability  is chosen at each step. This method is also known as greedy sampling, and corresponds to applying the  operator to the distribution vector, which gives the ID of the chosen node. After all elements have been predicted, we end up with a starting sequence , and an ending sequence . The two sequences are paired together to obtain the ordered edge sequence of the generated graph. Figure  shows the generative process visually.





*

Implementation details
The model is implemented using the PyTorch   library; after an initial exploratory phase (not reported), some hyper-parameters (such as the number of recurrent layers) were fixed in advance. We use model selection with a 80-20 splits to select the other hyper-parameters. We choose among 32 and 64 for the embedding dimension , 128 and 256 for the recurrent state size . We apply a dropout layer to the embedding layer, whose rate  is chosen between 0.25 and 0.35. As recurrent cells, we use 2 layers of Gated Recurrent Units (GRU) . As regards the learning parameters, we use the Adam  optimizer with learning rate of 0.001, halved every 200 epochs. We train all models for a maximum of 2000 epochs, applying early stopping whenever the loss starts to plateau (we found a suitable empirical threshold after running a number of exploratory training instances).

Experiments
Following, we detail the experiments to assess our model, describing the datasets used for learning, the baselines we compared to, and the framework used for evaluation.

Datasets
In our experiments, we evaluate our model on 3 synthetic datasets and 2 real-world datasets of molecular graphs. Every synthetic dataset represents graphs with particular node/edge dependencies. The rationale is to assess whether our approach is general enough to give good performances on very different graph distributions. Concretely, we use the following datasets:


    LADDERS, a synthetic dataset of ladder graphs. We generate all possible ladder graphs having number of nodes , and replicate them 10 times, resulting in a total size of 180 graphs. This dataset is inspired from the grids dataset used in you2018graphrnn. In our case, we use ladder graphs because they have a similar node degree distribution, while being computationally manageable. In this dataset, the model has to capture very strong dependencies: the nodes of a ladder graph have degree of 3, except nodes at the four corners, which have degree of two. Any graph that does not respect this dependencies is not a ladder graph. An example of ladder graph is shown in Figure ;
    COMMUNITY, a synthetic dataset of graphs with two-community structure. Community graphs are composed of two densely connected clusters of nodes, which are weakly connected between themselves. Here, the model has to capture the community structure. Community dependencies are very common in biological settings: for example, densely connected communities in metabolic networks often represent functional groups (see girvan2002commstructsocialbionet). To create this dataset, we firstly generate two clique graphs of random size between . We then remove some edges randomly from the two clusters with probability , and then connect the two communities with 1 or 2 edges at random. The generated dataset is composed of 1000 graphs. This dataset is similar to the COMMUNITY dataset used in you2018graphrnn, which unfortunately we could not reproduce. An example of community graph is shown in Figure ;
    EGO, a dataset of ego networks extracted from the Citeseer dataset giles1998citeseer. In this case, the model has to capture the presence of a focal node (the ego node), which is connected to the majority of nodes in the graph, while the other nodes (the alter nodes) have weaker connectivity. This dependency is typical of social networks, and can be modeled with the BA model for preferential attachment. To create the dataset, we extract all possible ego networks of radius 2 from the Citeseer dataset. Thus, the path length between the ego node and the alter nodes is at most 2. The total number of graphs in the dataset is 1719. An example of ego graph is shown in Figure ;
    ENZYMES, a subset of 436 graphs taken from the ENZYMES dataset schomburg2004enzymes (see Section  for details);
    PROTEINS, a subset of 794 graphs taken from the Dobson and Doig dataset dobson2003dd (see Section  for details). In these two last datasets, the model should capture patterns of connectivity typical of molecules, such as functional groups or aromatic rings.

All datasets have a number of nodes comprised between 4 and 40, and a number of edges comprised between 2 and 130. Before training, we held out a portion of the dataset from testing purposes. In particular, for all datasets except LADDERS, we held out 30 of graphs, and used them for evaluation. This held-out set acts as a random sample drawn from the corresponding unknown graph distribution.





















*
For the LADDERS dataset, we held out 10 of graphs in a stratified fashion. In practice, the held-out set for LADDERS is composed of the same 18 unique ladder graphs found in the training set. To motivate this choice, let us clarify the role of the LADDERS dataset: its purpose is not to evaluate generalization capability, but to show that adaptive models can overfit hard node/edge dependencies among nodes, while non-adaptive models (such as the ER and BA models) cannot. The statistics of the datasets used for evaluation are presented in Table .




















Baselines
We compare against four baseline models. Two of them are classical generative models of graphs coming from graph theory literature, namely the ER and BA models. The rationale behind this choice is to assess whether our model is able to perform better than random models that model graph connectivity independently (ER), or consider just one simple edge dependency (BA, where the probability of an edge is a function of the node degree).

The ER model has two parameters: , the number of nodes of the graph to be generated, and , the probability of adding an edge between a pair of nodes. The generative process of the ER model can be described informally as follows: first, pick , then, for each possible pair of nodes, sample a Bernoulli random variable with probability , and connect the two nodes according to the sampled value. We sample  from the empirical distribution of the number of nodes in the datasets, and choose  from a grid of values. The best value of  is obtained by minimizing the earth mover distance between the empirical distributionand the generated distribution of graph properties

Similarly, the BA model has two parameters: , the number of nodes of the graph to be generated, and , a maximum number of nodes a node can be connected to. The generative process for a BA graph proceeds incrementally: given an already formed graph, add a new node and connect it (or not) to at most  nodes with probability proportional to the nodes degree. In our experiments, the two parameters of the BA model are optimized in a similar fashion as the ER model.

Besides graph theory baslines, we also compare to a strong Deep-Learning based generative baseline. In particular, we choose the GraphRNN model of you2018graphrnn, which holds state-of-the-art performances in the graph generation task. We implemented the model according to the original code repository provided by the authors, following their implementation and their hyper-parameters configuration closely.

Lastly, we introduce a third baseline model, a recurrent neural network with GRU cells that is trained to predict the adjacency matrix of the graph one entry at a time. We call this baseline GRU-B(aseline) from now on. It is arguably the simplest model one can come up with to model graph generation in a recurrent fashion, with the limitation that it has to sample  entries to fully specify the adjacency matrix of an undirected graph with  nodes, making it susceptible to learning issues induced by long-term dependencies bengio1994learninglongtermdependenciesdifficult. Clearly, even though Deep-Learning based, this baseline has purposedly limited expressiveness with respect of our model and GraphRNN.

Evaluation Framework
We assess our model against the baselines following the quantitative and qualitative evaluation principles described in Section . The experiments are described as follows:

    our first experiment consists in evaluating the model quantitatively. To be sure to detect overfitting, we generate large samples. Specifically, we generate two samples of size 1000 and 5000 from all the candidates. For each of sample, we measure the novelty and uniqueness of the generated graphs. Despite using chemical datasets (ENZYMES and PROTEINS), we do not evaluate chemical validity because we are concerned about generating unlabelled graphs; thus, chemical validity cannot be assessed in our case. Finally, we also measure the time that each model takes to generate the 5000 graph sample;
    the second experiment is of qualitative nature. In practice, we assess how much the generated samples resemble a random sample from the graph distribution of reference. To do so, we first generate from all the models a sample of equal size to the held-out test set. Then, on each sample, we evaluate the kld on a set of graph statistics, collected in the generated sample and in the test sample. Specifically, we evaluate Average Degree Distribution (ADD), Average Clustering Coefficient (ACC), and Average Orbit Count (AOC). Note that, since each graph can have a different number of nodes, the vectors of statistics for each graph have different lengths in general. Therefore, for each of the two samples, we concatenate the graph statistics vectors into a unique vector, on which we fit an histogram of 100 bins. The kld is calculated on the two resulting histograms. This process is repeated 10 times, each time drawing a novel sample from the models.

Lastly, we provide a study on how using different node orderings affects performances. To do so, we compare our proposed model with variants where graphs are preprocessed used different node orderings. The five analyzed variants are:

    Random: the node ordering is a random permutation of the nodes. This strategy is expected to perform very poorly, since the model does not see consistent connectivity patterns during training;
    BF Random: the node ordering strategy proposed by you2018graphrnn. Such strategy is very similar to ours (in fact, our strategy is inspired by it), but differs in that, each time a graph is picked from the training set at different epochs, the root node for the visit can be different. In principle, this means that the model is trained on every possible node permutation induced by a breadth-first (BF) visit of the graph. On one hand, this strategy acts as a regularizer, since at each epoch the training set changes completely. On the other hand, however, changing the node ordering at each epoch could in principle prevent our model from focusing on useful connectivity patterns, simply because they are "masked" away by different node permutations;
    DF Random: the same strategy as BF random, but using depth-first (DF) visit instead;
    DF: a variant of our proposed strategy which uses depth-first traversal instead of breadth-first;
    SMILES: the node ordering imposed by the SMILES encoding of the graph. This strategy is available only in the PROTEINS and ENZYMES datasets, which consist of molecular graphs;

Our first experiment for this analysis consists in training the models with the alternative node ordering strategies for a large number of epochs without regularization. In practice, we try to overfit the dataset on purpose. The idea is to assess whether the alternative node ordering strategies are able to memorize connectivity patterns (hence, given proper regularization, are able to generalize to unseen ones). In our second experiment, we perform a qualitative analysis comparing the performances of our model to the different variants.

Results
Here, we present the results of our experiments. We divide the analysis of the results into a quantitative and qualitative sections for readability purposes.

Quantitative Analysis
Table  shows the results of our quantitative experiments. The best performances in terms of novelty and uniqueness are obtained consistently by the ER and BA models; this was expected, since by definition they produce random graphs, hence very likely to be different from the training set, as well as from the generated sample. One exception is the EGO dataset, where random models score poorly with respect to the competitors. We argue that this result is due to the nature of the EGO dataset, which is composed of graphs with very weak connectivity (except for the ego nodes) and a very low number of cycles. With such characteristics, it is easier for a random model to produce duplicates or overfit the training sample, just by setting the parameters that regulate connectivity to small values.










































In contrast, our model and GraphRNN consistently generate graphs with high novelty and uniqueness rates in almost all scenarios. The only exception to this trend is the ladders dataset, where both our model and GraphRNN overfit (they tend to generate the same graphs over and over), while random models do not. However, we remark the peculiarity of the LADDERS we mentioned in Section . The main purpose of that particular dataset is to show that random models are unable to learn the strong edge dependency of ladder graphs. Thus, the analysis of novelty and uniqueness must be put into perspective by the qualitative analysis, to see whether structural dependencies have also been learned.

The GRU-B model greatly under-performs in all but the COMMUNITY dataset. From this result alone, one could legitimately infer that the model is not generalizing to unseen adjacency matrices. However, the qualitative analysis and the high value of the training loss (not shown) suggest that the model cannot perform any better. This provides evidence that a simple recurrent model such as GRU-B, at least in this form, is not well suited for the task of graph generation.

Our model obtainx excellent novelty and uniqueness scores in every dataset except LADDERS (for reasons explained above), with the notable mention of the PROTEINS dataset, where it obtains the best performances in every scored metric with a margin of 0.02 to 0.06 points with respect to the GraphRNN model. On the other hand, the GraphRNN model obtains a perfect score in the EGO dataset, beating our model with a margin of 0.3 to 0.1 points. However, our model is the most consistent across all the datsets, obtaining a score of above 0.87 in all considered scores in every dataset except LADDERS.

Note how all models obtain a perfect score in the COMMUNITY dataset. This can be explained by considering the nature of the COMMUNITY dataset, whose graphs are essentially composed by two random graphs weakly connected among each other. Thus, since generating a graph from that distribution is very similar to generating one at random, samples are highly likely to be different from each other.

As regards sampling time, random models are the fastest during generation; this was expected, since they have only 2 parameters, while all RNN-based models have a larger number of parameters. Among the RNN-based models, our model is the fastest at generating new samples. One exception is the COMMUNITY dataset, where however it elapses only 1.5 minutes more than the winning model, GRU-B. In contrast, the GraphRNN model has sampling times 5 to 20 times slower than our model. For completeness, however, we report that while we sampled from the GraphRNN model with batch size of 1 to achieve a fair comparison, its implementation allows to draw samples in batches, greatly speeding up the sampling process.

Table  shows the mean rank of the models for each considered metric (except time), averaged over all datasets. More precisely, for a given metric, we collect the scores of all models on that particular metric, sort the corresponding vector in descending order from highest to lowest score. The order of the vector is the corresponding rank. The ranks are finally averaged across all datasets, to provide a measure of how models behave globally. Results show that the BA model has the highest mean rank as regards novelty on a sample size of 1000 (for the reasons discussed above), while GraphRNN has the highest mean rank as regards novelty on a sample of 5000. Our model obtains the highest mean rank on uniqueness, on both sample sizes.


















Qualitative Analysis
Table  shows the results of the qualitative evaluation of the models. We remark that lower values are better.
It can be clearly seen how random models are unable to learn complex dependencies, scoring poorly on all datasets in every considered metric. One exception is the EGO dataset, where both the ER and BA models obtain the best KLD for the AOC metric (orbit counts) with 0.07 and 0.09, respectively (results are indistinguishable since the standard deviation intervals overlap). This result is in accordance with the argument made in the quantitative analysis, where we explained that random models fit the EGO with a small connectivity parameter. Since ego graphs have very low higher-order connectivity, in this case they are able to capture the correct orbit counts distribution. In the LADDERS dataset, the BA model obtains the best ACC (clustering coefficient), scoring  in all experiments. A similar argument can be made in this case: in fact, the clustering coefficient of ladder graphs is 0 by construction. Thus, it can be easily emulated with a very small connectivity parameter value. In contrast, note how the BA model completely fails at generating graphs with the correct node degree distribution, scoring the worst among all datasets. A similar trend happens to the ER model, providing evidence that random models are suited to model only a small subset of graph distributions.

































As regards the GRU-B model, it struggles especially with the orbit count distributions: this is easily explained by the fact that higher-order dependencies cannot be learned using the adjacency matrix alone. This reinforces the argument made with the quantitative analysis, that this model suffers from limited expressiveness.

Among all models, GraphRNN and ours perform consistently at the state of the art as regards the quality of generated graphs. In most cases, they perform indistinguishably. In one case, namely the EGO dataset, our model outperforms GraphRNN in all considered metrics. However, for the sake of fairness, we remind that results on the EGO dataset are probably slightly biased in favor of our model, which obtained lower uniqueness and novelty rates in that case. Thus, performances might be biased from repetitions of "good" graphs. While we did not investigate this issue any further, we also note that the margin by which our model outperforms GraphRNN is the highest among all datasets, with GraphRNN trailing by margins from 0.1 to 0.3 points. In contrast, note that whenever our model performs worse than GraphRNN, margins are instead noticeably narrower (the largest being 0.05 in the ENZYMES dataset on the AOC metric).

The global performances of the models across all datasets are summarized in Table , where the mean rank of all models is computed in a similar fashion as reported in the quantitative analysis. Our model obtains the highest mean ranking in all considered qualitative metrics.
















In Figure  we compare the empirical distribution of the three metrics on the test sample, versus the same metrics in the generated sample. The plots show that, even in cases where the test empirical distribution is skewed or multimodal, the true and generated distributions match coherently.






Finally, in Figure  we show graphs drawn from our model against real graphs, on three datasets. Indeed, visual inspection confirms that generated graphs display connectivity patterns typical of real graphs, for example ego nodes in EGO-like graphs, long chains in PROTEINS-like graphs, and dense clusters in the COMMUNITY-like graphs.







Effect of Node Ordering
As explained before, our first experiment to assess the effect of the different node ordering is to verify if the dataset can be overfit using different ordering strategies. In Figure , we plot the loss obtained by every variant in the COMMUNITY, EGO and PROTEINS dataset. The figure shows that the models trained with our strategy, the variant of our strategy based on DFS, and the SMILES ordering are able to reach a lower loss than the alternative strategies, which in contrast plateau at higher loss values. This provides evidence that the chosen node ordering strategy is well coupled with the architecture of for our model, and it is sufficiently general to learn connectivity patterns from very different graph datasets. Note that the results of the SMILES ordering are similar to ours as expected. In fact, our node ordering procedure is similar to the way SMILES orders the atoms in a molecular graph. However, we also remark that the SMILES ordering cannot be applied in general, but it is only suitable for molecular graphs.







We complement the discussion about node ordering with a qualitative analysis, whose results are detailed in Table . It can be easily noticed that the our node ordering strategy yields superior results in every chosen metric, with respect to every other variant. The only competitive node ordering strategy is DF, which basically differs from ours in the way graph nodes are visited. However, despite being able to this strategy seems to underperform in some cases, for example the clustering coefficient in the COMMUNITY dataset and the orbit count in the EGO dataset. We hypothesize this is due to using DF traversal instead of BF, which is less suited to capture local dependencies. In conclusion, these results further extend the evidences supporting the robustness of our approach.



































Conclusions
In this work, we have presented a novel generative model for graphs based on Recurrent Neural Networks, which is capable of generating high quality graphs from very different graph distributions. The key idea at the root of our work is to move from the domain of graphs to the one of sequences to simplify the learning task, while retaining most of the expressiveness. Our motivation to frame the generative process as learning problem on sequences is three-fold: (i) it is intuitive, (ii) it allows to work indirectly on smaller and informative portions of the exponentially large graph space, and (iii) it enables the use of the reliable and heavily optimized machinery of Recurrent Neural Networks to learn effectively.

With these ideas in mind, we developed a model which, first, orders nodes in a graph sequentially, then converts graphs into ordered sequences of edges, and finally learns these sequences using two RNNs. The first one predicts a sequence of source nodes, while the second uses the output of the first to predict all necessary endpoints to construct a novel graph. We tested our model against canonical random models from graph theory, a RNN baseline with limited expressiveness, and a state-of-the-art model on five different datasets. The evaluation was conducted on multiple levels, considering both quantitative performance measures such as novelty and uniqueness of generated graphs, as well as qualitative metrics to assess whether generated samples retain structural features of those in the training set. The results clearly show that our model performs at, and sometimes surpasses, the state of the art in the task.
We also conducted a study of how much the procedure chosen to superimpose an order on graph nodes is effective for the particular task of graph generation. More in detail, we compared with 5 variants of our model that were trained on different node orderings. Results show that with the ordering procedure of choice, the model can reach a lower training error, which is essential to learn the complex patterns needed to generate good quality graphs that resemble real-world ones.

The proposed model has also limitations. Even if it works empirically, the constraint imposed by the reliance on a specific ordering is unsatisfactory from a theoretical point of view. For this reason, one key step in future research is to study the node ordering problem more thoroughly, in order to relax and perhaps remove completely such constraint. Another limitation of our approach in its current form is the fact that it is not formulated to include node and edge labels, which are usually found in large classes of graph datasets such as molecular ones. While on one hand this limits the applicability of our approach on those domains, on the other we are also confident that extending the model to include node and edge labels might lead to improvements in generative tasks that deal with those kinds of graphs. Our belief is supported by the intuition that the ability of our model to well approximate structural features of graph distributions, coupled with a reliable mechanism to generate labels, could in perspective improve generalization, since conditioning the generative process not only on structure, but also on features, could help the model learn general connectivity patterns more reliably. This would also allow to compare our model to current molecular graph generator, and investigate the influence of domain specificity on this task.

As for other research directions, we mention the possibility to extend the model by adopting attention mechanisms bahdanau2015attention, instead of conditioning the second network only on the last hidden state of the first. Another easily implementable extension of our approach is to model the generative process using a Variational Auto-Encoder in between the two recurrent neural networks, in order to constrain the latent space where the encoding of the first sequence is mapped to be normally distributed. This would have the advantage of cutting sampling time by approximately half, since the inference phase would only require to sample a point from latent space, and decode it using the second network. Lastly, a challenge for the near future is to test whether our approach is capable to scale to larger graphs, which would strengthen our positive results further, as well as expand its applicability to a broader class of problems.

Deep Generative Learning for Drug Discovery 

The term de novo Drug Design (DD) refers to a plethora of methods for the production of novel chemical compounds endowed with desired pharmaceutical properties. It is an important step of the drug discovery pipeline, the process that goes from the identification of a biological target for which treatment is needed, to a novel drug hitting the markets. The cost of producing a new drug is now over 1 billion USD, and the average time taken to develop one is approximately 12 years ?. In this scenario, computational (or in-silico) methodologies are gradually substituting more traditional in-vitro solutions, as they allow to expedite almost every aspect of the discovery process, while also being ethically more sustainable (for example, avoiding experimenting on animals). In this chapter, we focus on Deep Learning-based generative methods for de-novo drug design.

Background

Molecules and their representation
A key concept in chemistry is the notion of compound, a group of atoms held together by the electrostatic attraction among the electrons surrounding the nuclei of the atoms. Depending on how this attraction is originated, we distinguish ionic and covalent bonds. An ionic bond between two neutrally charged atoms is formed whenever one loses an electron, and the other acquires it. The resulting charge imbalance creates attraction. Covalent bonds are instead formed whenever one or more pairs of electrons are shared between the two atoms. Compounds formed only by covalent bonds are called molecules. In our treatment of the subject, the focus will be on molecules rather than compounds; however, for colloquial reasons only, we will sometimes slightly abuse their definitions and refer to molecules using both terms.

Chemical Formulae 
To convey information about the composition of a molecule, often its chemical formula is given. A chemical formula concisely reports the number of atoms constituting the molecule. For example, the chemical formula of aspirin is , indicating that it contains nine Carbon (C) atoms, eight Hydrogen (H) atoms, and four Oxygen (O) atoms.

Molecular Fingerprints
Molecular fingerprints auer2008fingerprints,maggiora2014molecularsimilarity are a kind of molecular representation that facilitates structural comparison among molecules and allows fast retrieval in large databases. Essentially, a fingerprint is a binary vector whose bits indicate the presence (1) or absence (0) of a particular structural feature. Depending on how this structural features are calculated, we distinguish structural and hash fingerprints.

The bits of a structural fingerprint encode whether the molecule matches with a predefined set of substructures, that is if these substructures are induced subgraphs of the molecular graph. Two examples are the Molecular ACCess System (MACCS) keys (166 or 320 bits long), and the CACTVS substructure keys (881 bits long), used in the PubChem database.

Hash fingerprints implement a different approach. To calculate one bit of a hash fingerprint, all linear substructures with  atoms rooted at a particular atom are enumerated (where  is usually chosen between 3 and 7). Each linear substructure is transformed into a string based on its structural properties and then turned into an integer  by passing it through a hash function. The corresponding -th position in the fingerprint bit vector is then set to 1. Hash fingerprints come generally in lengths multiple of 8 bits, from 128 up to 4096. Prominent examples of this class of fingerprints are the Daylight Fingerprints and Extended Circular FingerPrints (ECFP) rogers2010ecfp.

SMILES
Very often in chemical tasks, molecules are translated into textual form. While many of such translations exist (such as the InChi), SMILES is by far the most widely employed in pratice. Essentially, the SMILES representation of a molecular graph is a string of ASCII characters, formed by variations of the following general algorithm:

    first, all the hydrogen atoms are removed (they can be inferred using valency rules), and the molecular graph is transformed into a spanning tree by removing cycles (disconnecting rings). The disconnected bonds are marked with unique integers, to allow their reconstruction;
    then, the spanning tree is visited in depth-first order; each time a new atom is visited, the corresponding atomic symbol (and a symbol indicating its chemical bond, if different from a single bond) is added to the SMILES string. Tree branches are marked by parentheses.

Even though SMILES strings are essentially sequences of characters, they are generally considered structured representations of the molecular graph, since they are constructed on its traversal. As a consequence of their construction method, SMILES strings are not unique, i.e. different SMILES strings can be associated with the same molecule by changing the root node of the spanning tree or the order in which cycles are broken. In fact, molecules with  atoms can be represented with at least  different SMILES strings. To this end, most chemical packages include canonicalization algorithms that ensure that one molecule is associated with a unique SMILES string (except few degenerate cases). Despite these issues, the SMILES encoding is widely adopted across the Cheminformatics landscape. For example, canonical SMILES are often used to index molecules in large databases.

Molecular Graph
Molecules naturally contain many information that the aforementioned representations cannot convey. A more comprehensive representation of the molecule is given by its structural formula. The structural formula of a molecule represents the arrangement of atoms through a molecular graph, also called Lewis structure, in which atoms are nodes, and bonds are links between them. The molecular graph can carry additional information through the features attached to the nodes and edges of the associated molecular graph. Typical features added to the atoms are:

    one-hot encoding of the atom type;
    number of Hydrogen atoms attached to a given atom;
    whether the atom is part of a ring or not;
    charge of the atom, its number of protons and electrons;
    stereochemistry and chirality information, its 3-D structure. A typical example are coordinates representing the position of the atom in 3-D space.

As regards molecular bonds, the features added are:

    a one-hot encoding of the bond type;
    whether the bond is part of a ring or not;
    stereochemistry information.


Generative Tasks
We identify three, different but connected, generative tasks in de-novo drug design where dgm can be used:

    molecular generation corresponds to the usual unconditional generation task. Given a dataset , where  are attributed molecular graphs, the objective is to approximate , their underlying probability, or a mechanism to sample from it;
    molecular optimization concerns optimizing some complex chemical property of compounds in a dataset of molecular graphs , which might be given or generated. In practice, a molecular optimization task often complements one of molecular generation;
    molecular translation is in some sense a supervised task of conditional generation. Here, the dataset is composed of pairs of attributed molecular graphs , where the targets  are structurally similar to the inputs , but with enhanced pharmaceutical properties. The task is to learn their relationship such that, when given an unseen compound , the model generates a similar and chemically enhanced compound .


Deep Generative Models of Molecules 
Broadly speaking, there are two major lines of research as regards de novo drug design with dgm, which are distinguished by the type of decoder they use to generate a molecule

    SMILES-based approaches generate molecules through their SMILES strings with autoregressive decoders. In this case, the architecture learns a language model of SMILES strings, a model that predicts the next SMILES character, given the previous characters;
    graph-based approaches learn to generate the molecular graph directly (with one-shot decoders) or incrementally (with autoregressive decoders);
    substructure-based approaches learn to generate the molecular graph compositionally, by combining smaller substructures together.

Below, we provide a brief overview of the literature in dgm models of molecules based on this taxonomy.

SMILES-based Approaches
A first class of SMILES-based approaches model the generation with a simple rnn decoder segler2017moleculelibrariesrnn,bjerrum2017molecularrnn. These models do not use a structured latent space, and hence are used only for molecular generation. In some cases, reinforcement learning techniques are adopted to perform molecular optimization olivecrona2017deepreinforcementlearningmolecules,neil2018rnnmolecule: in practice, the model is first pretrained autoregressively with mle; then, the molecules are optimized with reinforcement learning, by giving higher rewards to molecules that respect validity constraints, or whose properties are above a desired threshold. Historically, one of the first SMILES-based generative models using a latent space is that of gomez2018vaemolecule, which we term ChemVAE. Basically, the model is a seq2seq language model for SMILES strings, where the latent space is shaped through a vae. However, with respect to pure rnn approaches, the ChemVAE has been shown to produce a fairly high number of invalid molecules. A first improvement has been proposed by kusner2017grammarvae, which uses a rule-based generative model called GrammarVAE, which learns to generate productions of the SMILES grammar to produce syntactically valid molecules. Subsequently, dai2018sdvae proposed a Syntax-Directed (SD)-VAE, which uses a SMILES attributed grammar to enforce semantic constraints to the generation. These models are adapted to perform molecular optimization by jointy training a property predictor on the latent space; then, starting from a random point, a novel molecule with optimized properties is found in latent space using Bayesian optimization. Finally, some models use transfer learning to optimize molecular properties maragakis2020desmiles,grisoni2018transferlearningmolecules. In this case, the models are pretrained on a large dataset of molecules, and fine tuned on a smaller dataset of molecules with optimized properties.

Graph-based Approaches
Among graph-based approaches that generate the molecule in one shot, one of the first attempts was that of simonovsky2018graphvae. The model is a VAE, whose decoder generates three matrices: a probabilistic adjacency matrix, a matrix of node features (where the features are the atom types), and a matrix of edge features (where the features are the bond types). The sampled graph is aligned with the one in input with approximate graph matching. The approach of decao2018molgan uses a GAN, whose generator produces one probabilistic adjacency matrix for each bond type, and a node feature matrix, which are sampled to produce an actual graph. The graph is then used to train the discriminator, and also passed to a reward network that is used to optimize molecular properties with a reinforcement learning objective. Both the discriminator and the reward network are implemented as dgn. Among autoregressive graph decoders, one approach is that of popova2019molecularrnn, which extends GraphRNN you2018graphrnn to work with molecular generation (to output labeled graphs). Molecular optimization is achieved using a policy gradient optimization objective. The Graph Convolutional Policy Network (GCPN) by you2018graphconvpolicynetwork formulates the molecule generation as a Markov Decision Process, where actions consist in adding nodes to an existing graph. The Constrained Graph VAE by liu2018cgvae uses a different approach to generate a molecule: first, a set of node representations is sampled from latent space; then, the algorithm picks one node at a time, samples its atom type, and connects it to the remaining nodes, sampling the bond types. Optimization is achieved with gradient ascent (on the property value) in latent space. A similar generative approach is used in the work by samanta2019nevae, whilst optimization is achieved using Bayesian optimization as in gomez2018vaemolecule.

Substructure-based Approaches
One of the first work that used substrcture generation is that of jin2018jtvae, called Junction Tree (JT)-VAE. In the work, a junction tree of the molecule is built by considering some substructures such as rings as a single component. The model learns to generate junction trees, which guide the generation of the molecular graph. Thus, the model generates valid molecules by design. Optimization in latent space is achieved with Bayesian optimization. The model has been extended by the same authors in jin2019multimodalmoltranslation to a hierarchical model, where different levels of coarseness of the molecular graph are used to connect the different substructures. This work is also the first to introduce the molecular translation problem. In a similar fashion, fu2020core proposed CoRe, a JT-VAE enhanced with a Copy and Refine mechanism that learns how to copy some substructures from the input molecule, rather than sample them. Molecule Chef, by bradshaw2019moleculechef, operates under a different paradigm: the architecture is still a VAE (trained with Wasserstrain distance rather than the KLD), but trained to generate a sequence of reactants, rather than proper substructures of the desired molecule. A novel molecule is then synthesized by mapping the predicted reactants to a desired compound through a reaction predictor. The model is also extended to allow retro-synthesis, mapping a molecule to the most probably reactants that generated it. The model has later been extended in bradshaw2020barking, where the procedure has been modified to synthesize molecule through DAGs in a multi-step fashion, similarly to the actual procedure used in laboratories. Finally, the model by bostrom2019fragments works on chemical fragments, similarly to the contribution we present in Section . Specifically, the encode each fragment using a balanced binary tree, where similar fragments are placed in nearby leaves. The model is then trained to learn to replace fragments in an input molecule, to produce an optimized one.

Evaluation of Generative Models of Molecules
There are several metrics under which the molecular generators can be evaluated. Ideally, generated molecules should be at the same time:

    structurally different enough from training molecules to span the largest possible chemical subspace;
    structurally close enough to possess similar interesting chemical properties.

Thus, besides chemical validity, novelty and uniqueness, the diversity of the generated samples plays a major role. One measure of diversity requires to evaluate the average distance between all the possible pairs of generated molecules. This is called internal diversity. Conversely, external diversity measures the distance of each molecule in the generated sample against its nearest neighbor molecule in the training sample. The distance function usually employed to calculate diversity is the Tanimoto distance, which is a bit-wise distance function defined over the molecular fingerprints associated with a molecule. Another, more recent, diversity metric is the Frecht ChemNet Distance , which evaluates the distance between the training and generated samples by first feeding them to a pre-trained Deep Neural Network (ChemNet ) for property prediction, and then comparing several statistics computed on the respective activations at the penultimate layer.
For molecular generation tasks, other useful metrics to evaluate the generative performance are based on comparing the distributions of chemical properties between the training sample and the generated sample, as shown in Section . Typical properties evaluated to this aim are number of atoms, number of bonds, number of rings of the generated molecules (compared to the training sample). To evaluate performances on the optimization tasks, one intuitive metric to compute is the improvement, which measures the average increase (or decrease) of the property obtained in the generated sample, with respect to the training sample. In molecular translation tasks, often success is measured. It is the ratio of molecules in the generated sample that were correctly translated, meaning that their molecular properties are above some predefined threshold. Notice that success and improvement should be evaluated after having cleaned the generated sample from duplicates, to avoid biased results. In general, improvement and success cannot be calculated directly, as the ground truth value of the property may be not known or hardly computable for the generated molecule. This is often the case, for example, when the target is biological activity. In these cases, it is often useful to compare against an already trained predictor, to obtain an approximation of the ground truth property.

A Deep Generative Model for Fragment-Based Molecule Generation 
In this section, we present an original contribution in the context of SMILES-based Deep Generative Models of molecules.


A Primer on Fragments
Fragments are very-small-weight compounds, typically composed of  non-hydrogen atoms. Small size has several advantages: firstly, they are easier to manipulate chemically than larger fragments. Secondly, the chemical space of fragments is narrower than, for example, the one of drug-like molecules typically generated from other DD approaches such as HTS. Thus, it is easier to explore and characterize. Thirdly, the small size makes fragments weakly interact with a broader spectrum of target proteins than larger compounds (higher molecular complexity translates into strongest interaction, albeit not necessarily beneficial). A typical FBDD experiment begins with the identification of a suitable collection of fragments, from which a subset with desired interactions with the target (hits) is identified. Subsequently, fragments are optimized into higher affinity compounds that become the starting points (leads) for subsequent drug discovery phases. Optimization is commonly carried out according to three different strategies: a) linking, which optimizes a given fragment by connecting it with another fragment; b) growing, where the fragment is functionally and structurally enriched to optimize binding site occupation; c) merging, which involves combining the structure of two overlapping fragments into a new one with increased affinity.
Since its inception in 1996, FBDD accounts for two clinically approved drugs, and more than thirty undergoing clinical trials at various stages davis2017fbdd.

Methods
Here, we present the methodologies adopted in our study. Specifically, how the molecules have been broken into sequences of fragments, how these fragments have been embedded in vectorial form, and the architecture used during training and generation. Finally, we describe Low-Frequency Masking, the technique we use to boost the number of unique molecules produced by the model. We start off with a set  of molecular graphs, where each node is labeled with its atom type, and each edge is labelled with its bond type.

Molecule Fragmentation
Given a dataset of molecules, the first step of our approach entails breaking them into an ordered sequence of fragments. To do so, we leverage the Breaking of Retrosynthetically Interesting Chemical Substructures (BRICS) algorithm degen2008brics, which breaks strategic bonds in a molecule that match a set of chemical reactions. "Dummy" atoms (with atomic number 0) are attached to each end of the cleavage sites, marking the position where two fragments can be joined together. BRICS cleavage rules are designed to retain molecular components with valuable structural and functional content, e.g. aromatic rings and side-chains, breaking only single bonds that connect among them. Our fragmentation algorithm works by scanning atoms in the order imposed by the SMILES encoding. As soon as a breakable bond (according to the BRICS rules) is encountered during the scan, the molecule is broken in two at that bond, applying a matching chemical reaction. After the cleavage, we collect the leftmost fragment, and repeat the process on the rightmost fragment in a recursive fashion. Note that fragment extraction is ordered from left to right according to the SMILES representation; this makes the process fully reversible, i.e. it is possible to reconstruct the original molecule from a sequence of fragments. In Figure , we show a practical example of the fragmentation process on the molecule of aspirin.






After the fragmentation phase, the set of molecular graphs  is transformed in a set of fragment sequences . We also add an additional data structure, a vocabulary , which holds the one-hot vector of every unique fragment found during the fragmentation process. We indicate with  the one-hot vector in the vocabulary corresponding to fragment , and with  the size of the vocabulary.

Skip-Gram Embedding of Fragments
In this phase, we pretrain the one-hot vectors specifying the fragments into continuous vectors enriched with context semantics. In analogy with the work of bowman2016sentencescontinuousspace, we view a sequence of fragments as a sentence; therefore, each fragment is a word. Given a sequence  of fragments, this objective corresponds to optimizing the following skipgram loss function mikolov2014skipgram:

where  is called context window,  is an input fragment fragment, and  are fragments that occur in the context of . We implement  as the following neural network:
*
    fi &= E V(f_[i])

    oi &= (E_outfi),

where  is an embedding matrix and  is an output matrix. The model is trained with negative sampling mikolov2014skipgram. After training, the columns of the embedding matrix  contains -dimensional embeddings of each fragment in the vocabulary. As a result, fragment embeddings are endowed with context semantics: namely, embeddings that are close in a sequence of fragments are represented by nearby vectors in embedding space.

Training
The model architecture is a standard vae s2s model for graph generation, composed of an encoder  and an autoregressive graph decoder , where  is a generic input fragment sequence. The encoder is a rnn that operates at fragment level and computes the following:
*
    fi &= E V(f_[i])

    hi &= GRU_hi-1, fi,

where  is the pretrained embedding matrix, and  as usual. The final hidden state  is used as a representation of the whole fragment sequence. The sequence representation is then mapped to two vectors as follows:
*
    &= W__ hs

    &= W__ hs,

 and  represent the mean and variance of the encoding distribution, and  are weight matrices. Subsequently, a latent sample from the encoding distribution is drawn with reparameterization as , with , where  is a unit covariance matrix. The encoder is trained to minimize the kld between the distribution parameterized by  and  and a standard Gaussian prior:

The decoder is initialized with the latent vector, by setting . Specifically, the decoder is a rnn that computes the following:
*
    fi-1 &= E V(f_[i-1])

    h'i &= GRU_(hi-1, fi-1)

    o'i &= V_h'i + b'_,

where ,  is the shared embedding matrix, and ,  are the parameters of the softmax output layer. The vector  is the output distribution over all possible fragments in the vocabulary. Each output of the network is compared with the ground truth fragment  with the ce function as follows:

where  is added to account for the end of sequence token. The whole model is trained with mle to minimize the following objective function:

where  as usual. The architecture is shown visually in Figure .








Generation
Generation starts by sampling a latent point  from the standard gaussian ,
and setting  to initialize the decoder. The initial hidden state is fed to the network together with the starting token . At each step, the network produces a conditional output distribution on every possible word in the vocabulary, from which a fragment  is taken using greedy sampling. The sampled token then becomes the input of the next decoding step, together with the updated hidden state. The process is repeated until the end of sequence token  is sampled, at which point the generation is interrupted. The fragments of the sampled sequence  are then joined together in the same order to form a novel molecule. Figure  shows the process visually.







Low-Frequency Masking
To foster molecule diversity, we start from the observation that the distribution of fragments in the data can be roughly approximated by a power law distribution. In fact, there is usually a small number of fragments with very high frequency, as opposed to a very large number of fragments that occur rarely. Hence, infrequent fragments are unlikely to be sampled during generation. To counter this, we develop a strategy which we term Low-Frequency Masking (LFM). During training, we mask fragments with frequency below a certain threshold  with a token composed of its frequency and the number of attachment points. As an example, suppose that fragment *Nc1ccc(O*)cc1 occurs 5 times in the dataset, and the threshold is . Thus, this fragment is masked with the token 52, where 5 denotes its frequency, and 2 denotes the number of attachment points. Similarly, fragment *C(=O)N1CCN(Cc2ccccc2)CC1 with frequency of 3 is masked with the token 31. In contrast, fragment *c1ccccc1OC with a frequency of 200 is left unmasked, since its frequency is above the threshold. A reverse mapping from the masking tokens to the masked fragments is kept. During sampling, whenever a masking token is sampled, we replace it with a fragment sampled with uniform probability from the corresponding set of masked fragments. This strategy serves a double purpose. Firstly, it greatly reduces vocabulary size during training, speeding up the computations. Secondly, it fosters molecule diversity by indirectly boosting the probability of infrequent fragments, and injecting more randomness in the sampling process at the same time. From another point of view, LFM forces the model to generate molecules mostly composed of very frequent fragments, but with infrequent substructures that may vary uniformly from molecule to molecule.

Experiments
Following, we review our experimental setup, namely how experiments are conceived, which dataset and evaluation metrics were used, which baselines we compare to, as well as details about the hyper-parameters of our model. In our experiments, we try to provide an empirical answer to the following questions:

    Q1: is our fragment-based language model able to increase validity rates?
    Q2: is our LFM strategy beneficial to increase uniqueness rates?

To answer the first question, we compare our model against character-based baselines, which generate molecules atom by atom. As regards the second question, we perform an ablation study of performances with and without LFM. We also compare against graph-based approaches, to assess performances in relation to models that use more expressive molecule representations.

Data
We experiment on the ZINC dataset irwin2005zinc, consisting of  250k drug-like compounds. ZINC is a common benchmark for the generative task; as such, it is used to compare against several baselines. To assess the impact of LFM further, in our ablation study we also test our model variants with the PubChem BioAssay (PCBA) dataset  gindulyte2016pubchem, which comprises  440k small molecules. Dataset statistics are presented in Table . We applied some common preprocessing steps before training. In the PCBA dataset, we found 10822 duplicate or invalid molecules, which were removed. After fragmentation, we discarded molecules composed of  fragments. After preprocessing, our training samples were  (ZINC) and  (PCBA). For completeness, we report that we tried to test our model on the QM9 dataset ramakrishnan2014qm9 as well, but found out that approximately 70 of its molecules are composed of a single fragment, making assessment poorly informative due to the small sample size.






















Performance Metrics
Following the standards to evaluate molecular generators, we compare our model with the baselines quantitatively, measuring validity, novelty and uniqueness. Validity is assessed by checking the SMILES of the generated graph through the SMILES validator of the rdkit library in Python rdkit. To assess the quality of our samples, we compare the distributions of number of atoms, number of bonds and number of rings between the sampled and generated molecules. Moreover, we compare the distributions of the following molecular properties:

    octanol/water Partition coefficient (logP), which measures solubility;
    Quantitative Estimate of Drug-likeness bickerton2012qed (QED), which measures drug-likeness;
    Synthetic Accessibility Score ertl2009sas (SAS), which measures ease of synthesis.

In both cases, we remove duplicates before the valuation, to ensure
Baselines
We compare to baselines found in literature, representing both SMILES and graph-based generative models described in Section . As regards SMILES-based approaches, we consider ChemVAE gomez2018vaemolecule, GrammarVAE kusner2017grammarvae and SD-VAE dai2018sdvae, whereas as regards graph-based models, we compare against GraphVAE simonovsky2018graphvae, CGVAE liu2018cgvae and NeVAE samanta2019nevae.

Hyper-Parameters
We evaluate our model using the same hyper-parameters for both variants, in order to isolate the effect of our contribution from improvements due to hyper-parameter tuning. We set the embedding dimension to 64, the number of recurrent layers to 2, the number of GRU units per layer to 128 and the latent space size to 100. We used the Adam optimizer with an initial learning rate of 0.00001, annealed every epoch by a multiplicative factor of 0.9, a batch size of 128, and a dropout rate of 0.3 applied to the recurrent layers to prevent overfitting. Training required only 4 epochs: after that, we found empirically that the model started to severely overfit the training set. We used  as LFM threshold. The stopping criteria for training is the following: after each epoch, we sample 1000 molecules and measure validity, novelty and uniqueness rates of the sample, stopping whenever the uniqueness rate starts to drop (we found out empirically that samples were stable in terms of validity and novelty rates). After training, we sample 20k molecules for evaluation. We publicly release code and samples for reproducibility . Baseline results are taken from literature .

Results
The main results of our experiments are summarized in Table , and provide the answers to the experimental questions posed in Section . As regards Q1, we observe that our model achieves perfect validity scores in the ZINC data, greatly outperforming LM-based models and performing on par with the state of the art. This is true also as regards the PCBA dataset. Since both our variants improve over the LM-based competitors, it is safe to argue that our fragment-based approach can effectively increase validity rates. As regards Q2, we observe an improvement in uniqueness by both our variants, with respect to the LM-based competitors. However, the improvement is noticeably higher whenever the LFM strategy is employed. In the PCBA this trend is even more pronounced. Compared to graph-based models, we see how the model with LFM is now competitive with the state of the art. Lastly, we notice that using LFM yields a small improvement in novelty with respect to the vanilla variant.
























In Figure  (for the ZINC dataset) and Figure  (as regards the PCBA dataset), our samples against training compounds are compared, as regards the distribution of the three structural features, and molecular properties listed above. Notice that even without the help of an explicit supervision, generated molecules are qualitatively similar to the training data.






Finally, Figure  shows two random samples of 30 molecules taken from the ZINC dataset and generated by our model for visual comparison.
























Conclusions
Conclusions, Opportunities and Future Works 



















[heading=bibintoc]





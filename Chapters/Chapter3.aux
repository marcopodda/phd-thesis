\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Deep Learning in Structured Domains}{31}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:deep-learning-structures}{{3}{31}{Deep Learning in Structured Domains}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Graphs}{31}{section.3.1}}
\newlabel{sec:graphs}{{3.1}{31}{Graphs}{section.3.1}{}}
\newlabel{fig:undirected-graph}{{3.1a}{32}{An undirected graph.\relax }{figure.caption.23}{}}
\newlabel{sub@fig:undirected-graph}{{a}{32}{An undirected graph.\relax }{figure.caption.23}{}}
\newlabel{fig:directed-graph}{{3.1b}{32}{A directed graph.\relax }{figure.caption.23}{}}
\newlabel{sub@fig:directed-graph}{{b}{32}{A directed graph.\relax }{figure.caption.23}{}}
\newlabel{fig:bipartite-graph}{{3.1c}{32}{A bipartite graph.\relax }{figure.caption.23}{}}
\newlabel{sub@fig:bipartite-graph}{{c}{32}{A bipartite graph.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Three examples of graphs.\relax }}{32}{figure.caption.23}}
\@writefile{toc}{\contentsline {paragraph}{Directed Graphs}{32}{section*.24}}
\@writefile{toc}{\contentsline {paragraph}{Bipartite Graphs}{32}{section*.25}}
\@writefile{toc}{\contentsline {paragraph}{Walks, Paths, and Cycles}{32}{section*.26}}
\newlabel{fig:sequence}{{3.2a}{33}{A sequence.\relax }{figure.caption.28}{}}
\newlabel{sub@fig:sequence}{{a}{33}{A sequence.\relax }{figure.caption.28}{}}
\newlabel{fig:tree}{{3.2b}{33}{A tree.\relax }{figure.caption.28}{}}
\newlabel{sub@fig:tree}{{b}{33}{A tree.\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Special classes of graphs.\relax }}{33}{figure.caption.28}}
\@writefile{toc}{\contentsline {paragraph}{Trees and Sequences}{33}{section*.27}}
\@writefile{toc}{\contentsline {paragraph}{Subgraphs and Induced Subgraphs}{34}{section*.29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Attributed Graphs}{34}{subsection.3.1.1}}
\newlabel{sec:attr-graphs}{{3.1.1}{34}{Attributed Graphs}{subsection.3.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Isomorphisms, Automorphisms, and Canonization}{34}{subsection.3.1.2}}
\newlabel{sec:isomorphisms}{{3.1.2}{34}{Isomorphisms, Automorphisms, and Canonization}{subsection.3.1.2}{}}
\newlabel{fig:isomorphism}{{3.3a}{35}{Isomorphism.\relax }{figure.caption.30}{}}
\newlabel{sub@fig:isomorphism}{{a}{35}{Isomorphism.\relax }{figure.caption.30}{}}
\newlabel{fig:automorphism}{{3.3b}{35}{Automorphism.\relax }{figure.caption.30}{}}
\newlabel{sub@fig:automorphism}{{b}{35}{Automorphism.\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces An example of isomorphism and automorphism.\relax }}{35}{figure.caption.30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Graphs Matrices}{35}{subsection.3.1.3}}
\newlabel{sec:adj-matrix}{{3.1.3}{35}{Graphs Matrices}{subsection.3.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The Adaptive Processing of Structured Data}{36}{section.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Recurrent Neural Networks}{37}{section.3.3}}
\newlabel{sec:rnns}{{3.3}{37}{Recurrent Neural Networks}{section.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Training}{38}{subsection.3.3.1}}
\newlabel{fig:rnn}{{3.4a}{39}{\relax }{figure.caption.31}{}}
\newlabel{sub@fig:rnn}{{a}{39}{\relax }{figure.caption.31}{}}
\newlabel{fig:rnn-unfold}{{3.4b}{39}{\relax }{figure.caption.31}{}}
\newlabel{sub@fig:rnn-unfold}{{b}{39}{\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces ({\relax \fontsize  {8}{9.5}\selectfont  A}): An example of recurrent neural network that can learn a structure-to-structure task. ({\relax \fontsize  {8}{9.5}\selectfont  B}): the same network unfolded over a training pair of sequences of length $m$.\relax }}{39}{figure.caption.31}}
\newlabel{fig:rnn-example}{{3.4}{39}{({\scriptsize A}): An example of recurrent neural network that can learn a structure-to-structure task. ({\scriptsize B}): the same network unfolded over a training pair of sequences of length $m$.\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Gated Recurrent Neural Networks}{39}{subsection.3.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Recurrent Neural Networks as Autoregressive Models}{40}{subsection.3.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces An example of training a recurrent neural network for learning an autoregressive distribution. The dashed arrows indicate non-differentiable operations.\relax }}{41}{figure.caption.32}}
\newlabel{fig:rnn-autoregressive}{{3.5}{41}{An example of training a recurrent neural network for learning an autoregressive distribution. The dashed arrows indicate non-differentiable operations.\relax }{figure.caption.32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Recursive Neural Networks}{41}{section.3.4}}
\newlabel{sec:recnns}{{3.4}{41}{Recursive Neural Networks}{section.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces The teacher forcing strategy for training a sequence generator with RNNs.\relax }}{42}{figure.caption.33}}
\newlabel{fig:teacher-forcing}{{3.6}{42}{The teacher forcing strategy for training a sequence generator with RNNs.\relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Training}{43}{subsection.3.4.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Deep Graph Networks}{43}{section.3.5}}
\newlabel{sec:dgns}{{3.5}{43}{Deep Graph Networks}{section.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces A recursive neural network unfolded over the tree of Figure \ref  {fig:tree} for a structure-to-element task. The number at the left of the hidden states indicates the order in which they are calculated. Notice the initialization of the hidden states at the leaves (indicated by dashed boxes).\relax }}{44}{figure.caption.34}}
\newlabel{fig:recnn}{{3.7}{44}{A recursive neural network unfolded over the tree of Figure \ref {fig:tree} for a structure-to-element task. The number at the left of the hidden states indicates the order in which they are calculated. Notice the initialization of the hidden states at the leaves (indicated by dashed boxes).\relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Contextual Processing of Graph Information}{45}{subsection.3.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Context diffusion through message passing. Directed edges represent messages (\textit  {e.g.~}from node $u$ to $v$ at iteration $\ell =2$). Dashed arrows represent the implicit contextual information received by a node (in dark grey) through the messages from its neighbors (in light grey). Focusing on node $v$, its context at iteration $\ell =2$ is composed all the dark grey nodes (including $v$ itself).\relax }}{46}{figure.caption.35}}
\newlabel{fig:context-diffusion}{{3.8}{46}{Context diffusion through message passing. Directed edges represent messages (\eg from node $u$ to $v$ at iteration $\ell =2$). Dashed arrows represent the implicit contextual information received by a node (in dark grey) through the messages from its neighbors (in light grey). Focusing on node $v$, its context at iteration $\ell =2$ is composed all the dark grey nodes (including $v$ itself).\relax }{figure.caption.35}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Building Blocks of Deep Graph Networks}{47}{subsection.3.5.2}}
\newlabel{sec:graph-conv-layers}{{3.5.2}{47}{Graph Convolutional Layers}{section*.39}{}}
\newlabel{eq:simple-aggregation}{{3.1}{47}{Graph Convolutional Layers}{equation.3.5.1}{}}
\newlabel{eq:convolutional}{{3.2}{48}{Graph Convolutional Layers}{equation.3.5.2}{}}
\newlabel{eq:transform}{{3.3}{48}{Graph Convolutional Layers}{equation.3.5.3}{}}
\newlabel{eq:aggregate}{{3.4}{48}{Graph Convolutional Layers}{equation.3.5.4}{}}
\newlabel{eq:update}{{3.5}{48}{Graph Convolutional Layers}{equation.3.5.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Handling Edges}{49}{section*.40}}
\@writefile{toc}{\contentsline {paragraph}{Node Attention}{50}{section*.41}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces An example of node attention. Note that edge thickness is not related to the strength of the connection between node $v$ (in dark grey) and its neighbors (in light grey), but it represents the degree of similarity between the node states, quantified in a probabilistic sense by the attention score. Dashed edges connect nodes that are not involved in the attention score computation.\relax }}{50}{figure.caption.42}}
\newlabel{fig:attention}{{3.9}{50}{An example of node attention. Note that edge thickness is not related to the strength of the connection between node $v$ (in dark grey) and its neighbors (in light grey), but it represents the degree of similarity between the node states, quantified in a probabilistic sense by the attention score. Dashed edges connect nodes that are not involved in the attention score computation.\relax }{figure.caption.42}{}}
\@writefile{toc}{\contentsline {paragraph}{Node Sampling}{50}{section*.43}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces An example of node sampling. We focus on a node $v$ (in dark grey), and its neighbors (in light grey). Dashed edges connect nodes that are not involved in the neighborhood aggregation; notice how nodes $u_1$ and $u_2$ are excluded from the neighborhood aggregation, even though they are effectively neighbors of $v$.\relax }}{51}{figure.caption.44}}
\newlabel{fig:sampling}{{3.10}{51}{An example of node sampling. We focus on a node $v$ (in dark grey), and its neighbors (in light grey). Dashed edges connect nodes that are not involved in the neighborhood aggregation; notice how nodes $u_1$ and $u_2$ are excluded from the neighborhood aggregation, even though they are effectively neighbors of $v$.\relax }{figure.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces The role of hidden state readout function in a node classification/regression task. Here, we focus on a single node $\ensuremath  {\boldsymbol  {\lowercase {h}}}_{[v]}^{(\ell )}$, where we drop the subscripts to avoid visual cluttering. Grey nodes replace the $[v]$ subscript. The hidden state readout function creates a node representation $\ensuremath  {\boldsymbol  {\lowercase {h}}}_{[v]}^{(*)}$ by combining its three hidden states (one for each layer). Successively, the node representation is turned into an output by an output layer, and compared by the loss function to the same node $\ensuremath  {\boldsymbol  {\lowercase {y}}}_{[v]}$ in the isomorphic target graph. This operation is repeated for every node in the graph.\relax }}{52}{figure.caption.46}}
\newlabel{fig:node-readout}{{3.11}{52}{The role of hidden state readout function in a node classification/regression task. Here, we focus on a single node $\StateVector {v}{\ell }$, where we drop the subscripts to avoid visual cluttering. Grey nodes replace the $[v]$ subscript. The hidden state readout function creates a node representation $\StateVector {v}{*}$ by combining its three hidden states (one for each layer). Successively, the node representation is turned into an output by an output layer, and compared by the loss function to the same node $\Elem {y}{v}$ in the isomorphic target graph. This operation is repeated for every node in the graph.\relax }{figure.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces A graph readout on an example graph for a graph classification/regression task. Here, we assume the node representations $\ensuremath  {\boldsymbol  {\lowercase {h}}}_{[v]}^{(*)}$ have already been obtained by a node readout (not shown).\relax }}{53}{figure.caption.47}}
\newlabel{fig:graph-readout}{{3.12}{53}{A graph readout on an example graph for a graph classification/regression task. Here, we assume the node representations $\StateVector {v}{*}$ have already been obtained by a node readout (not shown).\relax }{figure.caption.47}{}}
\newlabel{sec:pooling}{{3.5.2}{53}{Graph Pooling Layers}{section*.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces A visual example of a graph pooling layer.\relax }}{54}{figure.caption.49}}
\newlabel{fig:pooling}{{3.13}{54}{A visual example of a graph pooling layer.\relax }{figure.caption.49}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Regularization}{54}{subsection.3.5.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Deep Generative Learning on Graphs}{54}{section.3.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}The Challenges of Graph Generation}{55}{subsection.3.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Generative Tasks}{56}{subsection.3.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Graph Decoders}{57}{subsection.3.6.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.4}Performance Evaluation}{60}{subsection.3.6.4}}
\newlabel{sec:evaluation-generative-graphs}{{3.6.4}{60}{Performance Evaluation}{subsection.3.6.4}{}}
\@setckpt{Chapters/Chapter3}{
\setcounter{page}{63}
\setcounter{equation}{6}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{4}
\setcounter{mpfootnote}{0}
\setcounter{part}{1}
\setcounter{chapter}{3}
\setcounter{section}{6}
\setcounter{subsection}{4}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{13}
\setcounter{table}{0}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{160}
\setcounter{maxnames}{1}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{2}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{1}
\setcounter{textcitetotal}{1}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{parentequation}{0}
\setcounter{su@anzahl}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{6}
\setcounter{bookmark@seq@number}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{theorem}{0}
\setcounter{corollary}{0}
\setcounter{lemma}{0}
\setcounter{definition}{0}
\setcounter{section@level}{2}
}

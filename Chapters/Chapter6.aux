\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}A Model for Edge-Based Graph Generation}{105}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:deep-generative-learning-graphs}{{6}{105}{A Model for Edge-Based Graph Generation}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Methods}{105}{section.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Ordered Edge Sequences}{105}{subsection.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Model}{106}{subsection.6.1.2}\protected@file@percent }
\newlabel{fig:example-graph}{{6.1a}{107}{\relax }{figure.caption.92}{}}
\newlabel{sub@fig:example-graph}{{a}{107}{\relax }{figure.caption.92}{}}
\newlabel{fig:labeled-graph}{{6.1b}{107}{\relax }{figure.caption.92}{}}
\newlabel{sub@fig:labeled-graph}{{b}{107}{\relax }{figure.caption.92}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces ({\scriptsize  A}): an example graph, where the starting node $v_1$ of the labelling algorithm is marked with a thicker border. ({\scriptsize  B}): the same graph, where nodes are labeled according to a breadth-first visit of the graph rooted at $v_1$.\relax }}{107}{figure.caption.92}\protected@file@percent }
\newlabel{fig:labelling-example}{{6.1}{107}{({\scriptsize A}): an example graph, where the starting node $v_1$ of the labelling algorithm is marked with a thicker border. ({\scriptsize B}): the same graph, where nodes are labeled according to a breadth-first visit of the graph rooted at $v_1$.\relax }{figure.caption.92}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Training}{107}{subsection.6.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces A depiction of the proposed architecture during training. We set $|\tau _S| = |\tau _E| = m$ to avoid cluttering. Notice that the embedding matrix is shared between the two networks.\relax }}{109}{figure.caption.93}\protected@file@percent }
\newlabel{fig:model-training}{{6.2}{109}{A depiction of the proposed architecture during training. We set $|\tau _S| = |\tau _E| = m$ to avoid cluttering. Notice that the embedding matrix is shared between the two networks.\relax }{figure.caption.93}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.4}Generation}{109}{subsection.6.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.5}Implementation details}{109}{subsection.6.1.5}\protected@file@percent }
\newlabel{sec:implementation}{{6.1.5}{109}{Implementation details}{subsection.6.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Graph generation using the proposed model. The output of the network are two sequences: one is a stochastic sequence $\ensuremath  {\boldsymbol  {\lowercase {\tilde  {s}}}}$ representing the starting sequence of the graph. The other is a deterministic ending sequence $\ensuremath  {\boldsymbol  {\lowercase {\hat  {e}}}}$, predicted using $\ensuremath  {\boldsymbol  {\lowercase {\tilde  {s}}}}$ as input. The two sequences are ultimately combined to produce the ordered edge sequence of a novel graph (not shown).\relax }}{110}{figure.caption.94}\protected@file@percent }
\newlabel{fig:model-sampling}{{6.3}{110}{Graph generation using the proposed model. The output of the network are two sequences: one is a stochastic sequence $\Vector {\tilde {s}}$ representing the starting sequence of the graph. The other is a deterministic ending sequence $\Vector {\hat {e}}$, predicted using $\Vector {\tilde {s}}$ as input. The two sequences are ultimately combined to produce the ordered edge sequence of a novel graph (not shown).\relax }{figure.caption.94}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Experiments}{110}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Datasets}{110}{subsection.6.2.1}\protected@file@percent }
\newlabel{sec:datasets}{{6.2.1}{110}{Datasets}{subsection.6.2.1}{}}
\newlabel{fig:ladder}{{6.4a}{112}{LADDERS.\relax }{figure.caption.95}{}}
\newlabel{sub@fig:ladder}{{a}{112}{LADDERS.\relax }{figure.caption.95}{}}
\newlabel{fig:community}{{6.4b}{112}{COMMUNITY.\relax }{figure.caption.95}{}}
\newlabel{sub@fig:community}{{b}{112}{COMMUNITY.\relax }{figure.caption.95}{}}
\newlabel{fig:ego}{{6.4c}{112}{EGO.\relax }{figure.caption.95}{}}
\newlabel{sub@fig:ego}{{c}{112}{EGO.\relax }{figure.caption.95}{}}
\newlabel{fig:btree}{{6.4d}{112}{TREES.\relax }{figure.caption.95}{}}
\newlabel{sub@fig:btree}{{d}{112}{TREES.\relax }{figure.caption.95}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Examples of the synthetic graphs used for the evaluation.\relax }}{112}{figure.caption.95}\protected@file@percent }
\newlabel{fig:synthetic-graphs}{{6.4}{112}{Examples of the synthetic graphs used for the evaluation.\relax }{figure.caption.95}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Baselines}{112}{subsection.6.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Statistics of datasets used in the experiments.\relax }}{113}{table.caption.96}\protected@file@percent }
\newlabel{tab:generation-datasets}{{6.1}{113}{Statistics of datasets used in the experiments.\relax }{table.caption.96}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}Evaluation Framework}{114}{subsection.6.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Results}{114}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Quantitative Analysis}{114}{subsection.6.3.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Results of the quantitative analysis of the generated samples. In the leftmost column, both the metric of interest as well as the sample size (either 1000 or 5000) is specified. Best performances of models based on RNNs are bolded.\relax }}{115}{table.caption.97}\protected@file@percent }
\newlabel{tab:graph-quantitative}{{6.2}{115}{Results of the quantitative analysis of the generated samples. In the leftmost column, both the metric of interest as well as the sample size (either 1000 or 5000) is specified. Best performances of models based on RNNs are bolded.\relax }{table.caption.97}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Qualitative Analysis}{116}{subsection.6.3.2}\protected@file@percent }
\newlabel{sec:qualitative-analysis}{{6.3.2}{116}{Qualitative Analysis}{subsection.6.3.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Results of the qualitative analysis of the generated samples. The three metrics considered are KLDs calculated on Average Degree Distribution (ADD), Average Clustering Coefficient (ACC), and Average Orbit Count (AOD). Best performances are bolded.\relax }}{117}{table.caption.98}\protected@file@percent }
\newlabel{tab:graph-qualitative}{{6.3}{117}{Results of the qualitative analysis of the generated samples. The three metrics considered are KLDs calculated on Average Degree Distribution (ADD), Average Clustering Coefficient (ACC), and Average Orbit Count (AOD). Best performances are bolded.\relax }{table.caption.98}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces Mean ranks obtained on the evaluated qualitative metrics by the examined models over all datasets. Best performances are bolded.\relax }}{118}{table.caption.99}\protected@file@percent }
\newlabel{tab:graph-qualitative-rank}{{6.4}{118}{Mean ranks obtained on the evaluated qualitative metrics by the examined models over all datasets. Best performances are bolded.\relax }{table.caption.99}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Effect of Node Ordering}{118}{subsection.6.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces We plot the KDE fitted on the histograms of graph statistics, taken both from the test and generated samples. Scales are omitted since values are normalized.\relax }}{119}{figure.caption.100}\protected@file@percent }
\newlabel{fig:distributions}{{6.5}{119}{We plot the KDE fitted on the histograms of graph statistics, taken both from the test and generated samples. Scales are omitted since values are normalized.\relax }{figure.caption.100}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Five randomly chosen graphs, both from the training sample (left) and generated by our model (right), on each considered dataset (displayed by row). Notice how our generative model has picked up all the characterizing connectivity patterns.\relax }}{120}{figure.caption.101}\protected@file@percent }
\newlabel{fig:samples}{{6.6}{120}{Five randomly chosen graphs, both from the training sample (left) and generated by our model (right), on each considered dataset (displayed by row). Notice how our generative model has picked up all the characterizing connectivity patterns.\relax }{figure.caption.101}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Plot of the loss on dataset Protein of variants of our approach trained under different node orderings. Notice how the models trained with our proposed node ordering (blue), and SMILES (orange) are able to reach a lower training loss in a smaller number of epochs, compared to variants trained with random ordering (red) and BFS random ordering (green).\relax }}{121}{figure.caption.102}\protected@file@percent }
\newlabel{fig:loss}{{6.7}{121}{Plot of the loss on dataset Protein of variants of our approach trained under different node orderings. Notice how the models trained with our proposed node ordering (blue), and SMILES (orange) are able to reach a lower training loss in a smaller number of epochs, compared to variants trained with random ordering (red) and BFS random ordering (green).\relax }{figure.caption.102}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.5}{\ignorespaces We show the results of training the model with different node ordering strategies. Best performances are bolded.\relax }}{122}{table.caption.103}\protected@file@percent }
\newlabel{tab:graph-ordering-qualitative}{{6.5}{122}{We show the results of training the model with different node ordering strategies. Best performances are bolded.\relax }{table.caption.103}{}}
\@setckpt{Chapters/Chapter6}{
\setcounter{page}{123}
\setcounter{equation}{0}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{1}
\setcounter{mpfootnote}{0}
\setcounter{part}{3}
\setcounter{chapter}{6}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{7}
\setcounter{table}{5}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{250}
\setcounter{maxnames}{1}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{6}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{2}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{1}
\setcounter{textcitetotal}{1}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{parentequation}{0}
\setcounter{su@anzahl}{0}
\setcounter{Item}{3}
\setcounter{Hfootnote}{12}
\setcounter{bookmark@seq@number}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{theorem}{0}
\setcounter{corollary}{0}
\setcounter{lemma}{0}
\setcounter{definition}{0}
\setcounter{section@level}{2}
}

\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Deep Graph Networks}{41}{chapter.65}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:dgn}{{4}{41}{Deep Graph Networks}{chapter.65}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Contextual Processing of Graph Information}{42}{section.66}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Recursive Approaches}{42}{subsection.68}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Context diffusion through message passing. Directed edges represent messages (\textit  {e.g.~}from node $u$ to $v$ at iteration $\ell =2$). Dashed arrows represent the implicit contextual information received by a node (in dark grey) through the messages from its neighbors (in light grey). Focusing on node $v$, its context at iteration $\ell =2$ is composed all the dark grey nodes (including $v$ itself).\relax }}{43}{figure.caption.67}}
\newlabel{fig:context-diffusion}{{4.1}{43}{Context diffusion through message passing. Directed edges represent messages (\eg from node $u$ to $v$ at iteration $\ell =2$). Dashed arrows represent the implicit contextual information received by a node (in dark grey) through the messages from its neighbors (in light grey). Focusing on node $v$, its context at iteration $\ell =2$ is composed all the dark grey nodes (including $v$ itself).\relax }{figure.caption.67}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Feed-Forward Approaches}{43}{subsection.69}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Constructive Approaches}{44}{subsection.70}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Building Blocks of Deep Graph Networks}{44}{section.71}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Graph Convolutional Layers}{44}{subsection.72}}
\newlabel{sec:graph-conv-layers}{{4.2.1}{44}{Graph Convolutional Layers}{subsection.72}{}}
\newlabel{eq:simple-aggregation}{{4.1}{44}{Graph Convolutional Layers}{equation.73}{}}
\newlabel{eq:convolutional}{{4.2}{45}{Graph Convolutional Layers}{equation.75}{}}
\newlabel{eq:transform}{{4.3}{45}{Graph Convolutional Layers}{equation.76}{}}
\newlabel{eq:aggregate}{{4.4}{45}{Graph Convolutional Layers}{equation.77}{}}
\newlabel{eq:update}{{4.5}{45}{Graph Convolutional Layers}{equation.78}{}}
\@writefile{toc}{\contentsline {paragraph}{Handling Edges}{46}{section*.79}}
\@writefile{toc}{\contentsline {paragraph}{Node Attention}{46}{section*.80}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces An example of node attention. Note that edge thickness is not related to the strength of the connection between node $v$ (in dark grey) and its neighbors (in light grey), but it represents the degree of similarity between the node states, quantified in a probabilistic sense by the attention score. Dashed edges connect nodes that are not involved in the attention score computation.\relax }}{47}{figure.caption.81}}
\newlabel{fig:attention}{{4.2}{47}{An example of node attention. Note that edge thickness is not related to the strength of the connection between node $v$ (in dark grey) and its neighbors (in light grey), but it represents the degree of similarity between the node states, quantified in a probabilistic sense by the attention score. Dashed edges connect nodes that are not involved in the attention score computation.\relax }{figure.caption.81}{}}
\@writefile{toc}{\contentsline {paragraph}{Node Sampling}{47}{section*.82}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces An example of node sampling. We focus on a node $v$ (in dark grey), and its neighbors (in light grey). Dashed edges connect nodes that are not involved in the neighborhood aggregation; notice how nodes $u_1$ and $u_2$ are excluded from the neighborhood aggregation, even though they are effectively neighbors of $v$.\relax }}{48}{figure.caption.83}}
\newlabel{fig:sampling}{{4.3}{48}{An example of node sampling. We focus on a node $v$ (in dark grey), and its neighbors (in light grey). Dashed edges connect nodes that are not involved in the neighborhood aggregation; notice how nodes $u_1$ and $u_2$ are excluded from the neighborhood aggregation, even though they are effectively neighbors of $v$.\relax }{figure.caption.83}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Readout Layers}{48}{subsection.84}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces The role of hidden state readout function in a node classification/regression task. Here, we focus on a single node $\ensuremath  {\mathbf  {\lowercase {h}}}_{[v]}^{\ell }$, where we drop the subscripts to avoid visual cluttering. Grey nodes replace the $[v]$ subscript. The hidden state readout function creates a node representation $\ensuremath  {\mathbf  {\lowercase {h}}}_{[v]}^{*}$ by combining its three hidden states (one for each layer). Successively, the node representation is turned into an output by an output layer, and compared by the loss function to the same node $\ensuremath  {\mathbf  {\lowercase {y}}}_{[v]}$ in the isomorphic target graph. This operation is repeated for every node in the graph.\relax }}{49}{figure.caption.85}}
\newlabel{fig:node-readout}{{4.4}{49}{The role of hidden state readout function in a node classification/regression task. Here, we focus on a single node $\Elem {h}{v}^{\ell }$, where we drop the subscripts to avoid visual cluttering. Grey nodes replace the $[v]$ subscript. The hidden state readout function creates a node representation $\Elem {h}{v}^{*}$ by combining its three hidden states (one for each layer). Successively, the node representation is turned into an output by an output layer, and compared by the loss function to the same node $\Elem {y}{v}$ in the isomorphic target graph. This operation is repeated for every node in the graph.\relax }{figure.caption.85}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Graph Pooling Layers}{49}{subsection.87}}
\newlabel{sec:pooling}{{4.2.3}{49}{Graph Pooling Layers}{subsection.87}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces A graph readout on an example graph for a graph classification/regression task. Here, we assume the node representations $\ensuremath  {\mathbf  {\lowercase {h}}}_{[v]}^{*}$ have already been obtained by a node readout (not shown).\relax }}{50}{figure.caption.86}}
\newlabel{fig:graph-readout}{{4.5}{50}{A graph readout on an example graph for a graph classification/regression task. Here, we assume the node representations $\Elem {h}{v}^{*}$ have already been obtained by a node readout (not shown).\relax }{figure.caption.86}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces A visual example of a graph pooling layer.\relax }}{51}{figure.caption.88}}
\newlabel{fig:pooling}{{4.6}{51}{A visual example of a graph pooling layer.\relax }{figure.caption.88}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Regularization}{51}{subsection.89}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}The Evaluation of Deep Graph Networks}{51}{section.92}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Datasets}{52}{subsection.93}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Dataset Statistics. Note that, when node labels are not present, we either assigned the same feature of 1 or the degree to all nodes in the dataset.\relax }}{53}{table.caption.94}}
\newlabel{tab:comparison-datasets}{{4.1}{53}{Dataset Statistics. Note that, when node labels are not present, we either assigned the same feature of 1 or the degree to all nodes in the dataset.\relax }{table.caption.94}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Architectures}{54}{subsection.95}}
\newlabel{sec:comparison-architectures}{{4.3.2}{54}{Architectures}{subsection.95}{}}
\@writefile{toc}{\contentsline {paragraph}{Graph Isomorphism Network}{54}{section*.96}}
\@writefile{toc}{\contentsline {paragraph}{GraphSAGE}{54}{section*.97}}
\@writefile{toc}{\contentsline {paragraph}{GraphSAGE + DiffPool}{54}{section*.98}}
\@writefile{toc}{\contentsline {paragraph}{ECC}{55}{section*.99}}
\@writefile{toc}{\contentsline {paragraph}{DGCNN}{55}{section*.100}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Baselines}{55}{subsection.101}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Experimental Setup}{56}{subsection.102}}
\newlabel{sec:comparison-exp-setup}{{4.3.4}{56}{Experimental Setup}{subsection.102}{}}
\@writefile{toc}{\contentsline {paragraph}{Hyper-Parameters}{56}{section*.103}}
\@writefile{toc}{\contentsline {paragraph}{Computational Considerations}{57}{section*.104}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.5}Results}{57}{subsection.105}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Results on chemical datasets with mean accuracy and standard deviation are reported. Best performances are highlighted in bold.\relax }}{57}{table.caption.106}}
\newlabel{tab:comparison-results-molecules}{{4.2}{57}{Results on chemical datasets with mean accuracy and standard deviation are reported. Best performances are highlighted in bold.\relax }{table.caption.106}{}}
\@writefile{toc}{\contentsline {paragraph}{The Importance of Baselines}{58}{section*.107}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Results on social datasets with mean accuracy and standard deviation are reported. Best performances are highlighted in bold. OOR means Out of Resources, either time ($>$ 72 hours for a single training) or GPU memory. \relax }}{58}{table.caption.108}}
\newlabel{tab:social-results}{{4.3}{58}{Results on social datasets with mean accuracy and standard deviation are reported. Best performances are highlighted in bold. OOR means Out of Resources, either time ($>$ 72 hours for a single training) or GPU memory. \relax }{table.caption.108}{}}
\newlabel{tab:comparison-results-social}{{4.3}{58}{Results on social datasets with mean accuracy and standard deviation are reported. Best performances are highlighted in bold. OOR means Out of Resources, either time ($>$ 72 hours for a single training) or GPU memory. \relax }{table.caption.108}{}}
\@writefile{toc}{\contentsline {paragraph}{The Effect of Node Degree}{58}{section*.109}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces The table displays the median number of selcted layers in relation to the addition of node degrees as input features on all social datasets. 1 indicates that an uninformative feature is used as node label.\relax }}{59}{table.caption.110}}
\newlabel{tab:effect-of-degree}{{4.4}{59}{The table displays the median number of selcted layers in relation to the addition of node degrees as input features on all social datasets. 1 indicates that an uninformative feature is used as node label.\relax }{table.caption.110}{}}
\@writefile{toc}{\contentsline {paragraph}{Comparison with Published Results}{59}{section*.111}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.6}Conclusions}{59}{subsection.114}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Results.\relax }}{60}{figure.caption.113}}
\newlabel{fig:comparison-plot}{{4.7}{60}{Results.\relax }{figure.caption.113}{}}
\@setckpt{Chapters/Chapter4}{
\setcounter{page}{61}
\setcounter{equation}{6}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{3}
\setcounter{mpfootnote}{0}
\setcounter{part}{2}
\setcounter{chapter}{4}
\setcounter{section}{3}
\setcounter{subsection}{6}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{7}
\setcounter{table}{4}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{108}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{2}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{parentequation}{0}
\setcounter{su@anzahl}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{5}
\setcounter{bookmark@seq@number}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{theorem}{0}
\setcounter{corollary}{0}
\setcounter{lemma}{0}
\setcounter{definition}{0}
\setcounter{section@level}{2}
}

\chapter{A Deep Generative Model for Fragment-Based Drug Discovery} % Main chapter title
\label{ch:deep-generative-learning-drug-discovery}
The term \emph{de novo} Drug Design (DD) refers to a plethora of methods for the production of novel chemical compounds endowed with desired pharmaceutical properties. It is an important step of the \emph{drug discovery} pipeline, the process that goes from the identification of a biological target for which treatment is needed, to a novel drug hitting the markets. The cost of producing a new drug is now over 1 billion USD, and the average time taken to develop one is approximately 12 years \citep{dimasi2016drugdiscoverycost}. In this scenario, computational (or \emph{in-silico}) methodologies are gradually substituting more traditional \emph{in-vitro} solutions, as they allow expediting almost every aspect of the discovery process, while also being ethically more sustainable (for example, avoiding experimenting on animals). In this chapter, we focus on Deep Learning-based generative methods for \emph{de novo} drug design.

\section{Background}
Here, we provide the necessary background to understand our contribution. We start with a brief primer on molecules and how they are represented for generative tasks. We follow by explaining the typical generative tasks in the context of drug discovery. Then, we briefly review current approaches for modeling molecule generation and how they are evaluated. Lastly, we provide a primer on chemical fragments and \gls{fbdd}, which is the main paradigm that inspired this work.

\subsection{Molecules and their Representation}
For generative tasks involving molecules, two kinds of data representations are usually employed. Below, we review both in more detail.

\subsubsection*{SMILES}\label{subsec:smiles}
One convenient representation of molecules is SMILES, which is essentially a linearization of the molecular graph as a string of ASCII characters, formed by variations of the following general algorithm:
\begin{itemize}
    \item the hydrogen atoms are removed (they can be inferred back using \emph{valency} rules);
    \item the molecular graph is transformed into a spanning tree by removing cycles (disconnecting rings);
    \item the disconnected bonds are marked with unique integers, to allow their reconstruction;
    \item the spanning tree is visited in depth-first order; each time a new atom is visited, the corresponding atomic symbol (and a symbol indicating its chemical bond, if different from a single bond) is added to the SMILES string. Tree branches are marked by parentheses.
\end{itemize}
Even though SMILES strings are essentially sequences of characters, they are generally considered structured representations of the molecular graph, since they are constructed upon its traversal. Unfortunately, SMILES strings are not unique, i.e. different SMILES strings can be associated with the same molecule by changing the root node of the spanning tree or starting the traversal from a different node. More precisely, molecules with $n$ atoms can be represented with at least $n$ different SMILES strings. To mitigate this issue, most chemical packages that implement SMILES parsers include canonicalization algorithms to ensure that uniqueness of the SMILES strings \cite{weininger1989smiles2} (except a few degenerate cases). Despite these issues, the SMILES encoding is widely adopted in Cheminformatics, for example to index molecules in large databases.

\subsubsection*{Molecular Graph}
A more expressive and natural representation of the molecule is given by the molecular graph, also called Lewis structure. In a molecular graph, atoms are nodes and bonds are links between them. The molecular graph can carry additional information through the features attached to the nodes and edges of the associated molecular graph. Typical features added to the atoms are:
\begin{itemize}
    \item one-hot encoding of the atom type;
    \item number of Hydrogen atoms attached to a given atom;
    \item whether the atom is part of a ring or not;
    \item charge of the atom, \ie its number of protons and electrons;
    \item \emph{stereochemistry} and \emph{chirality} information, \ie its 3-D structure. A typical example are coordinates representing the position of the atom in 3-D space.
\end{itemize}
As regards molecular bonds, the features added are:
\begin{itemize}
    \item a one-hot encoding of the bond type;
    \item whether the bond is part of a ring or not;
    \item stereochemistry information.
\end{itemize}

\subsection{Generative Tasks for Drug Design}
We identify three, different but connected, generative tasks in the context of \emph{de novo} drug design where \glspl{dgm} can be used:
\begin{itemize}
    \item \emph{molecular generation} corresponds to the usual unconditional generation task. Given a dataset $\Data = \{\PatternGraph{g}{i}\}_{i=1}^n$, where $\PatternGraph{g}{i}$ are attributed molecular graphs, the objective is to approximate $p(\Graph{G})$, their underlying probability, or a mechanism to sample from it;
    \item \emph{molecular optimization} concerns optimizing some complex chemical property of compounds in a dataset of molecular graphs $\Data$, which might be given or generated. In practice, a molecular optimization task often complements one of molecular generation;
    \item \emph{molecular translation} is in some sense a supervised task of conditional generation. Here, the dataset is composed of pairs of attributed molecular graphs $\Data = \{(\PatternGraph{g}{i}, \PatternGraph{h}{i})\}_{i=1}^n$, where the targets $\PatternGraph{h}{i}$ are structurally similar to the inputs $\PatternGraph{g}{i}$, but with enhanced pharmaceutical properties. The task is to learn their relationship such that, when given an unseen compound $\Graph{G'}$, the model generates a similar and chemically enhanced compound $\Graph{H'}$.
\end{itemize}

\subsection{Deep Generative Models of Molecules} \label{sec:review}
Broadly speaking, there are two major lines of research as regards \emph{de novo} drug design with \glspl{dgm}, which are distinguished by the type of decoder used to generate a molecule:
\begin{itemize}
    \item \emph{SMILES-based} approaches generate molecules through their SMILES strings with autoregressive decoders. In this case, the architecture learns a \emph{language model} of SMILES strings, \ie a model that predicts the next SMILES character, given the previous characters;
    \item \emph{graph-based} approaches learn to generate the molecular graph directly (with one-shot decoders) or incrementally (with autoregressive decoders);
    \item \emph{substructure-based} approaches learn to generate the molecular graph compositionally, by combining smaller substructures together.
\end{itemize}
Below, we provide a brief overview of the literature in \glspl{dgm} models of molecules based on this taxonomy.

\subsubsection*{SMILES-based Approaches}
A first class of SMILES-based approaches model the generation with a simple \gls{rnn} decoder \citep{segler2017moleculelibrariesrnn,bjerrum2017molecularrnn}. These models do not use a structured latent space, and hence are used only for molecular generation. In some cases, reinforcement learning techniques are adopted to perform molecular optimization \citep{olivecrona2017deepreinforcementlearningmolecules,neil2018rnnmolecule}: in practice, the model is first pretrained autoregressively with \gls{mle}; then, the molecules are optimized with reinforcement learning, by giving higher rewards to molecules that respect validity constraints, or whose properties are above a desired threshold. Historically, one of the first SMILES-based generative models using a latent space is that of \citet{gomez2018vaemolecule}, which we term ChemVAE. Basically, the model is a seq2seq language model for SMILES strings, where the latent space is shaped through a \gls{vae}. However, with respect to pure \gls{rnn} approaches, the ChemVAE has been shown to produce a fairly high number of invalid molecules. A first improvement has been proposed by \citet{kusner2017grammarvae}, which uses a rule-based generative model called GrammarVAE, which learns to generate productions of the SMILES grammar which yield syntactically valid molecules. Subsequently, \citet{dai2018sdvae} proposed a Syntax-Directed (SD)-VAE, which uses a SMILES attributed grammar to enforce semantic constraints to the generation. These models are adapted to perform molecular optimization by jointly training a property predictor on the latent space; then, starting from a random point, a novel molecule with optimized properties is found in latent space using Bayesian optimization. Finally, some models use \emph{transfer learning} to optimize molecular properties \citep{maragakis2020desmiles,grisoni2018transferlearningmolecules}. In this case, the models are pretrained on a large dataset of molecules, and fine-tuned on a smaller dataset with optimized molecules.

\subsubsection*{Graph-based Approaches}
Among graph-based approaches that generate the molecule in one shot, one of the first attempts was that of \citet{simonovsky2018graphvae}. The model is a VAE, whose decoder generates three matrices: a probabilistic adjacency matrix, a matrix of node features (where the features are the atom types), and a matrix of edge features (where the features are the bond types). The sampled graph is aligned with the one in input with approximate graph matching. The approach of \citet{decao2018molgan} uses a GAN, whose generator produces one probabilistic adjacency matrix for each bond type, and a node feature matrix, which are sampled to produce an actual graph. The graph is then used to train the discriminator, and also passed to a reward network that is used to optimize molecular properties with a reinforcement learning objective. Both the discriminator and the reward network are implemented as \glspl{dgn}. Among autoregressive graph decoders, one approach is that of \citet{popova2019molecularrnn}, which extends GraphRNN \citep{you2018graphrnn} to work with molecular generation (\ie to output labeled graphs). Molecular optimization is achieved using a policy gradient optimization objective. The Graph Convolutional Policy Network (GCPN) by \citet{you2018graphconvpolicynetwork} formulates the molecule generation as a Markov Decision Process, where actions consist in adding nodes to an existing graph. The Constrained Graph VAE by \citet{liu2018cgvae} uses a different approach to generate a molecule: first, a set of node representations is sampled from latent space; then, the algorithm picks one node at a time, samples its atom type, and connects it to the remaining nodes, sampling the bond types. Optimization is achieved with gradient ascent (on the property value) in latent space. A similar generative approach is used in the work by \citet{samanta2019nevae}, whilst optimization is achieved using Bayesian optimization as in \citet{gomez2018vaemolecule}.

\subsubsection*{Substructure-based Approaches}
One of the first work that used substructure generation is that of \citet{jin2018jtvae}, called Junction Tree (JT)-VAE. In the work, a junction tree of the molecule is built by considering some substructures such as rings as a single component. The model learns to generate junction trees, which guide the generation of the molecular graph. Thus, the model generates valid molecules by design. Optimization in latent space is achieved with Bayesian optimization. The model has been extended by the same authors in \citep{jin2019multimodalmoltranslation} to a hierarchical model, where different levels of coarseness of the molecular graph are used to connect the different substructures. This work is also the first to introduce the molecular translation problem. Similarly, \citet{fu2020core} proposed CoRe, a JT-VAE enhanced with a Copy and Refine mechanism that learns how to copy some substructures from the input molecule, rather than sample them. Molecule Chef, by \citet{bradshaw2019moleculechef}, operates under a different paradigm: the architecture is still a VAE (with Wasserstrain distance regularization instead of KLD), but trained to generate a sequence of reactants, rather than proper substructures of the desired molecule. A novel molecule is then synthesized by mapping the predicted reactants to a desired compound through a reaction predictor. The model is also extended to allow retro-synthesis, \ie mapping a molecule to the most probably reactants that generated it. The model has later been extended by \citet{bradshaw2020barking}, where the procedure has been modified to synthesize molecule through DAGs in a multi-step fashion, similarly to the actual procedure used in laboratories. Finally, the model by \citet{bostrom2019fragments} works on chemical fragments, similarly to the contribution we present in Section \ref{sec:aistats-methods}. Specifically, they encode each fragment using a balanced binary tree, where similar fragments are placed in nearby leaves. The model is then trained to learn to replace fragments in an input molecule, to produce an optimized one.

\subsection{The Evaluation of Deep Generative Models of Molecules}
There are several metrics under which the molecular generators can be evaluated. Ideally, generated molecules should be at the same time:
\begin{itemize}
    \item structurally \quotes{different} enough from training molecules to span the largest possible chemical subspace;
    \item structurally \quotes{close} enough to possess similar interesting chemical properties.
\end{itemize}
Thus, besides chemical validity, novelty and uniqueness, the \emph{diversity} of the generated samples plays a major role. One measure of diversity requires to evaluate the average distance between all the possible pairs of generated molecules. This is called \emph{internal} diversity. Conversely, \emph{external} diversity measures the distance of each molecule in the generated sample against its nearest neighbor molecule in the training sample. The distance function usually employed to calculate diversity is the \emph{Tanimoto distance}, which is a bit-wise distance function defined over the molecular fingerprints associated with a molecule. Another, more recent, diversity metric is the Frechét ChemNet Distance \cite{preuer2018frechetdistance}, which evaluates the distance between the training and generated samples by first feeding them to a pre-trained Deep Neural Network (ChemNet \cite{goh2017chemnet}) for property prediction, and then comparing several statistics computed on the respective activations at the penultimate layer.
For molecular generation tasks, other useful metrics to evaluate the generative performance are based on comparing the distributions of chemical properties between the training sample and the generated sample, as shown in Section \ref{sec:evaluation-generative-graphs}. Typical properties evaluated to this aim are number of atoms, number of bonds, and number of rings distribution of the generated molecules (compared to the training sample). To evaluate performances on the optimization tasks, one intuitive metric to compute is the \emph{improvement}, which measures the average increase (or decrease) of the property obtained in the generated sample, with respect to the training sample. In molecular translation tasks, often \emph{success} is measured. It is the ratio of molecules in the generated sample that were correctly \quotes{translated}, meaning that their molecular properties are above some predefined threshold. Notice that success and improvement should be evaluated after having cleaned the generated sample from duplicates, to avoid biased results. In general, improvement and success cannot be calculated directly, as the ground truth value of the property may be not known or hardly computable for the generated molecule. This is often the case, for example, when the target is biological activity. In these cases, it is often useful to compare against an already trained predictor, to obtain an approximation of the ground truth property.


\subsection{A Primer on Fragments}
Fragments are very-small-weight compounds, typically composed of $<20$ non-hydrogen atoms. Small size has several advantages: firstly, they are easier to manipulate chemically than larger fragments. Secondly, the chemical space of fragments is narrower than, for example, the one of drug-like molecules typically generated from other DD approaches such as HTS. Thus, it is easier to explore and characterize. Thirdly, the small size makes fragments weakly interact with a broader spectrum of target proteins than larger compounds (higher molecular complexity translates into stronger interaction, albeit not necessarily beneficial). A typical FBDD experiment begins with the identification of a suitable collection of fragments, from which a subset with desired interactions with the target (hits) is identified. Subsequently, fragments are optimized into higher affinity compounds that become the starting points (leads) for subsequent drug discovery phases. Optimization is commonly carried out according to three different strategies: (\emph{a}) linking, which optimizes a given fragment by connecting it with another fragment; (\emph{b}) growing, where the fragment is functionally and structurally enriched to optimize binding site occupation; (\emph{c}) merging, which involves combining the structure of two overlapping fragments into a new one with increased affinity.
Since its inception in 1996, FBDD accounts for two clinically approved drugs, and more than thirty undergoing clinical trials at various stages \citep{davis2017fbdd}.

\section{Methods}\label{sec:aistats-methods}
Here, we present the methodologies adopted in our study. Specifically, how the molecules have been broken into sequences of fragments, how these fragments have been embedded in vectorial form, and the architecture used during training and generation. Finally, we describe Low-Frequency Masking, the technique employed to boost the number of unique molecules produced by the model. We start off with a set $\mathcal{G} = \{\PatternGraph{G}{i}\}_{i=1}^n$ of molecular graphs, where each node is labeled with its atom type, and each edge is labeled with its bond type.

\subsection{Molecule Fragmentation}
Given a dataset of molecules, the first step of our approach entails breaking them into an ordered sequence of fragments. To do so, we leverage the Breaking of Retrosynthetically Interesting Chemical Substructures (BRICS) algorithm \citep{degen2008brics}, which breaks strategic bonds in a molecule that match a set of chemical reactions. \quotes{Dummy} atoms (with atomic number 0) are attached to each end of the cleavage sites, marking the position where two fragments can be joined together. BRICS cleavage rules are designed to retain molecular components with valuable structural and functional content, e.g. aromatic rings and side-chains, breaking only single bonds that connect among them. Our fragmentation algorithm works by scanning atoms in the order imposed by the SMILES encoding. As soon as a breakable bond (according to the BRICS rules) is encountered during the scan, the molecule is broken in two at that bond, applying a matching chemical reaction. After the cleavage, we collect the leftmost fragment, and repeat the process on the rightmost fragment in a recursive fashion. Note that fragment extraction is ordered from left to right according to the SMILES representation; this makes the process fully reversible, i.e. it is possible to reconstruct the original molecule from a sequence of fragments. In Figure \ref{fig:fragmentation}, we show a practical example of the fragmentation process on the molecule of aspirin.
\begin{figure}[h!]
    \centering
    \resizebox{.98\textwidth}{!}{\input{Figures/Chapter7/aspirin.tex}}
    \caption{Fragmentation of the aspirin molecule (SMILES string \texttt{CC(=O)OC1=CC=CC=C1C(=O)O}). In the bottom row, the wavy arrow indicates the first atom according to the SMILES ordering, and dashed bonds are broken according to the BRICS rules. At each iteration, the leftmost fragment is stripped out of the molecule until the remainder cannot be broken further. The derived fragment sequence is displayed in the top row, delimited by a dashed box.}
    \label{fig:fragmentation}
\end{figure}
After the fragmentation phase, the set of molecular graphs $\mathcal{G}$ is transformed in a set of fragment sequences $\Data = \{s_{(i)} = (\Graph{f}_{[1]}, \Graph{f}_{[2]}, \ldots, \Graph{f}_{[|s_{(i)}|]}) \mid \PatternGraph{G}{i} \in \mathcal{G}\}_{i=1}^n$. The unique fragments found during the fragmentation process are stored in a \emph{vocabulary} $V$ as one-hot vectors. We indicate with $V(\Graph{f}_{[i]}) \in \Real^{|V|}$ the one-hot vector in the vocabulary corresponding to fragment $\Graph{f}_{[i]}$, and with $|V|$ the size of the vocabulary.

\subsection{Skip-Gram Embedding of Fragments}
In this phase, we transform the one-hot vectors specifying the fragments into continuous vectors enriched with context semantics. In analogy with the work of \citet{bowman2016sentencescontinuousspace}, we view a sequence of fragments as a \quotes{sentence}; therefore, each fragment is a \quotes{word}. Given a sequence $s = (\Graph{f}_{[1]}, \Graph{f}_{[2]}, \ldots, \Graph{f}_{[|s|]})$ of fragments, this objective corresponds to optimizing the following Skipgram loss function \citep{mikolov2014skipgram}:
$$\Loss((\Matrix{E}, \Matrix{E}_{\mathrm{out}}), s) = - \sum_{i=1}^{|s|}\sum_{-w \leq j \leq w} \log p_{\Matrix{E}, \Matrix{E}_{\mathrm{out}}}(\Graph{f}_{[i+j]}| \Graph{f}_{[i]}),\ j \neq 0,$$
where $w$ is called context window, $\Graph{f}_{[i]}$ is an input fragment, and $\Graph{f}_{[i+j]}$ are fragments that occur in the context of $\Graph{f}_{[i]}$. We implement $p_{\Matrix{E}, \Matrix{E}_{\mathrm{out}}}$ as the following neural network:
\begin{align*}
    \Elem{f}{i} &= \Matrix{E}\, V(\Graph{f}_{[i]})\\
    \Elem{o}{i} &= \softmax(\Matrix{E}_{\mathrm{out}}\Elem{f}{i}),
\end{align*}
where $\Matrix{E} \in \Real^{d \times |V|}$ is an embedding matrix and $\Matrix{E}_{\mathrm{out}} \in \Real^{d \times |V|}$ is an output matrix. The model is pretrained with negative sampling \citep{mikolov2014skipgram}. After pretraining, the columns of the embedding matrix $\Matrix{E}$ contain $d$-dimensional embeddings of each fragment in the vocabulary. As a result, fragment embeddings are endowed with context semantics: precisely, embeddings that are close in a sequence of fragments are represented by nearby vectors in embedding space.

\subsection{Model}
Here, we describe the proposed model, detailing how it is trained, and how it can be used for inference and molecular generation.


\subsubsection*{Training}
The model architecture is a standard \gls{vae} \gls{s2s} model for graph generation, composed of an encoder $\Encoder(\Vector{z} \given s)$ and an autoregressive graph decoder $\Decoder(s \given \Vector{z})$, where $s = (\Graph{f}_{[1]}, \Graph{f}_{[2]}, \ldots, \Graph{f}_{[|s|]})$ is a generic input fragment sequence. The encoder is a \gls{rnn} that operates at fragment level and computes the following:
\begin{align*}
    \Elem{f}{i} &= \Matrix{E}\, V(\Graph{f}_{[i]})\\
    \Elem{h}{i} &= \Op{GRU}_{\EncParam}\Par{\Elem{h}{i-1}, \Elem{f}{i}},
\end{align*}
where $\Matrix{E}$ is the pretrained embedding matrix, and $\Elem{h}{0} = \Zeros$ as usual. The final hidden state $\Elem{h}{|s|} \in \Real^h$ is used as a representation of the whole fragment sequence. The sequence representation is then mapped to two vectors as follows:
\begin{align*}
    \MeanVec &= \Matrix{W}_{\EncParam_\mu} \Elem{h}{|s|}\\
    \StdVec &= \Matrix{W}_{\EncParam_\sigma} \Elem{h}{|s|},
\end{align*}
$\MeanVec \in \Real^h$ and $\StdVec \in \Real^h$ represent the mean and variance of the encoding distribution, and $\Matrix{W}_{\EncParam_\mu}, \Matrix{W}_{\EncParam_\sigma} \in \Real^h$ are weight matrices. Subsequently, a latent sample from the encoding distribution is drawn with reparameterization as $\Vector{z} = \MeanVec + \boldsymbol{\eps} \StdVec$, with $\boldsymbol{\eps} \in \Real^h \sim \Normal{\Zeros}{\Identity}$, where $\Identity \in \Real^{h \times h}$ is a unit covariance matrix. The encoder is trained to minimize the \gls{kld} between the distribution parameterized by $\MeanVec$ and $\StdVec$ and a standard Gaussian prior:
$$\Loss(\EncParam, s) = \KLD{\Encoder(\Vector{z} \given s)}{\Normal{\Zeros}{\Identity}}.$$
The decoder is initialized with the latent vector, \ie by setting $\Elem{h'}{0} = \Vector{z}$. Specifically, the decoder is a \gls{rnn} that computes the following:
\begin{align*}
    \Elem{f}{i-1} &= \Matrix{E}\, V(\Graph{f}_{[i-1]})\\
    \Elem{h'}{i} &= \Op{GRU}_{\DecParam}(\Elem{h}{i-1}, \Elem{f}{i-1})\\
    \Elem{o'}{i} &= \softmax\Par{\Matrix{V}_{\DecParam}\Elem{h'}{i} + \Vector{b'}_{\DecParam}},
\end{align*}
where $\Elem{f}{0} = \Matrix{e}\,V(\SOS)$, $\Matrix{E}$ is the shared embedding matrix, and $\Matrix{V} \in \Real^{|V| \times h}$, $\Vector{b'}_{\DecParam} \in \Real^{|V|}$ are the parameters of the softmax output layer. The vector $\Elem{o'}{i} \in \Real^{|V|}$ is the output distribution over all possible fragments in the vocabulary. Each output of the network is compared with the ground truth fragment $\Graph{f}_{[i]}$ with the \gls{ce} function as follows:
$$\Loss(\DecParam, s) = \frac{1}{|s| + 1} \sum_{i=1}^{|s|+1} -\log p_{\DecParam}(\Graph{f}_{[i]} \given \Graph{f}_{[<i]}) = \frac{1}{|s| + 1} \sum_{i=1}^{|s|+1} \Fun{CE}\Par{V(\Graph{f}_{[i]}), \Elem{o'}{i}},$$
where $+1$ is added to account for the end of sequence token. The whole model is trained with \gls{mle} to minimize the following objective function:
$$\argmin_{\Param} \frac{1}{n} \sum_{s \in \Data} \Loss(\EncParam, s) + \Loss(\DecParam, s),$$
where $\Param = (\EncParam, \DecParam)$ as usual. The architecture is shown visually in Figure \ref{fig:fragment-model-training}.

\begin{figure}[h!]
    \centering
    \resizebox{.99\textwidth}{!}{\input{Figures/Chapter7/model-training.tex}}
    \caption{The architecture of the proposed generative model of fragment sequences.}
    \label{fig:fragment-model-training}
\end{figure}

\subsubsection*{Generation}
Generation starts by sampling a latent point $\Vector{z} \in \Real^h$ from the standard Gaussian $\Normal{\Zeros}{\Matrix{I}}$,
and setting $\Vector{z} = \Elem{h}{0}$ to initialize the decoder. The initial hidden state is fed to the network together with the starting token $V(\SOS)$. At each step, the network produces a conditional output distribution on every possible word in the vocabulary, from which a fragment $\Graph{\hat{f}}_{[i]}$ is taken using greedy sampling. The sampled token then becomes the input of the next decoding step, together with the updated hidden state. The process is repeated until the end of sequence token $\EOS$ is sampled, at which point the generation is interrupted. The fragments of the sampled sequence $(\Graph{\hat{f}}_{[1]},\Graph{\hat{f}}_{[2]}, \ldots, \EOS)$ are then joined together in the same order to form a novel molecule. Figure \ref{fig:fragment-model-sampling} shows the process visually.
\begin{figure}[h!]
    \centering
    \resizebox{.55\textwidth}{!}{\input{Figures/Chapter7/model-generation.tex}}
    \caption{Fragment sequence generation using the proposed model.}
    \label{fig:fragment-model-sampling}
\end{figure}

\subsection{Low-Frequency Masking}
To foster molecule diversity, we start from the observation that the distribution of fragments in the data can be roughly approximated by a power law distribution. In fact, usually few fragments occur with very high frequency, while the majority of fragments occur rarely. Hence, infrequent fragments are unlikely to be sampled during generation. To counter this issue, we develop a strategy which we term Low-Frequency Masking (LFM). During training, we mask fragments with frequency below a certain threshold $k$ with a token composed of its frequency and the number of attachment points. As an example, suppose that fragment \texttt{*Nc1ccc(O*)cc1} occurs 5 times in the dataset, and the threshold is $k=10$. Thus, this fragment is masked with the token \texttt{5\_2}, where 5 denotes its frequency, and 2 denotes the number of attachment points. Similarly, fragment \texttt{*C(=O)N1CCN(Cc2ccccc2)CC1} with frequency of 3 is masked with the token \texttt{3\_1}. In contrast, fragment \texttt{*c1ccccc1OC} with a frequency of 200 is left unmasked, since its frequency is above the threshold. A reverse mapping from the masking tokens to the masked fragments is kept. During sampling, whenever a masking token is sampled, we replace it with a fragment sampled with uniform probability from the corresponding set of masked fragments. This strategy serves a double purpose. Firstly, it greatly reduces vocabulary size during training, speeding up the computations. Secondly, it fosters molecule diversity by indirectly boosting the probability of infrequent fragments, and injecting more randomness in the sampling process at the same time. From another point of view, LFM forces the model to generate molecules mostly composed of very frequent fragments, but with infrequent substructures that may vary uniformly from molecule to molecule.

\section{Experiments}\label{sec:aistats-experiments}
Following, we review our experimental setup, namely how the experiments are conceived, which datasets and evaluation metrics are used, which baselines we compare to, as well as details about the hyper-parameters of our model. In our experiments, we try to provide an empirical answer to the following questions:
\begin{itemize}
    \item Q1: is our fragment-based language model able to increase validity rates?
    \item Q2: is our LFM strategy beneficial to increase uniqueness rates?
\end{itemize}
To answer the first question, we compare our model against character-based baselines, which generate molecules atom by atom. As regards the second question, we perform an ablation study of performances with and without LFM. We also compare against graph-based approaches, to assess performances in relation to models that use more expressive molecule representations.

\subsection{Data}
We experiment on the ZINC dataset \citep{irwin2005zinc}, consisting of $\approx$ 250k drug-like compounds. ZINC is a common benchmark for the generative task; as such, it is used to compare against several baselines. To assess the impact of LFM further, in our ablation study we also test our model variants in the PubChem BioAssay (PCBA) dataset  \citep{gindulyte2016pubchem}, which comprises $\approx$ 440k small molecules. Dataset statistics are presented in Table~\ref{tab:mol-statistics}. We applied some common preprocessing steps before training. In the PCBA dataset, we found 10822 duplicate or invalid molecules, which were removed. During an initial preprocessing phase, we discarded molecules composed of $< 2$ fragments. After the preprocessing, our training samples were $227946$ (ZINC) and $383790$ (PCBA). For completeness, we report that we tried to test our model on the QM9 dataset \citep{ramakrishnan2014qm9} as well, but found out that approximately 70\% of its molecules are composed of a single fragment, making assessment poorly informative due to the small sample size.
\input{Tables/Chapter7/dataset-statistics.tex}

\subsection{Performance Metrics}
Following the standards to evaluate molecular generators, we compare our model with the baselines quantitatively, measuring validity, novelty and uniqueness. Validity is assessed by checking the SMILES of the generated graph through the SMILES validator of the \texttt{rdkit} library in Python \citep{rdkit}. To assess the quality of our samples, we compare the distributions of number of atoms, number of bonds and number of rings between the sampled and generated molecules. Moreover, we compare the distributions of the following molecular properties:
\begin{itemize}
    \item octanol/water Partition coefficient (logP), which measures solubility;
    \item Quantitative Estimate of Drug-likeness \citep{bickerton2012qed} (QED), which measures drug-likeness;
    \item Synthetic Accessibility Score \citep{ertl2009sas} (SAS), which measures ease of synthesis.
\end{itemize}
In both cases, we remove duplicates before the valuation, to unbias the results.
\subsubsection*{Baselines}
We compare to baselines found in the literature, representing both SMILES and graph-based generative models described in Section \ref{sec:review}. As regards SMILES-based approaches, we consider ChemVAE \citep{gomez2018vaemolecule}, GrammarVAE \citep{kusner2017grammarvae} and SD-VAE \citep{dai2018sdvae}, whereas as regards graph-based models, we compare against GraphVAE \citep{simonovsky2018graphvae}, CGVAE \citep{liu2018cgvae} and NeVAE \citep{samanta2019nevae}.

\subsection{Hyper-Parameters}
We evaluate our model using the same hyper-parameters for both variants, in order to isolate the effect of our contribution from improvements due to hyper-parameter tuning. The embedding dimension is set to 64, the number of recurrent layers to 2, the number of GRU units per layer to 128 and the latent space size to 100. We use the Adam optimizer with an initial learning rate of 0.00001, annealed every epoch by a multiplicative factor of 0.9, a batch size of 128, and a dropout rate of 0.3 applied to the recurrent layers to prevent overfitting. The model is trained for 4 epochs: after that, we found empirically that the model started to severely overfit the training set. We use $k=10$ as LFM threshold. The stopping criteria for training is the following: after each epoch, we sample 1000 molecules and measure validity, novelty and uniqueness rates of the sample, stopping whenever the uniqueness rate starts to drop (we found out empirically that samples were stable in terms of validity and novelty rates). After training, we sample 20k molecules for evaluation. We publicly release code and samples for reproducibility\footnote{\url{https://github.com/marcopodda/fragment-based-dgm}}. Baseline results are taken from literature\footnote{We found no results in the literature for the PCBA dataset.}.

\section{Results}
The main results of our experiments are summarized in Table \ref{tab:aistats-results}, and provide the answers to the experimental questions posed in Section \ref{sec:aistats-experiments}. As regards Q1, we observe that our model achieves perfect validity scores in the ZINC dataset, greatly outperforming LM-based models and performing on par with the state of the art. This is true also as regards the PCBA dataset. Since both our variants improve over the LM-based competitors, it is safe to argue that our fragment-based approach can effectively increase validity rates. As regards Q2, we observe an improvement in uniqueness by both our variants, with respect to the LM-based competitors. However, the improvement is noticeably higher whenever the LFM strategy is employed. In the PCBA dataset, this trend is even more apparent. Compared to graph-based models, we see how the model with LFM is now competitive with the state of the art. Lastly, we notice that using LFM yields a small improvement in novelty with respect to the vanilla variant.
\input{Tables/Chapter7/results.tex}
In Figure \ref{fig:zinc-props} (for the ZINC dataset) and Figure \ref{fig:pcba-props} (as regards the PCBA dataset), our samples against training compounds are compared, as regards the distribution of the three structural features, and molecular properties listed above. Notice that even without the help of an explicit supervision, generated molecules are qualitatively similar to the training data.
\begin{figure}[h!]
    \centering
    \includegraphics[width=.95\textwidth]{Figures/Chapter7/props-zinc.eps}
    \caption{Comparison of structural and molecular properties between the training and generated samples on the ZINC dataset.}
    \label{fig:zinc-props}
\end{figure}
Finally, Figure \ref{fig:mol-samples} shows two random samples of 30 molecules taken from the ZINC dataset and generated by our model for visual comparison.
\begin{figure}[h!]
    \centering
    \includegraphics[width=.95\textwidth]{Figures/Chapter7/props-pcba.eps}
    \caption{Comparison of structural and molecular properties between the training and generated samples on the PCBA dataset.}
    \label{fig:pcba-props}
\end{figure}
\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=.9\textwidth]{Figures/Chapter7/samples.eps}
        \caption{ZINC}
        \label{fig:zinc-samples}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=.9\textwidth]{Figures/Chapter7/generated.eps}
        \caption{Generated}
        \label{fig:generated-samples}
    \end{subfigure}
       \caption{A random sample of 30 molecules taken from the ZINC dataset ({\scriptsize A}) and generated by our model ({\scriptsize B}).}
       \label{fig:mol-samples}
\end{figure}
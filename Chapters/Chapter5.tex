\chapter{Case Study: Prediction of Dynamical Properties of Biochemical Pathways using Deep Graph Networks}\label{ch:prediction-biochemical-dgn}

In this chapter, we present an application of Deep Learning techniques on graphs to a life sciences problem related to computational biology. Specifically, we apply Deep Graph Networks to process biochemical pathways, \ie dynamical systems that model the complex interactions between molecules at the biochemical level. Biochemical pathways can be represented as a particular form of bipartite graphs known as Petri networks, which allow to study several properties of such systems. Here, we focus on the property of concentration robustness. To be measured, concentration robustness requires to perform time-expensive simulations. Here, we opt for an approximate but reliable solution, which is orders of magnitude faster to compute. Through processing of the Petri network associated to the biochemical pathway with Deep Graph Networks, we show experimentally that it is possible to build a model that predicts concentration robustness rapidly and with a satisfactory level of accuracy.

\section{Introduction and Motivation}
In order to understand the mechanisms underlying the functioning of living cells, it is necessary to analyze their activities at the biochemical level. Biochemical pathways (or pathways, in short) are complex dynamical systems in which molecules interact with each other through chemical reactions. In these reactions, molecules can take the role of reactant, product, promoter or inhibitor. The dynamics of a pathway are determined by the variation over time of the concentration of its molecules. To study these dynamics, two methodologies are traditionally employed. One consists in modelling the pathway as a system of \glspl{ode}, derived from the application of chemical kinetics laws such as the law of mass action. In cases where pathways involve molecules available in small concentrations, which make the dynamics of reactions sensitive to random events, stochastic modelling and simulation approaches are preferred. These are usually variants of the well-known Gillespie's simulation algorithm \citep{gillespie1977exact}. The use of these modelling tools allows to investigate dynamical properties of biochemical pathways such as the reachability of steady states, the occurrence of oscillatory behaviors, causalities between molecular species, and robustness. However, quantitatively measuring these properties often requires to execute a large number of numerical or stochastic simulation, which in turn are time-consuming and computationally intensive.

Given their nature, one widely used formalism to represent biochemical pathways is that of graphs. Many different graphical notations of pathways exist in the literature (see, e.g., \citet{karp1994representations,reddy1993petri,le2009systems}), most of which represent molecules as nodes, and reactions as multi-edges or as additional nodes. Using graphs to represent pathways is convenient for three main reasons. Firstly, they provide a quite natural visual representation of the reactions occurring in the pathway. Secondly, they enable the study of the pathway dynamics through methods such as network and structural analysis. Thirdly, graphs can easily be transformed into \glspl{ode} or stochastic models, to apply standard numerical simulation techniques.

In this study, we investigate whether predicting dynamical properties of biochemical pathways from the structure of their associated graphs is possible; and if so, to what extent. In other words, our main assumption is that the dynamics of the biological system modeled by the pathway can be correlated to the structural properties of the graph by which it is represented. If the assumption is correct, the positive implications are two-fold: on one hand, a good predictive model of desired biochemical properties could, in principle, replace numerical or stochastic simulations whenever time and computational budgets are limited. On the other hand, it could allow to predict the properties even in cases where the quantitative information is not available, for example whenever numerical or stochastic simulation methods cannot be applied.

The main idea behind this work is to use of Deep Graph Networks to learn structural features of pathways represented as Petri networks (or Petri nets, in short), which are used to predict a property of interest. Here, we focus on the assessment of the dynamical property of robustness, defined as the the ability of a pathway to preserve its dynamics despite the perturbation of some parameters or initial conditions. More specifically, given a pathway and a pair of molecular species (called \emph{input} and \emph{output} species), the robustness measures how much the concentration of the output species at the steady state is influenced by perturbations of the initial concentration of the input species. This is a notion of \emph{concentration robustness} \citep{kitano2004biological} which is to some extent correlated with the notion of global sensitivity \citep{zi2011sensitivity}. Robustness makes up for a perfect candidate to test our approach, as its assessment is time-consuming and computationally intensive, requiring a huge number of simulations to explore the parameters space.

The initial part of this work focuses on the creation of a dataset suited to train the \gls{dgn}. We start from collecting 706 curated pathway models in SBML format from the BioModels\footnote{BioModels: \url{https://www.ebi.ac.uk/biomodels/}} database \citep{BioModels2010}, which were initially converted into Petri nets. For every pathway in this initial dataset, the robustness of every possible pair of input and output species has been computed through \gls{ode}-based simulations. Then, these robustness values have been transformed into binary indicators of whether robustness holds for a given pathway and input/output species. Lastly, for each pathway and for each input/output species in that pathway, the induced subgraphs containing the input and output nodes (as well as other nodes that influence the pathway dynamics) have been extracted. To summarize, the final dataset obtained with this preparatory phase consists of a set of subgraphs, each associated to a pair of input/output molecular species, and their respective robustness indicator. The predictive task is thus one of binary classification: specifically, given a subgraph and two nodes corresponding to the input and output species, the model should correctly classify them as robust or not.

We model the task with a \gls{dgn} to learn structural features from the subgraphs and compute a graph embedding that is passed to a \gls{mlp} classifier. The performances of the model are assessed according to a rigorous framework similar to the one developed in \ref{sec:comparison-exp-setup}. Our experimental results show that we are indeed able to predict robustness with reasonable accuracy. We also conduct a follow-up investigation of how the architectural choices, such as type of graph convolutional layer and number of layer, impact performances. The analysis suggests that the depth of the \gls{dgn}, in terms of number of layers, plays an important role in capturing the right features that correlate the subgraph structure to the robustness, and that deep \glspl{dgn} perform better at this task.

To our knowledge, this is the first work that addresses the problem of predicting dynamical, properties of pathways on a large scale using Deep Learning. In contrast, other approaches in the literature mainly focus on inferring the parameters of a single pathway, or the relationships between its species. We believe this work has great potential in helping understand the functioning of living cells, by serving as a fast, and computationally friendlier, alternative to performing expensive simulations in the assessment of pathway properties.

\section{Background}\label{sect:background}
In this section, we provide the necessary formal background to understand the modeling of biochemical pathways with Petri nets, and the dynamical property of concentration robustness.

\subsection{Pathway Petri Nets}\label{sec:ppn}
Biochemical pathways are essentially sets of chemical reactions of the form:
\[
c_1 S_1 + c_2 S_2 + \ldots
\xrightarrow{k}
c_1' P_1 + c_2' P_2 \ldots,
\]
where $S_i,P_i$ are molecules (\emph{reactants} and \emph{products}, respectively), $c_i, c_i' \in \Natural$ are \emph{stoichiometric coefficients} expressing the multiplicities of reactants and products involved in the reaction, and $k \in \Real_{\geq 0}$ is the \emph{kinetic constant}, used to compute the reaction rate according to standard chemical kinetic laws such as the law of mass action. Besides reactants and products, the reactions of a biochemical pathway often include in their description other molecules, called \emph{modifiers}. These are not consumed nor produced by the reaction, but act either as \emph{promoters} or as \emph{inhibitors}, meaning that they can increase or decrease the reaction rate, respectively. Although these molecules are not listed among reactants and products, they do have a role in the kinetic formula, which no longer follows the mass action principle in this case. For example, in the SBML language \citep{hucka2018sbml}, a standard XML-based modeling language for biochemical pathways, reactions can be associated with a number of modifiers, whose concentration is used in the kinetic formula of the reaction. In Figure \ref{subfig:example-reactions} we show a table describing the set of reactions describing a biochemical pathway (first column), some of which include a modifier (second column), namely $A$ for the third reaction, and $F$ for the sixth. Each reaction is associated with its kinetic formula (third column), that, for simplicity, we reference through an alias of the form $\mathsf{r}i$ with $i = 1, \ldots, 7$. Using the kinetic formulas of the two reactions with modifiers as an example, it is clear that $A$ acts as a promoter (meaning that the reaction rate is proportional to the concentration of $A$) and that $F$ acts as inhibitor (meaning that the reaction rate is inversely proportional to the concentration of $F$). Kinetic formulas can then be used to construct a system of \glspl{ode} as shown in Figure \ref{subfig:example-odes}.

\input{Figures/Chapter5/biochemical-pathways}

A common way to represent biochemical pathways is through Petri nets \citep{?}. The formalism of Petri nets have been originally proposed for the description and analysis of concurrent systems \citep{peterson1977petri}, but has been later adopted to model other kinds of systems, such as biological ones. Several variants of Petri nets have been proposed in the literature. In this work, we consider a version of \emph{continuous} Petri nets \citep{gilbert2007petri} with promotion and inhibition edges and general kinetic functions. We call this biologically inspired variant \gls{ppn}. A \gls{ppn} is essentially a bipartite graph with two types of nodes and three types of labelled edges. According to standard Petri nets terminology, the two types of nodes are called \emph{places} and \emph{transitions}. The semantics of a \gls{ppn} in a continuous setting are described by a system of ODEs, with one equation for each place. In the case of pathways, such system corresponds exactly to the one obtained from the chemical reactions shown in Figure \ref{subfig:example-odes}. The state of a \gls{ppn} (called \emph{marking}) is then defined as an assignment of positive real values to the variables of the ODEs. We denote with $\Cal{M}$ the set of all possible markings.

More formally, a \gls{ppn} can be defined as a tuple $\wp = \Tuple{\Cal{P},\Cal{T},\Cal{A}_{S}, \Cal{A}_{P},\Cal{A}_{I},\vartheta,\varsigma,M_0}$ where:
\begin{itemize}
    \item $\Cal{P}$ and $\Cal{T}$ are finite, non empty disjoint sets of places and transitions, respectively;
    \item $\Cal{A}_{S} = ((\Cal{P}\times \Cal{T}) \cup (\Cal{T}\times \Cal{P})$ is a set of standard directed edges;
    \item $\Cal{A}_{P} \subseteq (\Cal{P} \times \Cal{T})$ is the set of promotion edges;
    \item $\Cal{A}_{I} \subseteq (\Cal{P} \times \Cal{T})$ is the set of inhibition edges;
    \item $\varsigma: \Cal{A}_{S} \shortrightarrow \mathbb{N}^{\geq 0}$ weights every standard edge by non-negative integer values;
    \item $\varsigma:\Cal{T} \shortrightarrow (\Cal{M} \rightarrow \mathbb{R}^{\geq 0})$,  is a function that assigns, to each transition, a function that computes a kinetic formula to every possible marking $M \in \Cal{M}$;
    \item $M_0 \in \Cal{M}$ is the initial marking.
\end{itemize}
A visual representation of the \gls{ppn} corresponding to the pathway in Figure \ref{subfig:example-reactions} is shown in Figure \ref{subfig:pathway-petri-net}. The sets of places $\Cal{P}$ and transitions $\Cal{T}$ of a pathway Petri net represent molecular species and reactants, and are displayed as circles and rectangles, respectively. In the figure, places are labeled with the name of the corresponding molecule. The directed edges, depicted as standard arrows, connect reactants to reactions and reactions to products. The weights of the edges (omitted if equal to one) correspond to the stoichiometric coefficients of reactant/product pairs. The sets of promotion and inhibition edges, $\Cal{A}_{P}$ and $\Cal{A}_{I}$, connect molecules to the reactions they promote or inhibit, respectively, and they are displayed as dotted or T-shaped arrows, respectively. The kinetic formulas of reactions (or rather, their aliases defined as in Figure \ref{subfig:pathway-petri-net}), are shown inside the rectangles of the corresponding transitions. As explained previously, molecules connected through promotion edges give a positive contribution to the value of the kinetic formula, while molecules connected through inhibition edges give a negative (inversely proportional) contribution. Finally, the initial marking $M_0$ is not shown in the figure, and it has to be described separately.
\begin{figure*}[h!]
    \centering
    \resizebox{.6\textwidth}{!}{\input{Figures/Chapter5/pathway-petri-net}}
    \caption{The Pathway Petri Net corresponding to the reactions described in Figure \ref{subfig:example-reactions}}
    \label{subfig:pathway-petri-net}
\end{figure*}

In this work, we use a variant of \glspl{ppn} where all the information unrelated to the structure of the pathway is discarded. Specifically, we ignore information about:
\begin{itemize}
    \item kinetic formulas;
    \item multiplicities of reactants and products (\ie edge labels);
    \item the initial marking $M_0$.
\end{itemize}
Basically, for our purposes, a \gls{ppn} is a tuple $\wp = \Tuple{\Cal{P},\Cal{T},\Cal{A}_{S}, \Cal{A}_{P},\Cal{A}_{I}}$ where the irrelevant components have been omitted. We rewrite this object, according to the graph notation introduced in Section \ref{sec:graphs}, into a \keyword{pathway graph} $G = \Tuple{\Cal{V}_G, \Cal{E}_G}$. To do so, we first define the following nodes and edges subsets:
\begin{itemize}
    \item $\Cal{V}_{G}^{\Fun{mol}} = \Cal{P}$ is the set of molecules;
    \item $\Cal{V}_{G}^{\Fun{rx}} = \Cal{T}$ is the set of reactions;
    \item $\Cal{E}_{G}^{\Fun{std}} = \Cal{A}_S$ is the set of standard edges;
    \item $\Cal{E}_{G}^{\Fun{pro}} = \Cal{A}_P$ is the set of promoter edges;
    \item $\Cal{E}_{G}^{\Fun{inh}} = \Cal{A}_I$ is the set of inhibitor edges,
\end{itemize}
where $\Cal{V}_{G}^{\Fun{mol}} \Inter \Cal{V}_{G}^{\Fun{rx}} = \emptyset$ and $\Cal{E}_{G}^{\Fun{std}} \Inter \Cal{E}_{G}^{\Fun{pro}} \Inter \Cal{E}_{G}^{\Fun{inh}} = \emptyset$. Then, we simply set $\Cal{V}_G = \Cal{V}_G^{\Fun{mol}} \Union \Cal{V}_G^{\Fun{rx}}$ and $\Cal{E}_G = \Cal{E}_G^{\Fun{std}} \Union \Cal{E}_G^{\Fun{pro}} \Union \Cal{E}_G^{\Fun{inh}}$.
Using the biochemical pathway of Figure \ref{fig:example-pathway} as reference, its associated pathway graph is shown in Figure \ref{fig:pathway-graph}.
\begin{figure*}[h!]
    \centering
    \resizebox{.6\textwidth}{!}{\input{Figures/Chapter5/pathway-graph}}
    \caption{The pathway graph obtained by omitting kinetic formulas and edge labels from the one in Figure \ref{subfig:pathway-petri-net}. Notice that the names of the molecules are displayed for visual aid, but are not included of the pathway graph.}
    \label{fig:pathway-graph}
\end{figure*}
More explicitly, the set of nodes of the pathway graph contains both molecules and reactions. The set of edges of the graph contains an edge (of any type) from a molecule to a reaction if and only if a perturbation in the concentration of the molecule determines a change in the reaction rate (that could in principle be computed through the omitted kinetic formula). Similarly, it contains an edge from a reaction to a molecule if and only if a perturbation in the reaction rate determines a change in the dynamics of the concentration of that molecule. This is intuitive for those molecular species that are products, as the dynamics of the product accumulation is determined by the reaction rate. By construction, a pathway graph is bipartite.

\subsection{Concentration Robustness}\label{sec:robustness}
Robustness is defined as the ability of a system to maintain its functionalities again external and internal perturbations \citep{kitano2004biological}, a property observed in many biological systems. A general notion of robustness has been formalized by Kitano in \cite{kitano2007towards}. This formalization considers a specific functionality of a system and the \emph{viability} of such functionality, defined as the ability of the system (\eg a cell) to carry it out. This could be expressed, for instance, in terms of the synthesis/degradation rate or concentration level of some target substance, in terms of cell growth rate, or in terms of  other suitable quantitative indicators. More specifically, according to Kitano's definition, the robustness $R$ of a system $\mathbb{S}$, with respect to a specific functionality $a$ and against a set of perturbations $P$ is expressed as:
\[
 R^{\mathbb{S}}_{a,P} = \int_p \Psi(p) D_a^\mathbb{S}(p)dp
\]
In the above definition, $\Psi(p)$ is the probability a perturbation $p$, and $D_a(p)$ evaluates the functionality $a$ of the system $\mathbb{S}$ under the perturbation $p$. More precisely, function $D_a(p)$ gives the viability of $a$ under perturbation $p$, in relation to the viability of $a$ under normal conditions. In the absence of perturbations, $D_a(p)=1$, meaning that the functionality $a$ is assumed to be carried out in an optimal way, or equivalently, that perturbations have irrelevant or no influence; conversely, $D_a(p) = 0$ if perturbations cause the system to fail completely in performing $a$, and $0 < D_a(p) < 1$ in the case of relevant perturbations.

An improvement to Kitano's formulation of robustness has been proposed in \citep{rizk2009general}, where functionalities to be maintained are described as linear temporal logic (LTL) formulas. In this formulation, the impact of perturbations is quantified through a notion of \emph{violation degree}, which measures the distance between the dynamics of the perturbed system and the LTL formula. Many more specific definitions exist, differing either in the class of biological systems they apply to, or in the way the functionality to be maintained is expressed \citep{larhlimi2011robustness}.

In the case of biochemical pathways, a common formulation of robustness can be expressed in terms of maintenance of the concentration levels of some species. This definition can be reduced to both general formulations in \citep{kitano2007towards} and \cite{rizk2009general}. In particular, the \emph{absolute concentration robustness} proposed in \cite{shinar2010structural} is based on the comparison of the concentration level of given species at the steady state, against perturbations (either in the kinetic parameters or in the initial concentrations) of some other species.

A generalization of absolute concentration robustness, called \emph{$\alpha$-robustness}, has been proposed in \citep{nasti2018formalizing}, where concentration intervals are introduced both for the perturbed molecules (input species) and for the molecules whose concentration is maintained (output species). Informally speaking, a biochemical pathway is $\alpha$-robust with respect to a given set of initial concentration intervals, if the concentration of a chosen output molecule at the steady state lies in the interval $[k-\alpha/2,k+\alpha/2]$ for some $k \in \mathbb{R}$. A relative version of $\alpha$-robustness can be obtained simply by dividing $\alpha$ by $k$. This notion of $\alpha$-robustness is related to the notion of global sensitivity \citep{zi2011sensitivity}, which typically measures the average effect of a set of perturbations. Hereafter, we use the term robustness to specifically refer to \emph{$\alpha$-robustness} for brevity.

The assessment of robustness usually requires to perform exhaustive numerical simulations in parameter space \citep{rizk2009general,iooss2015review} (where by parameters we intend mainly the kinetic parameters, or the initial concentrations). In some particular cases, one can exploit the biological network structure to avoid performing simulations altogether \citep{shinar2010structural}. Moreover, in cases where the dynamics of the network are monotonic, the number of such simulations can be significantly reduced \citep{gori2019towards}.

\section{Methods}
Here, we provide details about how the raw biological pathways have been converted into the dataset of graphs on which the \gls{dgn} has been trained.

\subsection{Subgraphs Extraction}\label{sec:subgraphs-extraction}
As explained in Section \ref{sec:robustness}, concentration robustness is defined in terms of pathway, and a pair of input and output molecules. However, for a fixed choice of input and output, not all the nodes in a pathway graph contribute to the assessment, but only a specific subset corresponding to an induced subgraph. Given a pathway graph $G$ and an input/output node pair $\mathsf{I}, \mathsf{O} \in \Cal{V}_G^{\Fun{mol}}$ with $\mathsf{I} \neq \mathsf{O}$, we call this subgraph the \emph{subgraph of G induced by the input/output pair} ($\mathsf{I}, \mathsf{O}$). In practice, this induced subgraph allows to focus on the graph portion that is relevant for the assessment of the property, by removing the nodes irrelevant for its computation. Before delving into details, let us first introduce a helper data structure which we call \keyword{enriched pathway graph}. Given a pathway graph $G$, its enriched version $G'$ is defined as follows: initially, $\Cal{V}_{G'}=\Cal{V}_G$, $\Cal{E}_{G'}=\Cal{E}_G$. Then, for every standard edge $(u, v) \in \Cal{E}_{G}^{\Fun{std}}$ where $u \in \Cal{V}_{G}^{\Fun{mol}}$ and $v \in \Cal{E}_{G}^{\Fun{rx}}$, we update the standard edges of $G'$ with a reverse standard edge $(v,u)$, setting $\Cal{E}_{G'}^{\Fun{std}} = \Cal{E}_{G'}^{\Fun{std}} \Union (v, u)$. Note that we do not reverse neither standard edges from reactions to molecules, nor promotion and inhibition edges. The enriched pathway graph obtained from the pathway graph of Figure \ref{fig:pathway-graph} is shown in Figure \ref{fig:pathway-graph-enriched}, where the additional edges are drawn in solid black. Such graph represents influence relationships between molecules and reactions. The reversed edges encode the fact that a perturbation in the reaction rates determines a variation in the reactants consumption. Hence, the enriched pathway graph essentially corresponds to the \emph{influence graph} that could be computed from the Jacobian matrix containing the partial derivatives of the system of ODEs associated to the pathway \citep{fages2008influencegraph}.
\begin{figure*}[h!]
    \centering
    \resizebox{.6\textwidth}{!}{\input{Figures/Chapter5/pathway-graph-enriched}}
    \caption{The enriched pathway graph obtained from the pathway graph of Figure \ref{fig:pathway-graph}. The edges added with respect to the original graph are shown in black.}
    \label{fig:pathway-graph-enriched}
\end{figure*}
The purpose of the enriched pathway graph is to determine which portion of the associated pathway graph is relevant for the assessment of the robustness. With the help of the enriched graph $G'$, we can introduce the subgraph of $G$ induced by an input pair ($\mathsf{I},\mathsf{O}$) as a graph $S_{\mathsf{I},\mathsf{O}} = \Tuple{\Cal{V}_{S_{\mathsf{I},\mathsf{O}}}, \Cal{E}_{S_{\mathsf{I},\mathsf{O}}}}$, defined informally as follows: $S_{\mathsf{I},\mathsf{O}}$ is the smallest subgraph of $G$ whose node set contains $\mathsf{I}$, $\mathsf{O}$, as well as nodes in every possible oriented path from $\mathsf{I}$ to $\mathsf{O}$ in $G'$. We remark that $S_{\mathsf{I},\mathsf{O}}$ is a subgraph of $G$, although its node set is computed on the basis of the paths in $G'$. Figure \ref{fig:subgraphs} shows some examples of induced subgraphs extracted from the graph in Figure \ref{fig:pathway-graph}.
\begin{figure*}[h!]
    \begin{subfigure}[b]{0.24\linewidth}
        \centering
        \resizebox{.9\textwidth}{!}{\input{Figures/Chapter5/subgraph-1}}
        \caption{$\mathsf{I}=\mathsf{B},\, \mathsf{O}=\mathsf{A}$}\label{subfig:subgraph1}
    \end{subfigure}
    %
    \begin{subfigure}[b]{0.29\linewidth}
        \centering
        \resizebox{.9\textwidth}{!}{\input{Figures/Chapter5/subgraph-2}}
        \caption{$\mathsf{I}=\mathsf{C},\, \mathsf{O}=\mathsf{F}$}\label{subfig:subgraph2}
    \end{subfigure}
    %
    \begin{subfigure}[b]{0.44\linewidth}
        \centering
        \resizebox{.9\textwidth}{!}{\input{Figures/Chapter5/subgraph-3}}
        \caption{$\mathsf{I}=\mathsf{A},\, \mathsf{O}=\mathsf{H}$}\label{subfig:subgraph3}
    \end{subfigure}
    \caption{Examples of subgraphs of the pathway graph in Figure \ref{fig:pathway-graph} induced by different input/output node pairs $(\mathsf{I},\mathsf{O})$.}\label{fig:subgraphs}
\end{figure*}

\subsection{Robustness Computation}\label{sec:robustness-computation}
The robustness values used to label the induced subgraphs are computed following the relative $\alpha$-robustness approach. The dynamics of each biochemical pathway has been simulated by applying a numerical solver (the \texttt{libRoadRunner} Python library by \cite{somogyi2015libroadrunner}) to its ODEs representation. Reference initial concentrations of involved molecules have been obtained from the original SBML model of each pathway. Moreover, 100 simulations have been performed for each molecule, by perturbing its initial concentration in the range $[-20\%,+20\%]$. The termination of each simulation has been set to the achievement of the steady state, with a timeout of 250 simulated time units.\footnote{The concentration values obtained at the end of the simulation are considered as steady state values also in the cases in which a timeout has been reached.} Given a subgraph $S_{\mathsf{I}, \mathsf{O}}$, we computed the width $\alpha$ of the range of concentrations reached by the output molecules $\mathsf{O}$ by varying the input $\mathsf{I}$ (as per definition of $\alpha$-robustness). A relative robustness $\overline{\alpha}$ has then been obtained by dividing $\alpha$ by the concentration reached by the output when the initial concentration of the input is the reference one (no perturbation). The final robustness value $r_{\mathsf{I}, \mathsf{O}} \in [0,1]$ has been computed by comparing $\overline{\alpha}$ (a relative representation of the output range) with $0.4$ (a relative representation of the initial input range, that is $40\%$) as follows:
\[
    r_{\mathsf{I}, \mathsf{O}} = 1 - min (1,\frac{\overline{\alpha}}{0.4})
\]

\subsection{Data Preprocessing}\label{subsec:data-collection}
Our initial data collection consisted of 706 SBML models of biochemical pathways, downloaded from the BioModels database \cite{le2006biomodels}. Specifically, the data correspond to the complete set of manually curated models present in the database at the time we started the construction of the dataset\footnote{May 2019.}. We removed empty models (not containing any node) and discarded duplicates, reducing the number of SBML models to 484. These models were first transformed into \glspl{ppn} representations and saved in DOT format\footnote{The DOT graph description language specification, available at: \url{https://graphviz.gitlab.io/_pages/doc/info/lang.html}}. For the translation of the SMBL models into \glspl{ppn}, we developed a Python script that, for each reaction in the SMBL model extracts reactants, products and modifiers. Furthermore, it also checks the kinetic formula in order to determine whether each modifier is either promoter or an inhibitor. With the conversion of each \gls{ppn} into a pathway graph, we obtained a collection $\Cal{G} = \Set{G_i}_{i=1}^{484}$ of pathway graphs. Each of these pathway graphs has been transformed into a set containing one induced subgraph (whose size does not exceed 100 nodes, for computational reasons) for every possible input/output combination of pathway graph nodes representing molecules, according to the procedure detailed in Section \ref{sec:subgraphs-extraction}. For each induced subgraph, the associated robustness has been calculated as explained in Section \ref{sec:robustness-computation}. The result of this preprocessing is a training dataset:
$$\mathbb{G} = \bigcup\limits_{G \in \Cal{G}} \Set{(S_{\mathsf{I},\mathsf{O}}, \overline{r}_{{\mathsf{I},\mathsf{O}}}) \mid \forall (\mathsf{I},\mathsf{O}) \in \Cal{V}_{G}^{\Fun{mol}}},$$
where $\overline{r}_{\mathsf{I}, \mathsf{O}} = \mathbb{I}[r_{\mathsf{I}, \mathsf{O}} > 0.5]$ are robustness indicators used as the labels for the associated binary classification task. The total number of subgraphs in the preprocessed dataset is 44928.

\subsection{Subgraph Features}
We encoded information such as node type, edge type, and input/output pair of the induced subgraphs in their node and feature vectors. Specifically, for each subgraph node, we associate a binary 3-dimensional feature vector which encodes whether the node is a molecule or a reaction, whether the node is an input node or not, and whether a node. Given an induced subgraph $S_{\mathsf{I},\mathsf{O}}$ and one of its nodes $v \in \Cal{V}_{S_{\mathsf{I},\mathsf{O}}}$, we define its 3-dimensional vector of node features $\Elem{x}{v} \in \Vector{x}_{S_{\mathsf{I},\mathsf{O}}}$ component by component as follows:
\begin{align*}
    \Elem{x}{v, 1} = \begin{cases}
        1  & \mathrm{if}\; v \in \Cal{V}_{G}^{\Fun{mol}}\\
        0  & \mathrm{if}\; v \in \Cal{V}_{G}^{\Fun{rx}},
    \end{cases}
    \hspace{1.5em}
    \Elem{x}{v, 2} = \begin{cases}
        1  & \mathrm{if}\; v = \mathsf{I}\\
        0  & \mathrm{otherwise},
    \end{cases}
    \hspace{1.5em}
    \Elem{x}{v, 3} = \begin{cases}
        1  & \mathrm{if}\; v = \mathsf{O}\\
        0  & \mathrm{otherwise},
    \end{cases}
\end{align*}
where the notation $\Elem{x}{v, i}$ indicates the $i$-th component of the node feature vector $\Elem{x}{v}$.
Similarly, for each subgraph edge, we encode its edge type as a one-hot vector out of the three possibilities (standard, promoter or inhibitor). Given an edge $(u,v) \in \Cal{E}_{S(\mathsf{I},\mathsf{O})}$, we define its 3-dimensional vector of edge features $\Elem{e}{u, v} \in \Vector{e}_{S_{\mathsf{I},\mathsf{O}}}$ component by component as follows:
\begin{align*}
    \Elem{e}{u, v, 1} = \mathbb{I}[(u,v) \in \Cal{E}_{G}^{\Fun{std}}], \quad
    \Elem{e}{u, v, 2} = \mathbb{I}[(u,v) \in \Cal{E}_{G}^{\Fun{pro}}], \quad
    \Elem{e}{u, v, 3} = \mathbb{I}[(u,v) \in \Cal{E}_{G}^{\Fun{inh}}],
\end{align*}
where $\mathbb{I}$ is the indicator function, and the notation $\Elem{e}{u, v, i}$ identifies the $i$-th component of the edge feature vector $\Elem{e}{u, v}$. In practice, the edge features encode the edge type as a one-hot vector. Figure \ref{fig:pathway-graph-features} shows the induced subgraph of Figure \ref{subfig:subgraph3} with the corresponding feature vectors in place of its nodes and edges.
\begin{figure*}[h!]
    \centering
    \resizebox{.8\textwidth}{!}{\input{Figures/Chapter5/pathway-graph-features}}
    \caption{A representation of the node and edge feature vectors of the induced subgraph shown in Figure \ref{subfig:subgraph3}. Each node is represented as a 3-dimensional binary vector, where 1 is indicated with dark grey and 0 with white. To avoid visual cluttering, we only show one example of edge feature vector per edge type (the column vectors inside the dashed circles).}
    \label{fig:pathway-graph-features}
\end{figure*}

\section{Experiments}\label{sec:pathway-experiments}
In this section, we provide all the necessary details concerning our experimental procedures. In particular, we describe the architecture of the \gls{dgn} in detail, and discuss the assessment protocol by which we evaluated the proposed model on the predictive task.

The proposed \gls{dgn} architecture for robustness prediction, shown at a high level in Figure \ref{fig:architecture}, consists in a series of $L$ graph convolutional layers, followed by a node readout that aggregates the hidden states at each layer into node representations by concatenation, a graph readout that aggregates the node representations into a graph representation, and a downstream \gls{mlp} classifier which uses the graph representation to compute a probability of whether the graph is robust or not.
\begin{figure*}[h!]
    \centering
    \resizebox{.95\textwidth}{!}{\input{Figures/Chapter5/architecture}}
    \caption{Model Architecture.}
    \label{fig:architecture}
\end{figure*}
Some hyper-parameters of the network are fixed before-hand: the downstream MLP used for classification has two hidden layers of size 128 and 64, respectively, interleaved by ReLU non-linearities. As output layer, since our interest is to compute robustness probabilities, we used a sigmoid function that maps its input to the $(0, 1)$ range. All models are trained with \gls{mle} using \gls{sgd}, by minimizing the \gls{bce} loss function as usual in binary classification tasks. For the minimization, we used the Adam optimizer, with a learning rate of 0.001 and scheduled learning rate annealing with a shrinking factor of 0.6 every 50 epochs. To prevent overfitting, we use two strategies: the hidden layers are regularized via Dropout, with drop probability of 0.1; and we used early stopping with a maximum number of epochs of 500, and patience parameter of 100 epochs.

Since this is the first time \glspl{dgn} are applied to this task, we also report a baseline, which is simply a model that always predict the most frequent class (robust, in this case). Its purpose is to serve as reference point to understand whether the task is being learned to some extent or not.

Our experiments are divided in two sequential phases. The experimental protocol is detailed at a high level as follows:
\begin{itemize}
    \item in a first phase, which we abbreviate as \textbf{E1}, we performed model assessment on a smaller dataset of induced subgraphs. The rationale of this phase is to get a sense of which architectural choices, \eg with respect to the type of \gls{gcl}, are most promising to carry out the full-fledged evaluation on the entire dataset;
    \item in the second phase, termed \textbf{E2}, we perform a final evaluation of the architecture on the full dataset, fixing some architectural components guided by the results obtained in \textbf{E1}.
\end{itemize}
In both experiments, the evaluation procedure consists of an external 5-fold \gls{cv} for model evaluation, with an internal hold-out split of 90\% training and 10\% validation for model selection. After a set of hyper-parameters is selected by the inner model selection, the winning configuration is trained and evaluated 3 times in the corresponding test fold, to mitigate the effect of random initialization. The score of the model for that fold is given by the average of these 3 trials. Following, we detail about the hyper-parameters and the evaluation metrics used in both experiments.

\subsection{E1 Setup}
As explained before, the first evaluation of the model is carried out a subset of induced subgraphs, speficially those with number of nodes $<= 40$. This resulted in a dataset with a total of 7036 induced subgraphs, which we term $\mathbb{G}_{\Fun{small}}$. In this phase, we evaluate the model selecting among the following hyper-parameters:
\begin{itemize}
    \item number of \gls{gcl} layers $L$, choosing between 1 and 8;
    \item hidden state size $h$, which once chosen remains fixed across all the $L$ layers, choosing between 128 and 64;
    \item type of \gls{gcl};
    \item whether the \gls{gcl} handles the contribution of the different edge types or not;
    \item type of graph readout function, choosing between sum, mean and max.
\end{itemize}
Specifically to the third point, we evaluate the following \gls{gcl} variants:
\begin{itemize}
    \item Graph Convolutional Network (GCN), as described in Section \ref{sec:graph-conv-layers};
    \item Graph Isomorphism Network (GIN), as described in Section \ref{sec:comparison-architectures};
    \item Weifeiler Lehman Graph Convolution (WLGCN), \ie the \gls{gcl} layer presented in \citep{morris2019weisfeilerlehmangoneural}, whose implementation is the following:
    $$\Elem{h}{v}^{\ell} = \Fun{ReLU}\Paren{\Matrix{W}_{\ell}^{\Transpose} \Elem{h}{v}^{\ell-1} + \Matrix{U}_{\ell}^{\Transpose} \sum_{u \in \Cal{N}(v)} e_{uv}\, \Elem{h}{u}^{\ell-1}},$$
    where $e_{uv} \in \Real$ is a learned scalar that weighs the connections between the current node and its neighbors.
\end{itemize}
For each considered \gls{gcl}, we evaluate a vanilla variant, which does not take into account the different edge types, as well as an edge-aware variant as described in Section \ref{sec:graph-conv-layers}, using three different weight matrices (one for each edge type) for the neighborhood aggregation. More specifically, given an induced subgraph $G$\footnote{For consistency, we slightly change notation and refer to induced subgraphs with the letter $G$ instead of $S$ from now on.} and one of its nodes $v \in \Cal{V}_G$, the edge-aware neighborhood function used to select neighbors of a certain type is the following:
$$\Cal{N}_c(v) = \Set{u \in \Cal{N}(v) \mid \mathbb{I}[ (u,v) \in \Cal{E}_{G}^c]},$$
where $c \in \Cal{C} = \Set{\Fun{std}, \Fun{pro}, \Fun{inh}}$.

\paragraph{Evaluation Metrics} As performances metric, we used accuracy on the predictions, defined as usual as the number of correct predictions out of the total number of predictions. We remark that the model outputs probabilities; hence, we round the prediction to the nearest integer (which is either 0 or 1) to compute a \quotes{hard} prediction, which is used in the accuracy calculations. Formally, if: \begin{itemize}
    \item $TP$ is the number of true positives, \ie the number of correctly predicted robust subgraphs;
    \item $TN$ is the number of true negatives, \ie the number of correctly predicted not robust subgraphs;
    \item $FP$ is the number of false positive, \ie the number of subgraphs wrongly predicted as robust;
    \item $FN$ is the number of false negatives, \ie the number of subgraphs wrongly predicted as not robust,
\end{itemize}
the accuracy is defined as:
$$ \Fun{ACC} = \frac{TP + TN}{TP + TN + FP + FN}.$$

\subsection{E2 Setup}
In the second experiment, we fix the \gls{gcl} (and the fact that it handles edge types or not) to the one that obtains the best evaluation score in \textbf{E1}.
The other hyper-parameters evaluated are the same as in \textbf{E1}, though the selection is now performed on the full dataset $\mathbb{G}$.

\paragraph{Evaluation Metrics} In this experiments, there is a high imbalance in favor of the positive class ($\approx 90\%$) with respect to the negative class in the dataset, which is not so dramatic for the small dataset ($\approx 70\%$ in favor of the positive class). When the disproportion is so high, in fact, the accuracy can be misleading. Thus, besides accuracy, we evaluate the performances of the model using other metrics related to the accuracy such as:
\begin{itemize}
    \item Sensitivity, also known as True Positive Rate (TPR), defined as $\Frac{TP}{TP + FN}$, which intuitively measures how good is the classifier to detect the positive (robust) class;
    \item Specificity, defined as $\Frac{TN}{TN + FP}$, which intuitively measures how good is the classifier to detect the negative (not robust) class.
\end{itemize}
Another metric we evaluated, which is not related to the accuracy, is the Area under the Receiver Operating Characteristics curve (AUROC), which quantifies the ability of the classifier to discriminate between negative and positive examples. The AUROC is computed as the integral of a ROC curve, which measures how the TPR and the False Positive Rate $\Fun{FPR} = 1 - \Fun{TPR}$ vary as one moves a threshold parameter $\beta$. It is drawn as a curve plot where the x-axis represents the false positive rate, and the y-axis represents the true positive rate, both with values between 0 and 1. A point of the curve has thus coordinates ($\Fun{FPR}(\beta), \Fun{TPR}(\beta))$, for a fixed value of $\beta$, with point (0; 0) being associated with $\beta = 1$ and point (1; 1) being associated with $\beta = 0$. The point (0; 1) is the optimum of the curve, since the false positive rate is minimized and the true positive rate is maximized. An example of ROC curve is shown in Figure \ref{fig:roc}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=.8\textwidth]{Figures/Chapter5/roc}
    \caption{An example of ROC curve. The dashed line is the ROC curve of a random classifier.}
    \label{fig:roc}
\end{figure}
To illustrate the behavior of the curve, it is useful to consider three extreme cases:
\begin{enumerate}
\item for a perfect classifier, which makes no errors, the curve goes from point $(0;0)$, to point $(1;0)$, to point $(1;1)$;
\item for a completely random classifier, which whatever the threshold always produces the same number of false positives and true positives, the curve created is the line segment bounded by the points $(0;0)$ and $\,(1;1)$;
\item no reasonable classifier is assumed to produce a ROC curve whose points are located under the $(0;0)$, $\,(1;1)$ segment.
\end{enumerate}
An AUROC value of 0.5 describes a completely random classifier (since it is equal to the area of the triangle described by the extreme case 2), while an AUROC of 1 describes a perfect classifier (since it is equal to the area of the square described by the extreme case 1). Good classifiers have an AUROC score generally equal or higher than 0.8. Besides being a metric independent of class proportions, the AUROC also offers a probabilistic interpretation of its value, since it is equals to the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example. For an extensive survey of ROC curves, see for example \cite{fawcett2006roc}.

\section{Results}
Here, we present the results obtained on the two experiments, \textbf{E1} and \textbf{E2}. Subsequently, we present an analysis of the performances obtained by the model on two use cases: one on synthetic data, and one on a real-world pathway.

\subsection{Results on the E1 Experiment}
Table \ref{tab:e1-results} shows the average accuracy on the 5 test folds obtained by each of the examined architectures on the \textbf{E1} experiment. Even though these results are obtained on a dataset approximately 15\% the size of the original one (consisting of 7036 induced subgraphs), we can already make some interesting observations about the task and the models:
\begin{itemize}
    \item the task can be learned, as evidenced by the gap between the baseline and all the examined models, which is approximately 13\% on average. This result verifies our initial assumption that it is indeed possible to predict robustness using only pathway structure;
    \item the comparison between the various \glspl{gcl} highlights the fact that there is no clear winner between the three tested architectures, which obtain very similar results when assessed on the same experimental conditions (\ie all models with/withour edge handling capabilities);
    \item as expected, adding edge handling to the \glspl{gcl} is beneficial to improve performances. On average, the models obtain a 1.8\% improvement in accuracy when edge handling is used.
\end{itemize}

\input{Tables/Chapter5/e1-results.tex}

In a second experiment, we assess whether depth (intendended as number of \gls{dgn} layers) is a good inductive for this task.To do so, we perform a \posthoc study the validation scores, to see how they relate to number of \gls{dgn} layers. Specifically, for each model, and for each number of layers from 1 to 8, we compute the average validation score obtained by all hyper-parameters configurations with an identical number of layers. Note that, although validation accuracy is in general an over-estimate of the true accuracy, the relative difference in performance as the number of layers change stays proportional independently of which specific data is used. In other words, we are not interested in the score by itself, but rather to the trend (increase or decrease) in performance in relation to the number of \gls{dgn} layers. Figure \ref{fig:e1-layering} clearly shows that, in all cases, increasing the number of layers improves accuracy, up to a certain depth (around 4-5 layers on average) where performances start to plateau. This provides evidence that depth is a good inductive bias for \glspl{dgn} on this task, up to some extent.

\begin{figure}[h!]
    \centering
    \includegraphics[width=.8\textwidth]{Figures/Chapter5/e1-layering}
    \caption{The effect of the number of \gls{dgn} layers to the performances of the models.}
    \label{fig:e1-layering}
\end{figure}

\subsection{Results on the E2 Experiments}\label{sec:e2-results}
For the second round of experiments, we fix the \gls{gcl} to the \gls{gcn} model with edge-handling capabilities, since it is the model that obtains the highest accuracy in \textbf{E1}.
The results of our experiments are reported in Table \ref{tab:e2-results}, where we average the computed metrics across the 5 test folds as usual.
\input{Tables/Chapter5/e2-results.tex}
In this case, we stratify the performances by number of nodes, in order to also analyze the model performances in relation to the size of the input subgraph. From the results, one can immediately see, by looking at the last row, how the model accurately predicts robustness better than all models tested in the \textbf{E1} phase by a large margin: this is probably caused by the largest size of the dataset, which usually results in major improvements with any \gls{ml} model, and in particular with \gls{dl} models. Specifically, we report an overall accuracy of $0.913 \pm 0.003$, as well as an AUROC of $0.948 \pm 0.004$. The model shows very high sensitivity ($0.965 \pm 0.006$) but a low specificity in comparison ($0.646 \pm 0.042$); this indicates that it is \quotes{harder} for the model to predict induced subgraphs that are not robust. This effect is a probable consequence of the class misproportion between negative and positive examples, which is around 86\% in favor of the positive class for this data sample. One result that is consistent across all measurements are the the very narrow standard deviations of the estimates, which indicate stable predictions regardless of the specific folds on which they are computed. To display this trend visually, we plot in Figure \ref{fig:e2-rolling-acc} the ROC curves obtained on the 5 test folds. Their similarity strongly indicates that the model performances are consistent across different test samples.
\begin{figure}
    \centering
    \includegraphics[width=.65\textwidth]{Figures/Chapter5/e2-roc-curve.eps}
    \caption{ROC curves for each of the five test folds. The black dashed line shows the performance of a baseline (\quotes{Null}) classifier that always predicts one class.}\label{fig:e2-roc-curve}
\end{figure}
The results of Table \ref{tab:e2-results} show a good performance of the model under the several stratifications tested. In particular, it performs better when dealing on subgraphs with 21-80 nodes, reaching an average in that strata AUROC of over 0.955. To better visualize this trend, we plot in Figure \ref{fig:e2-roc-curve} the rolling accuracy of the model, using a window size of 20, and averaging across the 5 test folds as usual. The plot clearly shows the improvement in accuracy as the number of nodes increases. Interestingly, when the size of the graph exceeds 80 nodes, the model performances start to decrease. This might be a consequence of the smaller sample sizes of subgraph with 81-100 nodes which occur in the dataset 3 times less on average than subgraphs with 21-80 nodes. The same trend can be noticed for smaller subgraphs, with up to 20 nodes. Finally, Figure \ref{fig:e2-conf-matrix} shows the confusion matrix of the predictions computed by the model, where the entries are the averages computed across the five test folds. In the plot, we can visualize the good performances of the model as regards the number of correctly predicted subgraphs (on the diagonal) with respect to the cases where the model makes wrong predictions. Looking at the anti-diagonal the confusion matrix, we can also see that the model has a higher rate of false positives than false negative. Again, this is expected behaviour due to class misproportion in the dataset.
\begin{figure}[h!]
    \begin{subfigure}[b]{0.48\linewidth}
    \centering
        \includegraphics[height=130px]{Figures/Chapter5/e2-rolling-acc.eps}
        \subcaption{}\label{fig:e2-rolling-acc}
    \end{subfigure}
    %
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[height=130px]{Figures/Chapter5/e2-conf-matrix.eps}
        \subcaption{}\label{fig:e2-conf-matrix}
    \end{subfigure}
    \caption{({\scriptsize A}) Rolling mean accuracy (with a window of size 20), averaged over the 5 test folds, showing an increasing trend in performance as the number of graph nodes grows. ({\scriptsize B}) Confusion Matrix of the predictions computed by the model. Each entry of the matrix is the average of the corresponding entries in the five different confusion matrices of the test folds.}
\end{figure}

Lastly, we emphasize the fact that, once trained, the time needed to obtain a prediction from the model is in the order of milliseconds, while performing numerical simulations can be orders of magnitude slower. For this specific case, the simulation of most of the considered models (as detailed in Section \ref{sec:robustness-computation}) requires a amount of time in the order of minutes, while bigger models can take many hours to produce an output value. While training and evaluating the model is expensive, and can take hours, it needs to be performed only once. To conclude, we believe that, once perfected, methods inspired by our approach have the potential of enabling faster advances in understanding the functioning of cells through pathway modelling.

Following, we present two application cases of the model: the first is on the synthetic pathway shown in Figure \ref{fig:example-pathway}, and serves as an explanation tool of why the model performs sub-par in cases of smaller subgraphs; the second is an example of how the model can correctly infer the robustness relations in a real-world pathway.

\subsection{Application on Synthetic Data}
The lower prediction accuracy in the case of small graphs (1-10 nodes) can be put into perspective, in light of the fact the fact we trained the model with subgraphs in which kinetic, stoichiometric and initial concentration parameters have been omitted (as explained in Section \ref{sec:ppn}). In general, the smaller the graph, the higher the influence on its dynamics these parameters exert. As an example, let us consider the synthetic example of biochemical pathway introduced in Figure \ref{fig:example-pathway} and the corresponding pathway graph of Figure \ref{fig:pathway-graph}. Let us now consider the following kinetic and initial concentration (marking) parameters:
\begin{gather*}
  k1 = 1.0 \quad
  k3 = 0.01 \quad
  k5 = 0.01 \quad
  k7 = 0.3  \quad
  k2 = 5.0  \quad
  k4 = 0.1  \quad
  k6 = 5.0 \\
  m_0(A) = 50\quad
  m_0(B) = 50\quad
  m_0(C) = 100\quad
  m_0(D) = 100\\
  m_0(E) = 0\quad
  m_0(F) = 0\quad
  m_0(G) = 100\quad
  m_0(H) = 0.
\end{gather*}
We used these parameters to simulate the ODEs of Figure \ref{subfig:example-odes}, calculating the corresponding robustness by varying the initial concentration of each molecule in the interval $[-20\%,+20\%]$. The robustness values obtained with the simulations are displayed in Table \ref{tab:uc1-odes}. Analogously, in Table \ref{tab:uc1-predictions} we list the average and standard deviations obtained by the 5 different models evaluated in Section \ref{sec:e2-results} (one trained on the respective CV fold), when tasked to predict the robustness probabilities of some input/output pairs of interest.
\input{Tables/Chapter5/uc1}
We remark that values in these two tables are not directly comparable: those in Table \ref{tab:uc1-odes} are exact robustness values, while those in Table \ref{tab:uc1-predictions} are probabilities of the robustness values to be greater than 0.5. In this specific case, the prediction turns out to be accurate in the case of input/output pairs corresponding to big induced subgraphs. This happens in the cases of the input/output pairs $A/F$ and $A/H$, whose induced subgraphs are among the largest ones. In contrast, the prediction is incorrect for $C/F$: in this case, the models predicts a small robustness probability, while the simulations give 0.99. We observe that the robustness value of this input/output combination is sensitive to the perturbation of parameters that have been discarded when constructing the dataset. In particular, if the initial (omitted) concentration of $C$ was $80$ instead of $100$, the robustness value of the pair $C/F$ would become $0.5$ rather than $0.99$. Another case when the model prediction is wrong is that of the pair The prediction turns out to be wrong also in the case of input $B/A$. In this case, the probability given by the network is under $0.50$, which contrasts to a value of $1$ obtained by the ODEs simulation. Even in this case, we notice that the parameters that have been omitted in the dataset might have a strong influence on the robustness, such as kinetic formulas and the multiplicity of the edge directed to node $B$. Finally, in the case of the pair $G/H$, the prediction gives a small probability of robustness and indeed the actual measured value is borderline ($0.50$). More in general, we observe that smaller subgraphs are less frequent than medium-sized subgraphs in the dataset. Thus, it is possible that the model has learned to be more accurate on the latter subgraphs (to maximize the accuracy), at the expense of making more errors when predicting the former.

\subsection{Application on a model of the EGF Pathway}
As a second use case, we consider the SBML model of the Epidermal Growth Factor (EGF) pathway proposed by Sivakumar et al. in \citep{sivakumar2011systems}. This model corresponds to model \texttt{BIOMD0000000394} of the BioModels database, and is shown in Figure \ref{fig:EGF} as represented by the CellDesigner tool \citep{funahashi2003celldesigners}. We adopt this visual representation style for this case study for readability, but the pathway can can be trivially translated into a \gls{ppn}.
\begin{figure}[t]
    \centering
    \includegraphics[width=12cm]{Figures/Chapter5/pathway394}
\caption{A model of the EGF pathway by Sivakumar et al. \citep{sivakumar2011systems}.Image generated form the SMBL file in the BioModels database using the CellDesigner tool.}\label{fig:EGF}
\end{figure}
The pathway describes, in a simplified way, the transduction of the EGF signal, and the consequent activation of the mitogenesis and cell differentiation processes (modelled as an abstract species in the pathway). When the EGF signal protein is available in the cell environment, it can be perceived by the receptor protein EGFR (where R stands for Receptor), which then initiates a cascade of reactions inside the cell, ultimately leading to the activation of mitogenesis and differentiation. The initial steps of the pathway give rise to a rather big complex, involving an activated EGFR dimer and a number of other proteins (the big box in the upper-left corner of Figure \ref{fig:EGF}). The formation of such complex is described in a very simplified way in this pathway model, by concentrating everything in only two reactions. The big complex then promotes a cascade of reactions inside the cell, which are modelled more in detail.

Considering the mitogenesis and differentiation (abstract) species as output (in pink, bottom left corner of Figure \ref{fig:EGF}), with respect to EGF and EGFR as input (in green and yellow, respectively, top left corner of Figure \ref{fig:EGF}), numerical simulations assign a very high robustness ($>0.995$) in both cases. This is actually expected in a signal transduction pathway, since it behaves as an amplifier that must be able to activate the target cell process despite of perturbations in the signal and receptor concentrations. On the other hand, if we look at the robustness of the first portion of the pathway up to the creation of the big complex, we can then observe a different behavior. Specifically, if we consider the big complex as output and EGF as input, we still obtain a very high robustness ($>0.999$). However, when the input is EGFR, we obtain a robustness value of only $0.19$. Again, this is not surprising, since EGF is modelled in the pathway as a promoter of the first reaction (\ie, it is not consumed), while EGFR is a reactant and it will be included in the big complex.

In this case, the model correctly captures the different roles of EGF and EGFR. Indeed, using EGF as input and the big complex as output, the model gives $0.9474 \pm 0.014$ as probability of robustness, whereas it gives $0.145 \pm 0.121$ when the input is with EGFR. The model captured also captures the robustness of the whole model, namely the previous case where either EGF or EGFR were considered as input, and the abstract mitogenesis and differentiation as output. Precisely, it gives probability $0.973 \pm 0.005$ with EGF as input and $0.970 \pm 0.008$ with EGFR as input.

\section{Conclusion and Further Work}
This work shows experimentally, and for the first time, that we can correlate structural properties of biological pathways to their dynamical properties. We consider this result very promising and deserving of further investigation. Indeed, the promise of this approach if that of creating a novel class of learning methods for pathways, which could help to better study the functioning of living cells.

We consider this approach impacting for two main reasons. Firstly, predicting dynamical properties of pathways with Deep Learning methods is faster than performing numerical or stochastic simulations. While the training process is expensive in terms of time required, it needs to be performed only once, and can be amortized nicely as more and larger subgraphs are used. Secondly, our approach works considering only a minimal amount of information regarding the pathway: its structure, an input and output node, and the types of nodes and relations between them. Hence, it is applicable even in cases where other information, such as kinetic constants, is not available.

One research direction to pursue in the immediate future concerns evaluating this architecture on datasets where the information we omitted in the present study has been added. For example, it would be interesting to assess how the addition of edge labels (multiplicities of reactants/products), or kinetic formulas, would affect performances. These additional information could be useful, for example, to help the preditions of the model in case of smaller subgraphs. Moreover, one could think to extend this approach to the assessment of other related dynamical properties of pathways, such as other notions of robustness, or even completely different ones such as monotonicity, or oscillatory and bistability properties.

We also believe this work opens up fascinating research scenarios to pursue in the medium term. One important aspect that needs further investigation is model explainability. Even though their predictive result are very often excellent, neural networks are sometimes seen as black-box since the exact function (in this case, from pathway structure to robustness) they approximate is difficult to characterize. One possible research direction is to investigate generative models, to uncover the underlying structure of pathway space and help design better predictive models, as well as provide better explanations to the decisions they take.

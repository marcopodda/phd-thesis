% Chapter 1

\chapter{Introduction}\label{ch:introduction}
One of the long-standing goals of humanity is to understand the mechanisms that underlie biological life. The journey towards this end objective requires to solve several fascinating sub-problems: how do biological entities work at the cell level? Which processes determine a particular phenotype? How can we cure diseases? These broad questions require dedicated scientific studies, which necessarily entail analyzing large quantities of data to model complex phenomena. In this sense, the latest technological advancements in collecting, storing, and processing biological information have brought forth unprecedented progress. At the same time, an equal effort has been dedicated to developing computational methodologies that learn difficult biological tasks by extracting useful knowledge from these sheer amounts of data. Among the many, Deep Learning with neural networks \citep{lecun2015naturedeeplearning} has been undoubtedly one of the driving forces in these latest years. As we write this thesis, a Deep Learning model called \emph{AlphaFold} has made a giant leap towards reliably predicting a protein's structure from its sequence of amino acids --- the so-called \emph{protein folding} problem \citep{senior2020alphafold}. With Deep Learning, we can now predict quantum properties of chemical structures within chemical accuracy, being 300k times faster than numerical simulations  \citep{gilmer2017neuralmessagepassing}. A Deep Learning method called \emph{DeepVariant} can reconstruct a true genome sequence from High-Throughput Sequencing data with significantly greater accuracy than previous classical methods \citep{poplin2018deepvariant}. Lastly, Deep Learning has been successfully applied to counter antibiotic resistance by finding chemically different antibiotics from known ones but having the same bactericidal activity \citep{stokes2020deeplearningantibiotic}. All these life sciences problems --- and most life sciences problems more broadly --- share apparent commonalities. One is that each of them concerns a poorly characterized phenomenon, meaning that the biological function that relates the data to the observed outcomes is only partially understood. In this case, Deep Learning is helpful because it shifts the burden of finding a suitable representation of the input data from the user to the modeling machine \citep{bengio2014representationlearning}. In other words, instead of hand-crafting the knowledge required to solve a task, Deep Learning models infer task-specific knowledge from the data themselves. Another commonality regards the nature of the biological datum. Indeed, most biological data are \emph{structured}, meaning that they can be decomposed into a set of entities and relations between them. Structured data in life sciences are ubiquitous: some examples are shown in Figure \ref{fig:biological-data}. Proteins can be thought of as \emph{sequences} of amino acids. Gene ontologies, which describe gene attributes across various species, are organized in \emph{hierarchies}. Molecules are naturally represented as \emph{graphs} where atoms are vertices, and chemical bonds are edges. Clearly, a biological datum taken as a whole has more informative content than its constituents taken in isolation. On the one hand, exploiting this superior expressiveness can be the key to design successful models of complex biological processes. On the other hand, working with structured data introduces additional challenges, such as representing them in a succinct way and preserving their relational nature at the same time --- or in other words, how to keep their informative content intact while also being computationally efficient.
\begin{figure*}[h!]
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \resizebox{.9\textwidth}{!}{\input{Figures/Chapter1/protein-sequence.tex}}
        \caption{Protein sequence.}
        \label{subfig:protein-sequence}
    \end{subfigure}
    \begin{subfigure}[b]{0.36\linewidth}
        \centering
        \resizebox{.9\textwidth}{!}{\input{Figures/Chapter1/gene-ontology.tex}}
        \caption{Gene ontology.}
        \label{subfig:gene-ontology}
    \end{subfigure}
    \begin{subfigure}[b]{0.28\linewidth}
        \centering
        \resizebox{.7\textwidth}{!}{\input{Figures/Chapter1/molecular-graph.tex}}
        \caption{Molecular graph.}
        \label{subfig:molecular-graph}
    \end{subfigure}
    \caption{Examples of biological data.}\label{fig:biological-data}
\end{figure*}
Traditional Machine Learning models are designed to work with non-relational real-valued vectors. Thus, they cannot be applied off-the-self to problems where data have a complex structure. In principle, it is possible to circumvent this complexity by hard-coding the relational information into vectors of structural descriptors. While useful to some extent, this approach builds upon a somewhat under-specified representation of knowledge. Moreover, it heavily relies on domain expertise to choose which structural information to retain and which to discard. In stark contrast, Deep Learning methodologies for structured domains allow to extract this relational knowledge directly and in a goal-oriented way. Neural approaches for the recursive processing of structured data have been studied long before the advent of Deep Learning \cite{elman1990rnn,hochreiter1997lstm,sperduti1997generalizedneuron,frasconi1998general} and were extended to general classes of graph data while modern Deep Learning methods were still under development \citep{micheli2009nn4g,scarselli2009gnn}. These research efforts culminated in three types of neural networks for structured data: Recurrent Neural Networks for sequence data, Recursive Neural Networks for tree/hierarchical data, and Deep Graph Networks for general graphs (\eg cyclic/acyclic, directed/undirected). Recently, with the availability of large data sources and the increase of computational power, these models have been rediscovered and improved by the Deep Learning community. At the same time, their wide spectrum of potential applications attracted the attention of researchers coming from very different domains. When applied to challenging biological tasks, these models have demonstrated strong performances and great success
\cite{bianucci2000applicationcascorstructurechemistry,baldi2013recursiveneuralnets,duvenaud2015molecularfingerprint,bradshaw2019moleculechef}.

\section{Objective and Contributions}
The high-level objective of this thesis is to give the reader broad perspective on the kinds of biological tasks that can be tackled with Deep Learning for structured data. We do so by focusing on two classes of learning problems: \emph{predictive} and \emph{generative}. Predictive problems deal with modeling an unknown input-output relationship of interest; generative problems deal with learning the data distribution to sample novel instances. This thesis present one technical contribution and one applied contribution to a specific life sciences problem for each of these two learning paradigms.

The first technical contribution is an empirical evaluation of different Deep Graph Networks proposed in the literature for graph classification tasks. The applied contribution pertains to the life sciences sub-domain of computational biology. Specifically, we develop a novel learning framework for the prediction of dynamical properties of biochemical pathways with Deep Graph Networks. In the context of generative learning, the technical contribution is a general-purpose, domain-agnostic generative model of unlabeled graphs. The applied contribution concerns an applies deep generative modeling to the life sciences sub-domain of computational chemistry. Specifically, we present a novel generative model of molecules for \emph{de novo} drug design. Following, we discuss our contributions in detail.

\subsubsection*{An Empirical Evaluation of Deep Graph Networks for Graph Classification}
We provide a systematic and fair evaluation of Deep Graph Networks for graph classification tasks. In the literature, these models' evaluation is often biased by poor experimental setups and unfair comparisons. We develop a unified framework under which these models can be evaluated coherently and their results adequately compared. This work lets us also shed light on what Deep Graph Networks learn differently from structure-agnostic baselines and the relation between structural features of the graphs (precisely, the vertex degree) and model performances. Finally, we draw useful insights on how Deep Graph Networks behave on certain tasks in relation to their inner components.

\subsubsection*{A Novel Learning Framework for the Prediction of Dynamical Properties of Biochemical Pathways}
We address a newly formulated predictive problem in the context of biological pathways. A biochemical pathway is a dynamical system in which molecules interact with each other through chemical reactions. Each molecule can take different roles in a biochemical pathway: it can be a reactant, a product of some chemical reaction, and a promoter (or inhibitor) of other reactions. The pathway state at a given moment in time corresponds to some function being performed or not (for example, cell replication). Biochemical pathways can be conveniently represented as Petri networks, allowing researchers to study dynamical properties of the system such as reachability of steady states, causality between species, and robustness. Specifically, we focus on the property of robustness, which can be described informally as the pathway's resilience to maintain its function against external perturbations. The standard way to assess robustness relies on an extensive number of numerical simulations, which in turn are expensive in terms of computational time. Our contribution is to show that, given pathways represented as Petri networks, we can reliably predict an indicator of their robustness using a Deep Graph Network instead. The assumption at the basis of this work is that the structure of the pathway, and not other factors such as kinetic and chemical constants, provides (in most cases) enough signal to let the model infer its robustness. We show experimentally that this is indeed possible to a reasonable extent, opening up to a future where costly numerical simulations can be bypassed altogether. Furthermore, we study how different architectural choices of the Deep Graph Network influence performances.

\subsubsection*{A Generative Model for Unlabeled Graph Generation}
Graph generation is a challenging problem with applications in various research fields. We propose a general-purpose recurrent model to generate arbitrary unlabeled graphs, whose structure resembles that of training samples. Despite its conceptual simplicity, we experimentally demonstrate its effectiveness on a wide range of graph datasets, and that it is able to generalize both local and global properties of the training graphs. The results show that the proposed approach outperforms canonical baselines from graph theory and performs comparably to the current state of the art on the unlabeled graph generation task.

\subsubsection*{A Generative Model of Molecules for \emph{de novo} Drug Design}
We study the molecular generation problem in the context of \emph{de novo} drug design. Currently, deep generative approaches on this task belong to two broad categories, differing in how molecules are represented. One approach is based on encoding molecular graphs as strings of text and learning a generative language model of such strings. Another, arguably more expressive, approach works directly with the molecular graph. Our work addresses two main limitations of the former generative framework: the generation of chemically invalid or duplicate molecules. We develop a language model for small molecular substructures called fragments, loosely inspired by the well-known paradigm of Fragment-Based Drug Design \citep{erlanson2004fbdd} --- in other words, we generate molecules \emph{fragment-by-fragment}, rather than \emph{atom-by-atom}. We show experimentally that this novel approach greatly improves the validity rate of the generated molecules. To avoid generating duplicate molecules, we present a frequency-based masking strategy that encourages the model to build molecules using infrequent fragments. Our experiments demonstrate that our simple strategy largely outperforms other language model-based competitors, reaching performances comparable to the state of the art in the task typical of graph-based approaches. Moreover, we show how the generated samples are endowed similar molecular properties to those of training samples, even without explicitly being trained to do so.

\section{Structure of the Thesis}
This thesis is divided in four parts. In Part \ref{part:prelimiaries}, we present all the introductory notions to facilitate the understanding of this thesis' content. In the first part of Chapter \ref{ch:neural-networks}, we provide a brief overview of the concepts of Machine Learning that are functional to our discussion. The rest of the chapter is dedicated to discuss the foundations of (Deep) Neural Networks for supervised and unsupervised learning. In Chapter \ref{ch:deep-learning-structures}, we present the class of Deep Learning models for the processing of structured data, namely Recurrent Neural Networks for sequence data, Recursive Neural Networks for tree/hierarchical data, and Deep Graph Networks for graph data. We conclude presenting the framework of Deep Generative Models for graph generation. In Part \ref{part:evaluation-biology}, we present the two contributions pertaining predictive modeling using structured data and Deep Learning, and its application to computational biology. Specifically, in Chapter \ref{ch:evaluation-dgns}, we present a fair and rigorous evaluation of existing Deep Graph Networks; while in Chapter \ref{ch:prediction-biochemical-dgn}, we propose a novel application of Deep Graph Networks to the problem of predicting the dynamical properties of biological pathways. Part \ref{part:generative} of the thesis presents our two original contributions in the context deep generative learning of graphs, and its applications to computational chemistry. In Chapter \ref{ch:deep-generative-learning-graphs}, we discuss a general-purpose  model capable of constructing unattributed graphs by sequentially generating their edges. In Chapter \ref{ch:deep-generative-learning-drug-discovery}, we introduce a novel generative model of molecules in the context of drug discovery. Lastly, Part \ref{part:fin} concerns concluding remarks and future works, which we discuss in Chapter \ref{ch:conclusions}.

\section{Published Works}
Several publications have contributed to the development of this thesis. Section \ref{sec:dgns} is based on the journal article:

\vspace{1em}
\longfullcite{bacciu2020dgn},
\vspace{1em}

\noindent where we re-elaborated the concepts behind Deep Graph Networks in a systematic introductory review.

\noindent Chapter \ref{ch:evaluation-dgns} draws from our conference paper:

\vspace{1em}
\longfullcite{errica2020fairevaluation},
\vspace{1em}

\noindent which presented a fair and rigorous comparison of several state-of-the-art Deep Graph Networks.

\noindent Chapter \ref{ch:prediction-biochemical-dgn} is a compendium of the work developed in two conference papers:

\vspace{1em}
\longfullcite{bove2020prediction}

\longfullcite{podda2020esannbiochemical}.
\vspace{1em}

\noindent The former work first formulated the predictive problem, receiving the Best Paper Award in the related conference. Later, it has been selected to be extended in a post-proceedings journal publication (in review).

\noindent Chapter \ref{ch:deep-generative-learning-graphs} derives from our work on the conference paper:

\vspace{1em}
\longfullcite{bacciu2019graphesann},
\vspace{1em}

\noindent which was later extended to a journal article in:

\vspace{1em}
\longfullcite{bacciu2020graphgeneration}.
\vspace{1em}

\noindent Lastly, Chapter \ref{ch:deep-generative-learning-drug-discovery} concerns work published in the following conference paper:

\vspace{1em}
\longfullcite{podda2020aistats}.
\vspace{1em}
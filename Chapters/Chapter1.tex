% Chapter 1

\chapter{Introduction}\label{ch:introduction}
One of the long-standing goals of humanity is to understand the mechanisms that underlie biological life. The journey towards this end objective requires to solve several fascinating sub-problems: how do biological entities work at the cell level? Which processes determine a particular phenotype? How can we cure diseases? These broad questions require dedicated scientific studies, which necessarily entail analyzing large quantities of data to model complex phenomena. In this sense, the latest technological advancements in collecting, storing, and processing biological information have brought forth unprecedented progress. At the same time, an equal effort has been dedicated to developing computational methodologies that learn difficult biological tasks by extracting useful knowledge from these sheer amounts of data. Among the many, Deep Learning with neural networks \citep{lecun2015naturedeeplearning} has been undoubtedly one of the driving forces. As we write this thesis, a Deep Learning model called \emph{AlphaFold} has made a giant leap towards reliably predicting a protein's structure from its sequence of amino acids -- the so-called \emph{protein folding} problem \citep{senior2020alphafold}. With Deep Learning, we can now predict quantum properties of chemical structures within chemical accuracy, being 300k times faster than numerical simulations  \citep{gilmer2017neuralmessagepassing}. A Deep Learning method called \emph{DeepVariant} can reconstruct a true genome sequence from High-Throughput Sequencing data with significantly greater accuracy than previous classical methods \citep{poplin2018deepvariant}. Lastly, Deep Learning has been successfully applied to counter antibiotic resistance by finding chemically different antibiotics from known ones but having the same bactericidal activity \citep{stokes2020deeplearningantibiotic}. All these life sciences problems -- and most life sciences problems more broadly -- share critical commonalities. Perhaps the most apparent is that each of them concerns a poorly characterized phenomenon, meaning that the biological function that relates the data to the observed outcomes is only partially understood. In this case, Deep Learning is helpful because it shifts the burden of finding a suitable representation of the input data from the end-user to the modeling machine  \citep{bengio2014representationlearning}. In other words, instead of hand-crafting the knowledge required to solve a task, Deep Learning models infer task-specific knowledge from the data themselves. Another commonality regards the nature of the biological datum. Indeed, most biological data are \emph{structured}, meaning that they can be decomposed into a set of entities and relations between them. Examples of structured data in life sciences are ubiquitous: proteins can be thought of as \emph{sequences} of amino acids. Gene ontologies, which describe gene attributes across various species, are organized with \emph{hierarchical} data structures. Molecules are naturally represented as \emph{graphs} where atoms are vertices, and chemical bonds are edges. Clearly, a biological datum taken as a whole has more informative content than its constituents taken in isolation. On the one hand, this superior expressiveness allows devising more effective models of biological processes. On the other hand, working with structured data introduces additional challenges, such as representing them in a computationally efficient way while also retaining their relational nature.

Traditional Machine Learning models are designed to work with unstructured, real-valued vectors. Thus, they cannot be applied off-the-self to problems where data have a complex structure. In principle, it is possible to circumvent this complexity by \quotes{flattening} the relational information into vectors of structural descriptors. While useful to some extent, this approach is built upon a somewhat under-specified representation of knowledge. Moreover, it heavily relies on domain expertise to choose what structural information needs to be retained and what needs to be discarded.

Conversely, Deep Learning methodologies allow to handle structured data directly and extract knowledge in a goal-oriented way without losing the relational information. This feature well combines with the powerful nature of neural networks, which are capable of approximating any complex function with arbitrary precision under mild assumptions  \citep{cybenko1998approximationuniversal,hammer2005universal}. Neural approaches for the recursive processing of structured data have been studied long before the advent of Deep Learning \cite{elman1990rnn,sperduti1997generalizedneuron,frasconi1998general} and were generalized to arbitrary relational data while modern Deep Learning methods were still under development \citep{micheli2009nn4g,scarselli2009gnn}. These research efforts culminated in three classes of neural networks for structured data: Recurrent Neural Networks for sequence data, Recursive Neural Networks for tree/hierarchical data, and Deep Graph Networks for general graph data. Recently, with the availability of large data sources and the increase of computational power, these models have been rediscovered continuously and improved, attracting the attention of researchers coming from very different domains. Along with this ever-increasing usage, these models have demonstrated strong performances and great success when applied on challenging biological tasks.
\cite{bianucci2000applicationcascorstructurechemistry,baldi2013recursiveneuralnets,duvenaud2015molecularfingerprint,bradshaw2019moleculechef}.

\section{Objective and Contributions}
The high-level objective of this thesis is to provide the reader with a broad perspective on the kinds of biological tasks that can be tackled with Deep Learning. We do so by focusing on two classes of problems: \emph{predictive} and \emph{generative}. Predictive problems deal with modeling an unknown input-output relationship of interest. Generative problems deal with learning the data distribution to sample novel instances. We present two original contributions for each of these two classes of problems: one concerning methodological aspects and another concerning an application to real-world biological problems. Following, we discuss our contributions in detail.

\subsubsection*{Methodological Contribution to Predictive Deep Learning on Graphs}
We provide a systematic and fair evaluation of Deep Graph Networks for graph classification tasks. In the literature, these models' evaluation is often biased by poor experimental setups and unfair comparisons. We develop a unified framework under which these models can be evaluated coherently and their results adequately compared. This work lets us also shed light on what Deep Graph Networks learn differently from structure-agnostic baselines and the relation between structural features of the graphs (precisely, the vertex degree) and model performances.

\subsubsection*{Applied Contribution to Computational Biology}
We address a newly formulated predictive problem in the context of biological pathways. A biochemical pathway is a dynamical system in which molecules interact with each other through chemical reactions. Each molecule can take different roles in a biochemical pathway: it can be a reactant, a product of some chemical reaction, and an inhibitor or promoter of other reactions. The system's state corresponds to some function being performed or not (for example, cell replication). Biochemical pathways can be conveniently represented as Petri networks, allowing researchers to study dynamical properties of the system such as reachability of steady states, causality between species, and robustness. Specifically, we focus on the property of robustness, which can be described informally as the pathway's resilience to maintain its function against external perturbations. The standard way to assess robustness requires an extensive number of numerical simulations, which are usually expensive in terms of computational time. We apply Deep Graph Networks to predict robustness indicators on pathways represented as Petri nets, bypassing the need to performing costly simulations. The assumption at the basis of this work is that the structure of the pathway (and not other factors such as kinetic and chemical constants) is sufficient to be effective in this task. We show experimentally for the first time that this is indeed possible to a reasonable extent. Furthermore, we study how different architectural choices of the Deep Graph Network influence performances.



\subsubsection*{Methodological Contribution to Generative Deep Learning on Graphs}
Graph generation is a challenging problem with applications in various research fields. We propose a general-purpose recurrent model to generate arbitrary unlabelled graphs, whose structure resembles that of training samples. Despite its conceptual simplicity, we experimentally demonstrate its effectiveness on a wide range of graph datasets. The experimental results show that the model outperforms canonical baselines from graph theory and performs comparably to the current state of the art on unlabelled graph generation.

\subsubsection*{Applied Contribution in Computational Chemistry}
We study the molecular generation problem in the context of drug design. Currently, deep generative approaches on this task belong to two broad categories, differing in how molecules are represented. One approach is based on encoding molecular graphs as strings of text and learning a character-based language model on such strings. Another more expressive approach works directly with the molecular graph. The work addresses two limitations of the former generative framework: the generation of invalid or duplicate molecules. To improve validity rates, we develop a language model for small molecular substructures called fragments, loosely inspired by the well-known paradigm of Fragment-Based Drug Design. The proposed architecture generates molecules fragment-by-fragment, instead of atom-by-atom. To improve uniqueness rates, we present a frequency-based masking strategy that helps generate molecules with infrequent fragments. We show experimentally that our model largely outperforms other language model-based competitors, reaching performances comparable to the state of the art in the task. Moreover, we show how the generated samples display molecular properties similar to those of training samples, even in the absence of explicit task-specific supervision.


\section{Structure of the Thesis}
This thesis is divided in four parts. In Part \ref{part:prelimiaries}, we present the necessary introductory notions to facilitate the understanding of the following content. In the first part of Chapter \ref{ch:neural-networks}, we provide a brief overview of the concepts of Machine Learning that are functional to our discussion. The rest of the chapter is spent to discuss about the foundations of Neural Networks and Deep Learning for supervised and unsupervised tasks. In Chapter \ref{ch:deep-learning-structures}, we present the class of Deep Learning models for the processing of structured data, namely Recurrent Neural Networks for sequence data, Recursive Neural Networks for tree/hierarchical data, and Deep Graph Networks for graph data. In Part \ref{part:evaluation-biology}, we present our first two contributions in the context of predictive Deep Learning and its applications to computational biology. Specifically, in Chapter \ref{ch:evaluation-dgns}, we present a fair and rigorous evaluation of existing Deep Graph Networks; while in Chapter \ref{ch:prediction-biochemical-dgn}, we propose a novel application of Deep Graph Networks to the problem of predicting the dynamical properties of biological pathways. Part \ref{part:generative} of the thesis presents our two original contributions in the context deep generative learning of graphs, and its applications to computational chemistry. In Chapter \ref{ch:deep-generative-learning-graphs}, we discuss a general-purpose  model capable of generating unattributed graphs by sequentially generating its edges. In Chapter \ref{ch:deep-generative-learning-drug-discovery}, we introduce a novel generative model of molecules in the context of drug discovery. Lastly, Part \ref{part:fin} concerns concluding remarks, opportuninities and future works, which are discussed in Chapter \ref{ch:conclusions}.

\section{Published Works}
Several publications have contributed to the development of this thesis. Section \ref{sec:dgns} is based on the journal article:

\vspace{1em}
\fullcite{bacciu2020dgn},
\vspace{1em}

\noindent where we re-elaborated the concepts behind Deep Graph Networks in a systematic introductory review. Chapter \ref{ch:evaluation-dgns} draws from our conference paper:

\vspace{1em}
\fullcite{errica2020fairevaluation},
\vspace{1em}

\noindent which presented a fair and rigorous comparison of several state-of-the-art Deep Graph Networks. Chapter \ref{ch:prediction-biochemical-dgn} is a compendium of the work developed in two conference papers:

\vspace{1em}
\fullcite{bove2020prediction}

\fullcite{podda2020esannbiochemical}.
\vspace{1em}

\noindent The former work first formulated the predictive problem, and has been later been extended for post-proceedings journal publication (in review). Chapter \ref{ch:deep-generative-learning-graphs} derives from our work on the conference paper:

\vspace{1em}
\fullcite{bacciu2019graphesann},
\vspace{1em}

\noindent which was later extended to a journal article in:

\vspace{1em}
\fullcite{bacciu2020graphgeneration}.
\vspace{1em}

\noindent Lastly, Chapter \ref{ch:deep-generative-learning-drug-discovery} concerns work published in the following conference paper:

\vspace{1em}
\fullcite{podda2020aistats}.
\vspace{1em}
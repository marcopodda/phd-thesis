\chapter{Deep Generative Learning on Graphs} % Main chapter title
\label{ch:deep-generative-learning-graphs}
In this chapter, we discuss the graph generation problem. The term \quotes{graph generation} is purposedly kept broad, to include a variety of methodologies which learn processes that generate graphs from a set of training examples. This research field originates from generative models of graphs for the theoretical understanding of graph properties, which have been studied thoroughly since the '50s in the field of graph theory. The first known generative model is the  Erd\"{o}s-R\'{e}nyi (ER) model \cite{erdos1959randomgraphs}. The ER model studies random graphs, where the connectivity among graph nodes is modeled independently. This model is useful to study theoretical properties of graphs, such as how the global connectivity of the graph evolves as its size grows. Another historical model of graphs is the  Watts-Strogatz (WS)  model \cite{watts1998smallworld}. The WS model concerns \quotes{small world} graphs, \ie graphs where it is possible to reach any other node in the graph with very short paths, regardless of their ize. This property arises in several real-world graphs such as social and electrical networks. Lastly, the Barab{\'a}si-Albert (BA) model \cite{barabasi1999prefatt}, which models the so-called \quotes{preferential attachment} property, where the connectivity potential of a node is directly proportional to its number of neighbors. While being useful to the study and understanding of graph properties, these models usually fail to generalize to real-world graph distributions, because they can only model one or a limited number of graph properties, and that their parameters cannot be learned from data in general. Other methods such as stochastic block models \citep{airoldi2008mixedstochasticblock}, exponential random graphs \citep{robins2007exponentialrandomgraphs}, and Kronecker graphs \citep{leskovec2010kronecker} can approximate more complex graph distributions such as graphs with communities, but are still limited to specific kinds of graphs and scale poorly to large datasets.
These limitations clearly highlight that, in order to generate graphs for practical applications, a more powerful class of learning models is needed. In this chapter, we review a variety of generative models of graphs based on Deep Learning approaches. The advantages of such models are the possibility to approximate complex distributions efficiently and effectively, and the flexibility to use different generative paradigms to adapt to the task at hand.

\section{The Challenges of Graph Generation}
The problem of generating graph structures from arbitrary distributions is arguably harder than their predictive modeling. Some of the challenges that need to be addressed when designing deep generative models of graphs are related to the complexity of graph spaces. In particular, we report the following:
\begin{itemize}
    \item \emph{size of graphs spaces}. Graphs spaces are combinatorial, and thus very large. For example, the size of the space of undirected graphs with $m$ nodes is ${m \choose 2} = \frac{m(m-1)}{2}$, which becomes very large even for graphs with a moderate number of nodes. Thus, trivial approaches such as exhaustive enumeration are generally intractable. Moreover, real-world graph distributions are usually defined over attributed graphs with variable size, which makes the search space even larger;
    \item \emph{discreteness of graphs spaces}. Graphs are discrete objects. This contrasts with the nature of neural networks, which can backpropagate only through continuous and differentiable objects. Hence, the learning process must be accomodated to work with discrete structures;
    \item \emph{sparsity of graphs spaces}. For most generative tasks, only very small subsets of graph space contain graphs with non-zero probability. Thus, any generative method should be designed to focus on regions where interesting graphs are contained, to avoid efficiency issues;
    \item \emph{complex dependencies}. Most real-world graph have hard structural constraints that are very difficult to enforce on a generative model (which is inherently stochastic). For example, cycle graphs are such that if only one edge is missplaced, the graph is not a cycle anymore;
    \item \emph{non-unique representations}. In general, graphs are invariant under node permutation. Thus, the same graph can be potentially represented by up to $n!$ possible adjacency matrices, depending on the node permutation. This poses constraints on the kind of graph representations a generative model can learn. For example, generative methods that map graphs into a latent space must take into account that different node permutations of the same graph must map to the same latent vector. While invariance to node permutation can be addressed by assuming an order of the graph nodes, this introduces the necessity to maintain order consistency among different graphs.
\end{itemize}

\section{Generative Tasks}
The family of \glspl{dgm} of graphs is flexible enough to model different kinds of generative tasks. Loosely following the taxonomy proposed by \citet{guo2020systematicreviewgenerativegraphs}, we distinguish two main tasks related to graph generation:
\begin{itemize}
    \item \emph{unconditional generation}, where the task is to explicitly learn a distribution $p(\Graph{g})$ over graphs, or some parametrized function that produces samples from it. Here, the term \quotes{unconditional} refers to the fact that the generation starts with drawing a vector $\Vector{z} \in \Real^z$ from some easy to sample prior distribution $p(\Vector{z})$, which is usually assumed to be an isotropic Gaussian or a uniform distribution;
    \item \emph{conditional generation}, where the aim is to learn a conditional distribution $p(\Graph{g} \given \Vector{y})$ with $\Vector{y} \in \Real^y$, or the corresponding parameterized sampling mechanism. The purpose of the conditioning vector is to drive the generative process towards producing a graph with desired characteristics. For example, one might want to generate a graph whose structure resembles that of a graph given as input to the \gls{dgm}. In this case, $\Vector{y}$ is the representation of the conditioning graph.
\end{itemize}
Hereafter, we consider the task of unconditional generation, where we assume access to a dataset of graphs $\Data = \Set{\Graph{g}_i}_{i=1}^n$. Notice from the notation that, at least for the moment, we focus on unattributed graphs. Broadly speaking, defining a \gls{dgm} of graphs requires to specify two components: a \emph{graph decoder}, which takes care of generating a graph, and an end-to-end generative framework used to optimize the model parameters. As regards the latter, common generative frameworks include \glspl{vae}, \glspl{gan}, and flow-based models \citep{rezende2015normalizingflows}. Here, we only describe approaches based on the first two.

\section{Graph Decoders}
The graph decoder is the architectural component that outputs some conditional distribution, which can be sampled to generate a graph. If the framwork in which the decoder is placed allows for inference (such as the \gls{vae}), the conditional is also learned with maximum likelihood; otherwise, (such as with \glspl{gan}) it is used only for sampling, and its parameters are optimized with adversarial training. Ideally, graph decoders should generate permutation invariant graphs; however, this is rarely the case. The major hurdle to devise permutation invariant graph decoders is their computational cost; even though some methods do exist \citep{ermon2020permutationinvariantgraphgeneration}, they are still too inefficient to be deployed in real world scenarios. Thus, in the following, we assume non-invariance.
One particular caveat that needs to be addressed during the training phase of a graph decoder is maintaining the differentiability of the architecture while still generating hard graph samples. This is critical especially in \gls{gan}-like architectures, where the discriminator must be trained with actual graphs. As with sequence generation with \glspl{rnn}, the same techniques (straight-trough gradient estimation, reparameterization, or even reinforcement learning-based techniques \cite{williams1992reinforce}) can be used for this purpose. There are two main paradigms to implement adaptive graph decoders, which we detail in the following.

\subsubsection*{One-shot Decoders}
This class of graph decoders outputs a dense probabilistic adjacency matrix $\ProbAdjMatrix{g} \in \Real^{n \times n}$, where $n$ is the maximum number of nodes allowed. The probabilistic matrix is sampled entry by entry to produce an actual adjacency matrix. The entries of the matrix are modeled as independent Bernoulli variables, which indicate the presence or absence of an edge. In some cases, the elements in the adjacency matrix $a_{ii}$ are modeled as independent Bernoulli variables that specify if a node belongs to the graph or not. In practice, the entries of the matrix are produced by a neural network with sigmoid outputs that predicts an $n \times n$ vector. Thus, the adjacency matrix can be sampled in parallel (hence the term \quotes{one-shot}). Two possible approaches to specify a one-shot decoder are:

\begin{itemize}
    \item \emph{graph-based} decoders, which require a graph representation $\Vector{z}$. In this case, the decoder models the conditional as follows:
    $$p(\ProbAdjMatrix{g} \given \Vector{z}) \approx \prod_{i=1}^n \prod_{j=1}^n p_{\Param}(a_{ij} \given \Vector{z}),$$
    where $p_{\Param}$ is a neural network that predicts the matrix from the graph representation;
    \item \emph{node-based} decoders, which require a matrix of node representations $\Matrix{Z} \in \Real^{n \times z}$. In this case, the decoder models the conditional as follows:
    $$p(\ProbAdjMatrix{g} \given \Matrix{z}) \approx \prod_{i=1}^n \prod_{j=1}^n p_{\Param}(a_{ij} \given \Vector{z}_i, \Vector{z}_j),$$
    where $i$ and $j$ range over the matrix rows. In this case, $p_{\Param}$ is a neural network that takes as input pair of node representations, and applies a sigmoid function to their dot product. The idea is that nodes that are close in representation space should be more likely to be connected.
\end{itemize}
One-shot approaches are usually fast to train and to take samples from. However, they are too simplistic, in that they assume the edges are generated independently (which is usually not the case for real-world graphs). Furthermore, the maximum number of nodes must be pre-specified, which makes them unable to generalize to larger graphs.

\subsubsection*{Autoregressive Decoders}
Autoregressive decoders assume that graphs are generated by some sequential process that involves its set of nodes. Specifically, the generative process is the following:
$$p(\Graph{s}) = \int p(\Graph{s}, \pi)\, d\pi = \int p(\Graph{s} \given \pi)\, q(\pi)\, d\pi,$$
where $\Graph{s}$ are sequences that generate graphs one component at a time, and the order of generation is given directly or indirectly by a nodes permutation drawn from a prior $q(\pi)$. The idea is to decompose the generating sequence autoregressively as follows:
$$p(\Graph{s}) = \int p(\Graph{s}_i \given \Graph{s}_{[<i]}, \pi)\, q(\pi)\, d\pi.$$
However, this requires to integrate over all $n!$ possible node permutations, which becomes intractable for moderately large graphs. An approximate solution to this issue is to assume some ordering and create the sequences before training, as a preprocessing step. Once the sequence are fixed, the chain-rule decomposition becomes tractable:
$$p(\Graph{s}) \approx p_{\Param}^{\pi}(\Graph{s}) = p_{\Param}^{\pi}(\Graph{s}_i \given \Graph{s}_{[<i]}).$$
Depending on the nature of the graph generating sequence, we distinguish four possible approaches to develop autoregressive graph decoders:
\begin{itemize}
    \item \emph{node-based} approaches decompose the graph as a sequence of actions performed on an initially empty graph. These action correspond to decisions such as whether to add a node to the existing graph, and which nodes it must be connected to. In this case, $\Graph{s}_{[<i]}$ is a vector that represents the current state of the graph. For all these models, one has two options as to how to implement the autoregressive network. One approach is to use a hierarchy of \glspl{rnn}: one keeps track of the state of the nodes added to the graph, and the other is responsible to connect newly added nodes to the current graph, given the state of the first \citep{you2018graphrnn}. The other choice is to update the state of the current graph with a \gls{dgn}, which is passed to the networks responsible of deciding which action to perform \citep{li2018learningdeepgmg};
    \item \emph{edge-based} approaches decompose the graph as a sequence of edges. To produce an ordered sequence of edges, one must first order its nodes, then label the nodes with progressive integers, and then sort its set of edges in lexicographic order. In this case, $\Graph{s}_{[<i]}$ represents the state of the graph indirectly, by keeping memory of the edges of the sequence already generated. These approaches are mostly implemented with \glspl{rnn} \citep{goyal2020graphgen,bacciu2019edgegraphgenrnn};
    \item \emph{rule-based} approaches can be applied in cases where the graph generation can be decomposed in a sequence of production rules over some known grammar (\eg molecules or computer programs) \citep{kusner2017grammarvae,dai2018sdvae}. In this case, the model generates a sequence of production rules to construct a desired graph;
    \item \emph{motif-based} approaches decompose the graph as a sequence (or even a tree) of \emph{motifs}, \ie very small and manageable subgraphs, which are combined together adaptively. Here, the challenge is mainly how to decompose the graphs into sequences of motifs,
\end{itemize}
A special kind of sequential decomposition for graphs is one applied to molecules. In fact, molecular graphs can be linearized with approximated graph canonization into strings of the so-called \gls{smiles} language \citep{weininger1988smiles}. In practice, the SMILES encoding consists in transforming a molecular graph in a \gls{dpag}, breaking cycles in a certain order. Than, the \gls{dpag} is traversed in depth-first order, and a character is associated in an orderly manner to the atoms, chemical bonds and other chemical constructs. The result is a string representing the graph structure. Various canonization techniques can be applied during the process, such that the obtained string is unique for each unique graph (up to a few degenerate cases).

When a domain-specific linearization techniques such as SMILES are not available, the sequences representing the graph generative process are constructed based on some node ordering strategy. One general strategy to do so is to choose one node at random, then visit the graph nodes with a depth-first or breadth-first traversal. The order by which the nodes are visited is used to determine the order of the elements in the sequences. Clearly, this approach is not optimal since it heavily depends on the starting node, and may produce very different sequences for different starting node choices. However, it has been shown to work empirically \citep{you2018graphrnn,li2018learningdeepgmg,bacciu2019edgegraphgenrnn,goyal2020graphgen}. The pros and cons of autoregressive decoders are orthogonal to those of one-shot decoders: briefly, they allow to generate variable-sized graphs seamlessly, and they can model dependencies between nodes and edges by means of the autoregressive property. However, both training and sampling processes are slower in terms of computational time, because the graphs are reconstructed one sequence element at a time and not in parallel.

\section{Performance Evaluation}\label{sec:evaluation-generative-graphs}
The desired end result of training a generative model is that the structure of samples generated by the network should resemble that of graphs in the training set, without being identical. This is a very different setting with respect to predictive tasks, since the samples of a generative model do not exist until they are generated. Hence, one does not have access to held-out ground truth values to measure generalization. Furthermore, the model can very easily obtain deceptive-looking performances by learning to replicate training graphs exactly, or repeating the same graph typology over and over. Thus, one critical aspect of using generative models of graphs is how to assess performances. Below, we define two broad classes of metrics that allow the evaluation of graph models, assuming the availability of a training sample $\TrainingSample \subseteq \Data = \Set{\Graph{g}}_{i=1}^n$ , and a collection\footnote{Here, we use the term \quotes{collection} to indicate a multiset, meaning that it can possibly contain duplicate elements.} $\GeneratedSample = \Set{\Graph{g'}}_{i=1}^m$ of samples generated by the model.

\subsection{Quantitative Metrics}
\emph{Quantitative} metrics measure the rate at which the generative model produces diverse and heterogeneous graphs, without taking into account structural similarity. The three main quantitative matrics considered in the literature are:
\begin{itemize}
    \item \emph{novelty} measures the ratio of generated samples that are not training samples. A high novelty indicates that the model has not learned to replicate training graphs. Formally, it is measured as $1 - \frac{|\GeneratedSample \Inter \TrainingSample|}{|\TrainingSample|}$;
    \item \emph{uniqueness} measures the ratio of unique graphs with respect to the total number of graphs generated. A low uniqueness rate might indicate that the model has overfit one specific typology of graph. To calculate uniqueness, one first checks every graph for isomorphism with every other graph in the generated sample, removing them. If the resulting set of unique graphs is indicated by $\UniqueSample$, uniqueness can be simply calculated as $\frac{|\UniqueSample|}{|\GeneratedSample|}$.
    \item \emph{validity} measures the ratio of generated graphs that respect some validity constraint, out of the total number of graphs generated. To calculate validity, one needs to check wheter every generated graph satisfies some structural constraint or not (\eg the presence of a cycle). If $\ValidSample$ is the collection of graphs that satisfy the structural constraints, validity is calculated as $\frac{|\ValidSample|}{|\GeneratedSample|}$. This metric is particularly useful in molecular generation tasks, since chemically invalid molecules are useless. When assessing validity is required, novelty and uniqueness are usually conditioned on validity first, meaning that the all the invalid graphs are removed from $\GeneratedSample$ before calculating these two metrics.
\end{itemize}

\subsection{Qualitative Metrics}
Quantitative metrics give only one side of the spectrum relatively to how a generative model is performing. For example, assessing novelty alone might be misleading, since a high novelty rate can also be associated to underfitting (meaning that the model generates graphs very different from the training sample, which are trivially novel). Thus, a proper evaluation of generative models must also include a series of metrics that consider the structural properties of the generated graphs. We call such metrics \emph{qualitative}. The framework under which qualitative metrics are assessed consists of comparing the empirical distribution of a certain graph property in the training sample, to the empirical distribution on the generated sample. Given a generic graph with $d$ nodes, coming from one of the two samples indifferently, a relevant subset of such properties includes:
\begin{itemize}
    \item node degree distribution, that is, an $d$-dimensional vector where each position contains the degree of the corresponding node. Notice the the length of the vector may differ across different graphs, since their number of nodes may change;
    \item clustering coefficient distribution. The clustering coefficient of a node $v$ is defined as the ratio between the number of actual connections between neighbors of $v$ out of the total number of possible connections. In other words, it is a relative measure of how many \quotes{closed triangles} (fully connected graphs with three nodes) the node is part of. Similarly to the node degree distribution, it consists of an $d$-dimensional vector where each position contains the clustering coefficient of the corresponding node;
    \item number of nodes of the graph, which is a single integer;
    \item number of edges of the graph, which is again a single integer;
    \item average orbit counts. Orbits are subgraph with 4 nodes. Counting orbits in a graph can be viewed as a generalization of the clustering coefficient to 4-node subgraphs instead of triangles. In practice, it consists in a $dk$-dimensional vector, where $k$ is the number of orbits considered;
    \item \gls{nspdk} \citep{costa2010nspdk}, which measures the similarity between two graphs by counting the number of matching induced subgraphs between them. The subgraphs are derived node-wise, by considering neighborhoods of a node comprising nodes at increasing path lengths. Differently from the other qualitative metrics, the \gls{nspdk} provides a global measure of similarity between graphs, since it is based on multiple subgraph matchings. In practice, it is a vector of length $m-1$ ($n-1$, respectively), where each position measures the similarity of the graph with another graph in the sample.
\end{itemize}
Once the graph properties are calculated for each graph in the sample, there are two options to measure the distance between the empirical distribution of the training sample versus the generated sample, based on their respective number of elements:
\begin{itemize}
    \item if $n = m$, one can compute their empirical \gls{kld} as follows:
    $$\EKLD{\Prop(\GeneratedSample)}{\Prop(\TrainingSample)} = \frac{1}{n} \sum_{i=1}^{n} \Prop(\Graph{g}_{(i)}) \log \Par{\frac{\Prop(\Graph{g'}_{(i)})}{\Prop(\Graph{g}_{(i)})}},$$
    where $\Prop$ is one of the properties mentioned above;
    \item if $n \neq m$, one can either concatenate all the values of the property for each each in the sample, and then fit a histogram with an equal number of bins to make their length match in order to apply the empirical \gls{kld}. Another, more general, approach to compare distribution when the two samples have different lengths is to compute their \gls{mmd} \citep{gretton2012mmdkernel}. Intuitively, the \gls{mmd} measures the distance between two distributions as the sum of the distances of between their matching moments. The computation of these distances can be generalized to an infinite space of moments by applying a kernel trick \citep{smola2008kernels}.
\end{itemize}

\section{A Model for Edge-Based Graph Generation}
\subsection{Introduction}
In this section, we present an original contribution. Specifically, we introduce a novel generative model for graphs, capable of generating unattributed graphs coming from very different graph distributions. We transform graphs into sequences of ordered edges, from which we extract two sequences derived from the edge endpoints. We use a model composed of two \glspl{rnn} to learn the probability distribution of such sequences: the first is an autoregressive network which generates a specification of the graph to produce, which is completed into a graph by the second network. We experiment extensively with the proposed model, comparing its performances with a pool of baselines, one of which is a \gls{dgm} of graphs that holds state-of-the-art performances at the generative task. The experimental framework has been designed to evaluate the proposed model on concerning both quantitative and qualitative aspects, as discussed in Section \ref{sec:evaluation-generative-graphs}. Our experiments demonstrate that, under our evaluation framework, the proposed model is able to perform at, and sometimes surpass, the state-of-the-art in the task of generating graphs coming from very different distributions. Furthermore, we study the effect of changing the order of the edge sequence by experimenting with different node orderings. We show that the chosen node ordering strategy  is more effective for learning complex dependencies than the alternatives, and produces graphs of better quality.

\subsection{Methods}
In this section, we present the methodologies used to develop the model. In particular, we formally introduce the concept of ordered edge sequences, we develop the model,and  we show how it is trained and how graph generation is achieved.

\subsubsection*{Ordered Edge Sequences}
Let $\Graph{g} = \Tuple{\Nodes{g}, \Edges{g}}$ be a fully connected unattributed graph with $n$ nodes and $m$ edges. We assume $\Graph{g}$ is undirected for simplicity, without loss of generality. Let $\gamma: \Nodes{g} \shortrightarrow \Natural_{+}$ be a bijective node labelling function which assigns a unique positive integer (which we call node ID) to each node in the graph; thus, $\gamma$ defines a total order over the nodes of $\Graph{g}$. The \emph{ordered edge sequence} $\Cal{S}$ of graph $\Graph{g}$ is the sequence of pairs:
$$\OES{S}_{\Graph{g}} = ((s_1, e_1), (s_2, e_2), \ldots, (s_m, e_m)),$$
where $(\gamma^{-1}(s_i), \gamma^{-1}(e_i)) \in \Edges{g}$, and such that it is ordered lexicographically according to the IDs assigned to the nodes, \ie $(s_i,e_i) \leq (s_j,e_j)$ if and only if $s_i < s_j$, or $s_i = s_j$ and $e_i \leq e_j$. Given a generic pair $(s_i,e_i) \in \OES{S}_{\Graph{g}}$, we call $s_i \in \Natural_{+}$ its \emph{starting node} and $e_i \in \Natural_{+}$ its \emph{ending node}. Finally, let us define the \emph{starting sequence} $\Start{\OES{S}_{\Graph{g}}} = (s_1, s_2, \ldots, s_m)$, the sequence corresponding of starting nodes ordered as in $\Graph{e}$, and analogously, the \emph{ending sequence} $\End{\OES{S}_{\Graph{g}}} = (e_1, e_2, \ldots, e_m)$, corresponding to the ending nodes ordered as in $\OES{S}$. For conciseness, let us omit the dependece of $\OES{S}_{\Graph{g}}$ on the graph $\Graph{g}$, and of the starting and ending sequences from $\OES{S}_{\Graph{g}}$ whenever they are clear from the context. Clearly, the choice of the labelling function $\gamma$ is critical in determining the ordered sequence of a graph. Given the graph $\Graph{g}$, we choose to implement $\gamma$ with the following algorithm:
\begin{itemize}
    \item first, select a node $v_1$ at random from its set of nodes $\Nodes{g}$, and set its node ID as $\gamma(v_1) = \SF{1}$;
    \item then, traverse the graph in breadth-first order. Let $V = (v_2, v_3, \ldots, v_{n})$ be the ordered sequence of nodes visited during the traversal, excluding $v_s$. Assign node ID $\gamma(v_i) = \SF{i},\, \forall v_i \in V,\, i=2, \ldots, n$.
\end{itemize}
Assuming graph $\Graph{g}$ has the structure shown in Figure \ref{fig:example-graph}, and that node $v_1$ is chosen as the root node for the visit, an example of how the graph nodes are labelled by $\gamma$ is shown in Figure \ref{fig:labelled-graph}. Notice that $\gamma$ is trivially bijective, since it assigns a different integer to each node. Once the nodes are labelled, the ordered edge sequence of $\Graph{g}$ is $\OES{S} = (\OElem{1}{2},\OElem{1}{3},\OElem{1}{4},\OElem{3}{4},\OElem{3}{5})$, with $\tau_S = (\SF{1},\SF{1},\SF{1},\SF{3},\SF{3})$ and $\tau_E = (\SF{2},\SF{3},\SF{4},\SF{4},\SF{5})$. Notice that the graph can be readily reconstructed from its orderd edge sequence by first applying the inverse function $\gamma^{-1}$ to each element of its pairs to obtain $\Edges{g}$, which in turn gives $\Nodes{g}$ since we assumed that $\Graph{g}$ is fully connected.

\begin{figure*}[h!]
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \resizebox{.8\textwidth}{!}{\input{Figures/Chapter6/example-graph.tex}}
        \caption{}
        \label{fig:example-graph}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \resizebox{.8\textwidth}{!}{\input{Figures/Chapter6/labelled-graph.tex}}
        \caption{}
        \label{fig:labelled-graph}
    \end{subfigure}
    \caption{({\scriptsize A}): an example graph, where the starting node $v_1$ of the labelling algorithm is marked with a thicker border. ({\scriptsize B}): the same graph, where nodes are labelled according to a breadth-first visit of the graph rooted at $v_1$.}
    \label{fig:labelling-example}
\end{figure*}

\subsubsection*{Model}
Our goal is to model $p(\OES{S})$, the probability of ordered edge sequences, using a dataset $\Data = \Set{\OES{S}_{(i)}}_{i=1}^n$. Our key observation is that any ordered edge sequence $\OES{S}$ is uniquely defined by its starting and ending sequences. Therefore, instead of working on the ordered edge sequence directly, we work on their starting and ending sequences. Specifically, we model the probability of sampling $\OES{S}$ from $p(\OES{S})$ as follows:
$$p(\OES{S}) = p(\tau_S, \tau_E) = p(\tau_E \given \tau_S)\, q(\tau_S) = \prod_{i=1}^{|\OES{S}|} p(e_i \given s_i)\, \prod_{j=1}^{|\OES{S}|} q(s_j \given s_{[<j]}).$$
Intuitively, the generative process specified by the distribution is the following:
\begin{itemize}
    \item first, one samples $\tau_S$ from a prior probability $q(\tau_S)$, which is modeled autoregressively;
    \item once $\tau_S$ is available, it is used to condition the prediction of the ending sequence $\tau_E$.
\end{itemize}
Once both $\tau_S$ and $\tau_E$ are available, the ordered edge sequence (and thus, the corresponding graph) an be reconstructed simply pairing their elements. Here, we make an important remark. Notice that the the probability of $q(\tau_S)$ is modeled autoregressively, while the conditional $p(\tau_E \given \tau_S)$ is not. This means that to generate a sequence, the only stochastic part in the model relates to $q(\tau_S)$, while predicting $\tau_E$ is deterministic once $\tau_S$ is known. One possible interpretation of $q(\tau_S)$ is that it acts as a \quotes{soft prior} of the structure of the graph: in fact, it specifies a subset of relevant nodes for the generation and their degree, expressed as the number of times they appear in the starting sequence. The conditional $p(\tau_E \given \tau_S)$ uses this information to \quotes{complete} the graph based on the information provided by the prior.

\subsubsection*{Training}
The model specified above is implemented as two \glspl{rnn} in cascade, which we refer to as $q_{\EncParam}$ and $p_{\DecParam}$ respectively. The two sequences (starting and ending) are first encoded as sequences of $(k+2)$-dimensional one-hot vectors, where $k$ is the largest node ID assigned by $\gamma$ to any node in the dataset, and $+2$ is added for the start of sequence and end of sequence tokens. We indicate the one-hot encoded starting sequence with the notation $\Vector{s} = (\Elem{s}{1}, \ldots, \Elem{s}{|\tau_S|}, \EOS)$, where $\Elem{s}{i} \in \Real^{k+2}$, and analogously, we use $\Vector{e} = (\Elem{e}{1}, \ldots, \Elem{e}{|\tau_E|})$ for the one-hot encoded ending sequence, with $\Vector{e}_i \in \Real^{k+2}$. Notice that an end of sequence token is added to $\Vector{s}$: when it is predicted, the autoregressive process that yields $\Vector{s}$ is interrupted. Our dataset is has thus the following form: $\Data = \Set{(\Vector{s}_{(i)}, \Vector{e}_{(i)})}_{i=1}^n$. Focusing on a single training pair $(\Vector{s}, \Vector{e})$, we explain the feed-forward phase of the network. Firstly, the sequence $\Vector{s}$ is processed by $q_{\EncParam}$. Given an element $\Elem{s}{i} \in \Vector{s}$, the per-element output of $q_{\EncParam}$ is obtained as follows:
\begin{align*}
    \Elem{h}{i} &= \Op{GRU}_{\EncParam}(\Elem{h}{i-1}, \Matrix{E}^{\Transpose}\Elem{s}{i-1})\\
    \Elem{o}{i} &= \softmax(\Matrix{V}_{\EncParam}\Elem{h}{i} + \Vector{b}).
\end{align*}
In the formula above, $\Matrix{E} \in \Real^{k \times h}$ is an \emph{embedding matrix} whose entries are learnable, and the dot product selects the column of $\Vector{E}$ corresponding to the embedding of the node ID assigned to $\Elem{s}{i}$. Furthermore, $\Elem{h}{i} \in \Real^h$ is a hidden state, $\Op{GRU}_{\EncParam}$ is a multi-layer GRU network, $\Matrix{V}_{\EncParam} \in \Real^{h \times (k+2)}$ is an output weight matrix,  $\Vector{b} \in \Real^{k+2}$ is a bias vector, and $\Elem{o}{i}$ is an output distribution over all the possible node IDs. The process is initialized by setting $\Elem{s}{0} = \SOS$ and $\Elem{h}{0} = \Zeros$. We use teacher forcing, meaning that, at each step, the input of the network is the ground truth value taken from $\Vector{s}$, rather than the output predicted by the network. Lastly, the output $\Elem{o}{i}$ is compared to the ground truth value $\Elem{s}{i}$ using the \gls{ce} loss function. For the whole starting sequence, the computed loss is the following:
$$\Loss(\EncParam, \Vector{s}) = \frac{1}{|\Vector{s}|+1} \sum_{i=1}^{|\Vector{s}|+1} \mathrm{CE}(\Elem{s}{i}, \Elem{o}{i}).$$
After the starting sequence $\Vector{s}$ has been processed by $q_{\EncParam}$, the control flow passes onto $p_{\DecParam}$. The function computed by $p_{\DecParam}$ is similar to the one implemented by the first \gls{rnn}. Specifically, it is the following:
\begin{align*}
    \Elem{h'}{i} &= \Op{GRU}_{\DecParam}(\Elem{h'}{i-1}, \Matrix{E}^{\Transpose}\Elem{s}{i})\\
    \Elem{o'}{i} &= \softmax(\Matrix{V}_{\DecParam}\Elem{h'}{i} + \Vector{b'}),
\end{align*}
where $\Elem{h'}{0} = \Elem{h}{|\tau_S|}$, \ie the second network is initialized with the last hidden state of the first. Moreover, $\Elem{h'}{i} \in \Real^h$ is a hidden state, $\Op{GRU}_{\DecParam}$ is a multi-layer GRU network, $\Matrix{V}_{\DecParam} \in \Real^{h \times (k+2)}$ is an output weight matrix,  $\Vector{b'} \in \Real^{k+2}$ is a bias vector, $\Elem{o}{i}$ is an output distribution over all the possible node IDs, and $\Matrix{E}$ is the same embedding matrix used by $q_{\EncParam}$.  Lastly, the output $\Elem{o'}{i}$ is compared to the ground truth value $\Elem{e}{i}$ using the \gls{ce} loss function, similarly as before. For the whole ending sequence, the computed loss is the following:
$$\Loss(\DecParam, (\Vector{s}, \Vector{e})) = \frac{1}{|\Vector{e}|} \sum_{i=1}^{|\Vector{e}|} \mathrm{CE}(\Elem{e}{i}, \Elem{o'}{i}).$$
The whole network is trained with \gls{mle} by minimizing the following objective function:
$$\argmin_{\Param} \frac{1}{n} \sum_{(\Vector{s},\Vector{e}) \in \Data} \Loss(\EncParam, \Vector{s}) + \Loss(\DecParam, (\Vector{s}, \Vector{e})),$$
where $\Param = (\DecParam, \EncParam)$. Figure \ref{fig:model-training} shows the architecture during training.

\begin{figure*}[h!]
    \centering
    \resizebox{.8\textwidth}{!}{\input{Figures/Chapter6/model-training.tex}}
    \caption{A depiction of the proposed architecture during training. We set $|\tau_S| = |\tau_E| = m$ to avoid cluttering. Notice that the embedding matrix is shared between the two networks.}
    \label{fig:model-training}
\end{figure*}

\subsubsection*{Generation}
To generate an ordered edge sequence, one first generates a starting sequence $\Vector{\tilde{s}}$ from the first network in autoregressive sampling mode. The generative process is started by passing the $\SOS$ token to the network. The categorical distribution corresponding to all the possible Node IDs is sampled at each step, and the resulting node ID is used as input for the next step. The autoregressive sampling is interrupted once the $\EOS$ token is sampled. At this point, the sampled starting sequence $\Vector{\tilde{s}}$ is used as input for the second network, which predicts the ending sequence. This time, the output distribution is not sampled, but the token with the highest probability $\Elem{\hat{e}}{i}$ is chosen at each step. This method is also known as \emph{greedy sampling}, and corresponds to applying the $\argmax$ operator to the distribution vector, which gives the ID of the chosen node. After all elements have been predicted, we end up with a starting sequence $\Vector{\tilde{s}}$, and an ending sequence $\Vector{\hat{e}}$. The two sequences are paired together to obtain the ordered edge sequence of the generated graph. Figure \ref{fig:model-sampling} shows this process visually.
\begin{figure*}[h!]
    \centering
    \resizebox{.8\textwidth}{!}{\input{Figures/Chapter6/model-sampling.tex}}
    \caption{Graph generation. The output of the network are two sequences: one is a stochastic sequence $\Vector{\tilde{s}}$ representing the starting sequence of the graph. The other is a an ending sequence $\Vector{\hat{e}}$, predicted using $\Vector{\tilde{s}}$ as input. The two sequences are ultimately combined to produce the ordered edge sequence of a novel graph.}
    \label{fig:model-sampling}
\end{figure*}

\subsubsection*{Implementation details}\label{sec:implementation}
The model is implemented using the \texttt{PyTorch}\footnote{\url{https://github.com/marcopodda/grapher}} \cite{paszke2017pytorch} library; after an initial exploratory phase (not reported), some hyper-parameters (such as the number of recurrent layers) were fixed in advance. We used model selection with a 80\%-10\% splits to select the other hyper-parameters. We choose among 32 and 64 for the embedding dimension $d$, 128 and 256 for the recurrent state size $h$. we applied a dropout layer to the embedding layer, whose rate $p_{\mathrm{keep}}$ was chosen between 0.25 and 0.35. As recurrent cells, we used 2 layers of Gated Recurrent Units (GRU) \cite{cho2014gru}. We used the Adam \cite{kingma2015adam} optimizer with learning rate of 0.001, halved every 200 epochs. We trained all models for a maximum of 2000 epochs, stopping training whenever the loss started to plateau (we found a suitable empirical threshold after running a number of exploratory training instances).

\subsection{Experiments}
Following, we detail the experiments to assess our model, describing the datasets used for learning, the baselines we compared to, and the framework used for evaluation.

\subsubsection*{Datasets}\label{sec:datasets}
In our experiments, we evaluate our model on 5 datasets. We choose a heterogeneous set of graph datasets in order to evaluate whether our approach is general enough to give good performances on very different graph distributions. Concretely, we used the following datasets:

\begin{itemize}
    \item LADDERS, a synthetic dataset of ladder graphs. We created the dataset generating all possible ladder graphs having number of nodes $2N, N = 2, \ldots, 20$, and replicated them 10 times, resulting in a total size of 180 graphs. This dataset is inspired from the grids dataset employed in \citep{you2018graphrnn}. However, we resorted to ladder graphs because they have a similar node degree distribution, while being manageable computationally speaking. An example of ladder graph is shown in Figure \ref{fig:ladder};
    \item COMMUNITY, a synthetic dataset of graphs with two-community structure, \ie graphs composed of two densely connected clusters of nodes, which are weakly connected between themselves. Here, the model has to capture both the strong connectivity inside the cluster, as well as the weaker connectivity between the two clusters. These kind of dependencies are common in biological settings: for example, densely connected communities in metabolic networks often represent functional groups (see \eg \citep{girvan2002commstructsocialbionet}). To generate graphs for this dataset, we firstly generate two clique graphs of random size between $8 \leq N \leq 20$. We then remove some edges randomly from the two clusters with probability $0.4$, and then connect the two communities with 1 or 2 edges at random. The generated dataset is composed of 1000 graphs. This dataset is similar to the COMMUNITY dataset used in \citep{you2018graphrnn}, which unfortunately we could not reproduce. An example of community graph is shown in Figure \ref{fig:community};
    \item EGO, a dataset of ego networks extracted from the Citeseer dataset \citep{giles1998citeseer}. In this case, the dependency to capture is the presence of a focal node (the \quotes{ego} node), which is connected to the majority of nodes in the graph, while the other nodes (the \quotes{alter} nodes) have weaker connectivity. This dependency is observed in social networks, and is usually modeled with the BA model for preferential attachment. In our experiments, we extracted all possible ego networks of radius 2, which means that the path length between the ego node and the alter nodes is at most 2. This resulted in a total size of 1719 graphs. An example of ego graph is shown in Figure \ref{fig:ego};
    \item ENZYMES, a subset of 436 graphs taken from the ENZYMES dataset \citep{schomburg2004enzymes} (see Section \ref{sec:datasets} for details);
    \item PROTEINS, a subset of 794 graphs taken from the Dobson and Doig dataset \citep{dobson2003dd} (see Section \ref{sec:datasets} for details). In these two last datasets, the model should capture patterns of connectivity typical of molecules, such as functional groups or aromatic rings.
\end{itemize}
All datasets have a number of nodes comprised between 4 and 40, and a number of edges comprised between 2 and 130. Before training, we held out a portion of the dataset from testing purposes. In particular, for all datasets except LADDERS, we held out 30\% of graphs, and used them for evaluation. This held-out set acts as a random sample drawn from the corresponding unknown graph distribution.
\begin{figure*}[h!]
    \begin{subfigure}[b]{0.25\linewidth}
        \centering
        \resizebox{.55\textwidth}{!}{\input{Figures/Chapter6/graph-ladder.tex}}
        \caption{Ladder graph.}
        \label{fig:ladder}
    \end{subfigure}
    \begin{subfigure}[b]{0.43\linewidth}
        \centering
        \resizebox{.8\textwidth}{!}{\input{Figures/Chapter6/graph-community.tex}}
        \caption{Community graph.}
        \label{fig:community}
    \end{subfigure}
    \begin{subfigure}[b]{0.30\linewidth}
        \centering
        \resizebox{.8\textwidth}{!}{\input{Figures/Chapter6/graph-ego.tex}}
        \caption{Ego graph.}
        \label{fig:ego}
    \end{subfigure}
    \caption{Three simplified examples of the synthetic graphs from datasets used during the evaluation.}
    \label{fig:synthetic-graphs}
\end{figure*}
For the LADDERS dataset, we held out 10\% of graphs in a stratified fashion, meaning that the held-out set for LADDERS is composed of the same 18 unique ladder graphs found in the training set. To motivate this choice, let us clarify the role of the LADDERS dataset: its purpose is not to evaluate generalization capability, but to show that adaptive models can overfit hard dependencies among nodes, while non-adaptive models cannot. In fact, ladder graph nodes have an almost constant node degree of 3, except for nodes at the corner, which have degree of 2. This dependency is indeed very unlikely to be learned modelling connectivity in a random fashion. Dataset statistics are presented in Table \ref{tab:generation-datasets}.

\input{Tables/Chapter6/generation-datasets.tex}

\subsubsection*{Baselines}
We compare against four baseline models. Two of them are classical generative models of graphs coming from graph theory literature, namely the ER and BA models. The rationale behind this choice is to assess whether our model is able to perform better than random models that do not take into account edge dependencies (ER), or consider just one simple edge dependency (BA, where the dependency is the number of nodes in a graph a new node gets attached to).

The ER model has two parameters: $N$, the number of nodes of the graph to be generated, and $P$, the probability of adding an edge between a pair of nodes. The generative process that they enforce can be described informally as follows: first, pick $N$, then, for each possible pair of nodes, sample a Bernoulli random variable with probability $P$, and connect the two nodes according to the sampled value. We optimize these two parameters picking $N$ among the number of nodes of graphs in the training sample, and choosing $P$ from a grid of possible values. As loss function, we used the earth mover distance between the training graph statistics and the generated graph statistics is minimized.


The BA model has two parameters as well: besides $N$, it has a parameter $M$ that specifies the number of edges to attach from a new node to nodes existing in the current graph. The generative process for a BA graph proceeds incrementally: given an already formed graph, add a new node and connect it (or not) to  a destination node with success probability proportional to its degree. In our experiments, the two parameters of the BA model are optimized in a similar fashion as the ER model.

We compared our model to the GraphRNN model of \citet{you2018graphrnn}. This choice is mainly motivated by the fact that we wanted to compare the performances of our model against a strong, state-of-the-art competitor. We implemented the model according to the original code repository provided by the authors, following their implementation and their hyper-parameters configuration closely.

Lastly, we introduce a third baseline model, a recurrent neural network with GRU cells that is trained to predict the adjacency matrix of the graph, entry by entry. We will call this baseline GRU-B(aseline) from now on. It is arguably the simplest model one can come up with to model graph generation in a recurrent fashion, with the limitation that it has to sample $n(n-1)/2$ entries to fully specify the adjacency matrix of an undirected graph with $n$ nodes, making it prone to learning issues induced by long-term dependencies \citep{bengio1994learninglongtermdependenciesdifficult}. Another drawback of this model is that, since it is trained on adjacency matrices alone, it can hardly learn higher-order motifs. In this sense, it has limited expressiveness with respect to the proposed approach and GraphRNN, which are, at least in principle, capable of learning higher-order connectivity patterns.

\subsubsection*{Evaluation Framework}
We assess our model against the competitors following the quantitative and qualitative approach described in Section \ref{sec:evaluation-generative-graphs}. In a first, quantitative experiment, we evaluate how models are able to keep generating different graphs on large samples. This is a crucial aspect, as generated copies of graphs seen on the training set, as well as multiple copies of the same graphs, are clear signs of overfitting. Moreover, to detect such overfitting, generated samples must be large, as small samples might misleadingly indicate good generalization just as a consequence of a \quotes{lucky} draw. Thus, we generate two samples of size 1000 and 5000 from our model and the baselines, and for each of the two samples we measure the novelty and uniqueness of the generated graphs. Despite using chemical datasets (ENZYMES and PROTEINS) we do not evaluate chemical validity because we are concerned about generating unlabelled graphs; thus, chemical validity cannot be assessed in our case. Finally, we also measure the time that each model takes to generate the 5000 graph sample.

In a second, qualitative experiment, we assess how much samples from the models resemble a random sample from the graph distribution of reference. To do so, we sample from all the models a number of graphs equal in size to the held-out test set, and evaluate the Kullback-Leibler Divergence \gls{kld} between a set of graph statistics collected both on our samples and on the test sample. In this work, we choose to evaluate:
\begin{itemize}
    \item Average Degree Distribution (ADD);
    \item Average Clustering Coefficient (ACC);
    \item Average Orbit Count (AOC).
\end{itemize}
All these metrics, and the evaluation framework, are detailed in Section \ref{sec:evaluation-generative-graphs}. Since each graph can have a different number of nodes, the vectors of statistics for each graph have different lengths; hence, we concatenate each of these vectors further into a unique vector for the whole sample, and we fit an histogram of 100 bins to its values. All measurements are repeated 10 times, each time drawing a novel sample from the models.

\subsection{Results}
Here, we present the results of our experiments. We divide the analysis of the results into a quantitative and qualitative sections for readability purposes.

\subsubsection*{Quantitative Analysis}
Table \ref{tab:graph-quantitative} shows the results of our quantitative experiments. The best performances in terms of novelty and uniqueness are obtained consistently by the ER and BA models; this was expected, since by definition they produce random graphs, hence very likely to be different from the ones in the training set, as well as different with respect to other graphs in the generated sample. One exception is the EGO dataset, where random models score poorly with respect to the competitors. We argue that this result is due to the nature of the EGO dataset, which is composed of graphs with very scarce connectivity (except for the ego nodes) and a very low number of cycles. With such characteristics, it is easier for a random model to produce duplicates or overfit the training sample, just by setting the parameters that regulate connectivity to small values.
\input{Tables/Chapter6/graph-quantitative.tex}
In contrast, our model and GraphRNN consistently generate graphs with high novelty and uniqueness rates in almost all scenarios. The only exception to this trend is the ladders dataset, where both our model and GraphRNN score poorly, while random models score higher. However, we remark that LADDERS is an ill-constructed dataset, as mentioned in Section \ref{sec:datasets}. The main purpose of that particular dataset is not to quantify novelty or uniqueness of the samples, but to show that random models are unable to learn the strong edge dependency of ladder graphs. In a sense, the fact that random models score higher provides evidence  that they are unable to reproduce the connectivity patterns of ladder graphs: higher scores are likely due to the fact that they have fit random connectivity parameters to the training sample, resulting in very heterogeneous graphs. For further evidence, we refer the reader to the qualitative analysis, where results show clearly that random models generate low quality ladder graphs.

The GRU-B model greatly under-performs in all but the COMMUNITY dataset. One could legitimately infer that the model is overfitting and has just memorized training adjacency matrices, instead of more general connectivity paths; however, both from the qualitative analysis and by observing that the training loss (not shown) for the model has plateaued at a high loss, it is safe to affirm that the model cannot perform any better. This provides evidence that a simple recurrent model such as GRU-B, at least in this form, is not well suited for the task of graph generation.

Our model performs close to a 1.0 rate in novelty and uniqueness in every dataset except LADDERS (for reasons explained above), with the notable mention of the PROTEINS dataset, where it obtains the best performances in every scored metric with a margin of 0.02 to 0.06 points with respect to the GraphRNN model. On the other hand, the GraphRNN model obtains the best scores in the EGO dataset, scoring 1.0 in every considered metric, beating our model with a margin of 0.3 to 0.1 points. However, our model is the most consistent across all the datsets, performing over 0.87 in all considered scores in every dataset except LADDERS.

Note how all models generate novel and unique graphs in the COMMUNITY dataset, with a rate of 1.0 or very close. This can be explained by considering the nature of the COMMUNITY dataset, whose graphs are essentially composed by two random graphs weakly connected among each other. Thus, since generating a graph from that distribution is very similar to generating one at random, samples are highly likely to be different from each other.

As regards sampling time, we note that random models are the fastest during generation; this was expected as well, since they have only 2 parameters to sample from, while all RNN-based models have a larger number of parameters. Among the RNN-based models, our model is the fastest at generating new samples, the only exception being the COMMUNITY dataset, where however it elapses only 1 minute more than the winning model, GRU-B. In contrast, the GraphRNN model has sampling times 5 to 20 times slower than our model. For completeness, however, we report that while we sampled from the GraphRNN model with batch size of 1 to achieve a fair comparison, its implementation allows to draw samples in batches, greatly speeding up the sampling process.

Table \ref{tab:graph-quantitative-rank} shows the mean rank of the models for each considered metric (except time), averaged over all datasets. More precisely, given one metric, we collect the scores of all models on that particular metric, sort the corresponding vector in descending order from highest to lowest score, and assign as rank the index associated to the position of the models performance in the sorted vector. The ranks are finally averaged over all datasets, to provide a measure of how models behave globally. Results show that the BA model has the highest mean rank as regards novelty on a sample size of 1000 (for the reasons discussed above), while GraphRNN has the highest mean rank as regards novelty on a sample of 5000. Our model obtains the highest mean rank on uniqueness, on both sample sizes, and also on novelty for a sample size of 1000 among RNN-based models.
\input{Tables/Chapter6/graph-quantitative-rank.tex}

\subsubsection*{Qualitative Analysis}
Table \ref{tab:graph-qualitative} shows the results of the qualitative evaluation of the models.
It can be clearly seen how random models are not able to learn complex dependencies, scoring poorly on all datasets in every considered metric, in contrast with the RNN-based models. One exception is the EGO dataset, where both the ER and BA model obtain the best KLD for the AOC metric (orbit counts) with 0.07 and 0.09, respectively (standard deviation intervals overlap). This good result is in accordance with the argument exposed in the quantitative analysis, where we explained that random models have likely fitted the dataset with very small values of the parameters that regulate connectivity. Indeed, ego graphs have usually a very low higher-order connectivity, hence generating graphs with weak connectivity results in capturing the correct orbit counts distribution. As regards the LADDERS dataset, the BA model obtains the best ACC (clustering coefficient), scoring $0$ in all the repeated experiments. Again, a similar argument can be formulated to explain such good performance: the clustering coefficient of ladder graph nodes, which is 0 by construction, can be easily captured by random models by generating very weakly connected graphs. However, note how the BA model completely fails at generating graphs with the correct node degree distribution (most likely generating graphs with degree of 1 in most nodes), scoring the worst among all datasets. The same argument applies to the ER model, providing evidence that random models are only able to capture a limited set of graph properties, sacrificing many others.
\input{Tables/Chapter6/graph-qualitative.tex}
Results also highlight how the GRU-B model is not able to generate good quality graphs, and especially fails at capturing orbit count distributions: this poor performance can be motivated by the fact that higher-order dependencies are hardly learnable using the adjacency matrix alone. Note that this performance confirms the argument stated in the quantitative analysis, where we affirm that poor results on quantitative metrics are not a consequence of overfitting (otherwise the model would have scored higher simply by replicating graphs found in the training set), but of a limited expressiveness instead.

Among all models, GraphRNN and ours perform consistently at the state of the art as regards the quality of generated graphs. In most datasets, they perform indistinguishably, meaning that the interval spanned by the standard deviations on the metric overlap. In one case, namely the EGO dataset, our model outperforms GraphRNN in all considered metrics. However, for fairness, we remind that results on the EGO dataset are probably slightly biased in favor of our model with respect to GraphRNN, because samples drawn from our model have a lower uniqueness and novelty rate, as per the quantitative analysis. As such, performances might be biased from repetitions of "good" graphs. While we did not investigate this issue any further, we also note that the margin by which our model outperforms GraphRNN is the highest among all datasets, with GraphRNN trailing by margins from 0.1 to 0.3 points. In contrast, note that whenever our model performs worse than GraphRNN, margins are instead noticeably narrower (the largest being 0.05 in the ENZYMES dataset on the AOC metric).

Global performances of the models across all datasets are summarized in Table \ref{tab:graph-qualitative-rank}, where the mean rank of all models is computed in a similar fashion as reported in the quantitative analysis. Our model records the highest mean ranking in all considered qualitative metrics, confirming once again that our approach is able to approximate structural feature distributions of very heterogeneous graphs with high accuracy.
\input{Tables/Chapter6/graph-qualitative-rank.tex}
To strengthen this concept, in Figure \ref{fig:distributions} we plot the empirical distribution of the considered metrics obtained on the test sample, versus samples generated by our model in every considered dataset. The plots show that, even in cases where the test empirical distribution is skewed or multimodal, the distribution of structural features of the generated samples matches with good precision all the relevant test distribution features, such as long tails or peaks.
\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{Figures/Chapter6/distributions.eps}
\caption{Plot of the three qualitative statistics of test samples compared to samples generated by our model. Metrics are displayed by column, dataset by rows. The distribution of the test sample is shown in blue, while the distribution of samples drawn from our model is shown in orange. Scales are omitted since they are normalized, hence not informative.}
\label{fig:distributions}
\end{figure}
Finally, in Figure \ref{fig:samples} we show graphs drawn from our model against real graphs drawn from three out of five datasets. Indeed, visual inspection confirms that generated graphs display connectivity patterns detectable in real graphs, for example ego nodes in ego graphs, cliques in protein graphs, and densely connected clusters in the Community dataset.
\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{Figures/Chapter6/samples.eps}
\caption{Five randomly chosen graphs, both from the training sample (left) and generated by our model (right), on each considered dataset (displayed by row). Notice how our generative model has picked up all the characterizing connectivity patterns.}
\label{fig:samples}
\end{figure}

\subsubsection*{Effect of Node Ordering}
As we mentioned earlier, our model is strongly dependent on the type of ordering that is superimposed on graph nodes before training. To this end, we compared our model with 5 variants trained with different ordering strategies. Choices include:

\begin{itemize}
    \item a complete random strategy, which consist in randomly shuffling the node IDs before training, for each graph in the training set. This strategy is expected to perform very poorly, since the model does not see consistent connectivity patterns during training;
    \item the node ordering strategy proposed by \citet{you2018graphrnn}, which we term BF random. Such strategy is very similar to ours (in fact, our strategy is inspired by this), but differs in the fact that the starting node for the breadth-first visit of the graph is selected at random at each epoch. This in principle means that the model is trained on every possible node permutation induced by a breadth-first visit of the graph. On one hand this strategy acts as a regularizer, since at each epoch the training set changes completely. On the other hand, however, changing the node ordering at each epoch could in principle prevent our model from focusing on useful connectivity patterns, simply because they are "masked" away by different node permutations;
    \item a variant of the above mentioned BF random strategy which uses Depth-First Search (DFS) instead of BFS;
    \item only for the Enzymes and Protein datasets, a node ordering strategy induced by the SMILES encoding of the corresponding molecular graph; that is, the order of nodes is given by the position of the corresponding atom label in the SMILES string associated to the molecular graph. Notice that, even though this strategy can in principle produce canonical orderings, it is directly applicable only to molecular graphs who are annotated with atom (node) and bond (edge) labels;
    \item a variant of our proposed strategy which based on DFS.
\end{itemize}

Our first experiment consists in training the models with the alternative node ordering strategies with the same architecture as the proposed model, but trained for a large number of epochs without dropout; in short, trying to overfit the dataset on purpose. This choice is motivated by the fact that we wanted to assess whether the alternative node ordering strategies are able to memorize connectivity patterns (hence, given proper regularization, are able to generalize to unseen connectivity patterns). In Figure \ref{fig:loss}, we plot the loss obtained by the models, for each considered node ordering strategy, for the Protein dataset. The figure shows that the models trained with our strategy, the variant of our strategy based on DFS, and the SMILES ordering are able to reach a lower loss than the alternative node ordering strategies, which in contrast plateau at higher loss values. This provides evidence that our choice of node ordering strategy is well suited for our model, being sufficiently general to learn connectivity patterns from very different graph datasets. Note that the good results of the SMILES ordering was expected since our node ordering procedure resembles, although is not completely identical, to the way atoms (nodes) are ordered when deriving the SMILES string associated to the molecular graph. However, we also remark that such ordering cannot be applied in general, but it is only suitable for chemical data.

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{Figures/Chapter6/loss.eps}
\caption{Plot of the loss on dataset Protein of variants of our approach trained under different node orderings. Notice how the models trained with our proposed node ordering (blue), and SMILES (orange) are able to reach a lower training loss in a smaller number of epochs, compared to variants trained with random ordering (red) and BF random ordering (green).}
\label{fig:loss}
\end{figure}

We also conducted a qualitative evaluation similar to the one reported in the qualitative analysis, where we reused the results of our model and compared them to scores obtained on samples drawn from the three studied variants. Results are detailed in Table \ref{tab:graph-ordering-qualitative}, clearly showing that the model trained with our node ordering strategy is found to be superior in every chosen metric with respect to all assessed variants. The only competitive node ordering strategy is DF, which basically differs from ours in the way graph nodes are visited (DF instead of BF). However, despite being able to match the performances of our node ordering strategy in many cases, the table highlights that it may have problems in approximating structural characteristics in some dataset instances, for example the clustering coefficient in the Community dataset and the orbit count in the Ego dataset. This might be a consequence of the nature of DFS, which does not focus on local patterns first as BF does, but instead explores the graph going through sequential paths, followed by backtracking to the starting node, an approach that is probably harder to train on such datasets. In conclusion, these results further extend the evidences supporting the robustness of our approach.
\input{Tables/Chapter6/graph-ordering-qualitative.tex}


\subsection{Conclusions}
In this work, we have presented a novel generative model for graphs based on Recurrent Neural Networks, which is capable of generating high quality graphs from very different graph distributions. The key idea at the root of our work is to move from the domain of graphs to the one of sequences to simplify the learning task, while retaining most of the expressiveness. Our motivation to frame the generative process as learning problem on sequences is three-fold: (i) it is intuitive, (ii) it allows to work indirectly on smaller and informative portions of the exponentially large graph space, and (iii) it enables the use of the reliable and heavily optimized machinery of Recurrent Neural Networks to learn effectively.

With these ideas in mind, we developed a model which, first, orders nodes in a graph sequentially, then converts graphs into ordered sequences of edges, and finally learns these sequences using two RNNs. The first one predicts a sequence of source nodes, while the second uses the output of the first to predict all necessary endpoints to construct a novel graph. We tested our model against canonical random models from graph theory, a RNN baseline with limited expressiveness, and a state-of-the-art model on five different datasets. The evaluation was conducted on multiple levels, considering both quantitative performance measures such as novelty and uniqueness of generated graphs, as well as qualitative metrics to assess whether generated samples retain structural features of those in the training set. The results clearly show that our model performs at, and sometimes surpasses, the state of the art in the task.
We also conducted a study of how much the procedure chosen to superimpose an order on graph nodes is effective for the particular task of graph generation. More in detail, we compared with 5 variants of our model that were trained on different node orderings. Results show that with the ordering procedure of choice, the model can reach a lower training error, which is essential to learn the complex patterns needed to generate good quality graphs that resemble real-world ones.

The proposed model has also limitations. Even if it works empirically, the constraint imposed by the reliance on a specific ordering is unsatisfactory from a theoretical point of view. For this reason, one key step in future research is to study the node ordering problem more thoroughly, in order to relax and perhaps remove completely such constraint. Another limitation of our approach in its current form is the fact that it is not formulated to include node and edge labels, which are usually found in large classes of graph datasets such as molecular ones. While on one hand this limits the applicability of our approach on those domains, on the other we are also confident that extending the model to include node and edge labels might lead to improvements in generative tasks that deal with those kinds of graphs. Our belief is supported by the intuition that the ability of our model to well approximate structural features of graph distributions, coupled with a reliable mechanism to generate labels, could in perspective improve generalization, since conditioning the generative process not only on structure, but also on features, could help the model learn general connectivity patterns more reliably. This would also allow to compare our model to current molecular graph generator, and investigate the influence of domain specificity on this task.

As for other research directions, we mention the possibility to extend the model by adopting attention mechanisms \citep{bahdanau2015attention}, instead of conditioning the second network only on the last hidden state of the first. Another easily implementable extension of our approach is to model the generative process using a Variational Auto-Encoder in between the two recurrent neural networks, in order to constrain the latent space where the encoding of the first sequence is mapped to be normally distributed. This would have the advantage of cutting sampling time by approximately half, since the inference phase would only require to sample a point from latent space, and decode it using the second network. Lastly, a challenge for the near future is to test whether our approach is capable to scale to larger graphs, which would strengthen our positive results further, as well as expand its applicability to a broader class of problems.

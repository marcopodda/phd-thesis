\chapter{A Model for Edge-Based Graph Generation} % Main chapter title
\label{ch:deep-generative-learning-graphs}

\section{Introduction}
In this section, we present an original contribution. Specifically, we introduce a novel generative model for graphs, capable of generating unattributed graphs coming from very different graph distributions. We transform graphs into sequences of ordered edges, from which we extract two sequences derived from the edge endpoints. We use a model composed of two \glspl{rnn} to learn the probability distribution of such sequences: the first is an autoregressive network which generates a specification of the graph to produce, which is completed into a graph by the second network. We experiment extensively with the proposed model, comparing its performances with a pool of baselines, one of which is a \gls{dgm} of graphs that holds state-of-the-art performances at the generative task. The experimental framework has been designed to evaluate the proposed model on concerning both quantitative and qualitative aspects, as discussed in Section \ref{sec:evaluation-generative-graphs}. Our experiments demonstrate that, under our evaluation framework, the proposed model is able to perform at, and sometimes surpass, the state-of-the-art in the task of generating graphs coming from very different distributions. Furthermore, we study the effect of changing the order of the edge sequence by experimenting with different node orderings. We show that the chosen node ordering strategy  is more effective for learning complex dependencies than the alternatives, and produces graphs of better quality.

\section{Methods}
In this section, we present the methodologies used to develop the model. In particular, we formally introduce the concept of ordered edge sequences, we develop the model,and  we show how it is trained and how graph generation is achieved.

\subsection{Ordered Edge Sequences}
Let $\Graph{g} = \Tuple{\Nodes{g}, \Edges{g}}$ be a fully connected unattributed graph with $n$ nodes and $m$ edges. We assume $\Graph{g}$ is undirected for simplicity, without loss of generality. Let $\gamma: \Nodes{g} \shortrightarrow \Natural_{+}$ be a bijective node labelling function which assigns a unique positive integer (which we call node ID) to each node in the graph; thus, $\gamma$ defines a total order over the nodes of $\Graph{g}$. The \emph{ordered edge sequence} $\Cal{S}$ of graph $\Graph{g}$ is the sequence of pairs:
$$\OES{S}_{\Graph{g}} = ((s_1, e_1), (s_2, e_2), \ldots, (s_m, e_m)),$$
where $(\gamma^{-1}(s_i), \gamma^{-1}(e_i)) \in \Edges{g}$, and such that it is ordered lexicographically according to the IDs assigned to the nodes, \ie $(s_i,e_i) \leq (s_j,e_j)$ if and only if $s_i < s_j$, or $s_i = s_j$ and $e_i \leq e_j$. Given a generic pair $(s_i,e_i) \in \OES{S}_{\Graph{g}}$, we call $s_i \in \Natural_{+}$ its \emph{starting node} and $e_i \in \Natural_{+}$ its \emph{ending node}. Finally, let us define the \emph{starting sequence} $\Start{\OES{S}_{\Graph{g}}} = (s_1, s_2, \ldots, s_m)$, the sequence corresponding of starting nodes ordered as in $\OES{S}$, and analogously, the \emph{ending sequence} $\End{\OES{S}_{\Graph{g}}} = (e_1, e_2, \ldots, e_m)$, corresponding to the ending nodes ordered as in $\OES{S}$. For conciseness, let us omit the dependence of $\OES{S}_{\Graph{g}}$ on the graph $\Graph{g}$, and of the starting and ending sequences from $\OES{S}_{\Graph{g}}$ whenever they are clear from the context. Clearly, the choice of the labelling function $\gamma$ is critical in determining the ordered sequence of a graph. Given the graph $\Graph{g}$, we choose to implement $\gamma$ with the following algorithm:
\begin{itemize}
    \item first, select a node $v_1$ at random from its set of nodes $\Nodes{g}$, and set its node ID as $\gamma(v_1) = \SF{1}$;
    \item then, traverse the graph in breadth-first order. Let $V = (v_2, v_3, \ldots, v_{n})$ be the ordered sequence of nodes visited during the traversal, excluding $v_1$. Assign node ID $\gamma(v_i) = \SF{i},\, \forall v_i \in V,\, i=2, \ldots, n$.
\end{itemize}
Assuming graph $\Graph{g}$ has the structure shown in Figure \ref{fig:example-graph}, and that node $v_1$ is chosen as the root node for the visit, an example of how the graph nodes are labeled by $\gamma$ is shown in Figure \ref{fig:labeled-graph}. Notice that $\gamma$ is trivially bijective, since it assigns a different integer to each node. Once the nodes are labeled, the ordered edge sequence of $\Graph{g}$ is $\OES{S} = (\OElem{1}{2},\OElem{1}{3},\OElem{1}{4},\OElem{3}{4},\OElem{3}{5})$, with $\tau_S = (\SF{1},\SF{1},\SF{1},\SF{3},\SF{3})$ and $\tau_E = (\SF{2},\SF{3},\SF{4},\SF{4},\SF{5})$. Notice that the graph can be readily reconstructed from its orderd edge sequence by first applying the inverse function $\gamma^{-1}$ to each element of its pairs to obtain $\Edges{g}$, which in turn gives $\Nodes{g}$ since we assumed that $\Graph{g}$ is fully connected.

\begin{figure*}[h!]
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \resizebox{.8\textwidth}{!}{\input{Figures/Chapter6/example-graph.tex}}
        \caption{}
        \label{fig:example-graph}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \resizebox{.8\textwidth}{!}{\input{Figures/Chapter6/labeled-graph.tex}}
        \caption{}
        \label{fig:labeled-graph}
    \end{subfigure}
    \caption{({\scriptsize A}): an example graph, where the starting node $v_1$ of the labelling algorithm is marked with a thicker border. ({\scriptsize B}): the same graph, where nodes are labeled according to a breadth-first visit of the graph rooted at $v_1$.}
    \label{fig:labelling-example}
\end{figure*}

\subsection{Model}
Our goal is to model $p(\OES{S})$, the probability of ordered edge sequences, using a dataset $\Data = \Set{\OES{S}_{(i)}}_{i=1}^n$. Our key observation is that any ordered edge sequence $\OES{S}$ is uniquely defined by its starting and ending sequences. Therefore, instead of working on the ordered edge sequence directly, we work on their starting and ending sequences. Specifically, we model the probability of sampling $\OES{S}$ from $p(\OES{S})$ as follows:
$$p(\OES{S}) = p(\tau_S, \tau_E) = p(\tau_E \given \tau_S)\, q(\tau_S) = \prod_{i=1}^{|\OES{S}|} p(e_i \given s_i)\, \prod_{j=1}^{|\OES{S}|} q(s_j \given s_{[<j]}).$$
Intuitively, the generative process specified by the distribution is the following:
\begin{itemize}
    \item first, one samples $\tau_S$ from a prior probability $q(\tau_S)$, which is modeled autoregressively;
    \item once $\tau_S$ is available, it is used to condition the prediction of the ending sequence $\tau_E$.
\end{itemize}
Once both $\tau_S$ and $\tau_E$ are available, the ordered edge sequence (and thus, the corresponding graph) an be reconstructed simply pairing their elements. Here, we make an important remark. Notice that the the probability of $q(\tau_S)$ is modeled autoregressively, while the conditional $p(\tau_E \given \tau_S)$ is not. This means that to generate a sequence, the only stochastic part in the model relates to $q(\tau_S)$, while predicting $\tau_E$ is deterministic once $\tau_S$ is known. One possible interpretation of $q(\tau_S)$ is that it acts as a \quotes{soft prior} of the structure of the graph: in fact, it specifies a subset of relevant nodes for the generation and their degree, expressed as the number of times they appear in the starting sequence. The conditional $p(\tau_E \given \tau_S)$ uses this information to \quotes{complete} the graph based on the information provided by the prior.

\subsection{Training}
The model specified above is implemented as two \glspl{rnn} in cascade, which we refer to as $q_{\EncParam}$ and $p_{\DecParam}$ respectively. The two sequences (starting and ending) are first encoded as sequences of $(k+2)$-dimensional one-hot vectors, where $k$ is the largest node ID assigned by $\gamma$ to any node in the dataset, and $+2$ is added for the start of sequence and end of sequence tokens. We indicate the one-hot encoded starting sequence with the notation $\Vector{s} = (\Elem{s}{1}, \ldots, \Elem{s}{|\tau_S|}, \EOS)$, where $\Elem{s}{i} \in \Real^{k+2}$, and analogously, we use $\Vector{e} = (\Elem{e}{1}, \ldots, \Elem{e}{|\tau_E|})$ for the one-hot encoded ending sequence, with $\Elem{e}{i} \in \Real^{k+2}$. Notice that an end of sequence token is added to $\Vector{s}$: when it is predicted, the autoregressive process that yields $\Vector{s}$ is interrupted. Our dataset has thus the following form: $\Data = \Set{(\Vector{s}_{(i)}, \Vector{e}_{(i)})}_{i=1}^n$. Focusing on a single training pair $(\Vector{s}, \Vector{e})$, we explain the feed-forward phase of the network. Firstly, the sequence $\Vector{s}$ is processed by $q_{\EncParam}$. Given an element $\Elem{s}{i} \in \Vector{s}$, the per-element output of $q_{\EncParam}$ is obtained as follows:
\begin{align*}
    \Elem{h}{i} &= \Op{GRU}_{\EncParam}(\Elem{h}{i-1}, \Matrix{E}^{\Transpose}\Elem{s}{i-1})\\
    \Elem{o}{i} &= \softmax(\Matrix{V}_{\EncParam}\Elem{h}{i} + \Vector{b}_{\EncParam}).
\end{align*}
In the formula above, $\Matrix{E} \in \Real^{h \times (k+2)}$ is an \emph{embedding matrix} whose entries are learnable, and the dot product selects the column of $\Vector{E}$ corresponding to the embedding of the node ID assigned to $\Elem{s}{i}$. Furthermore, $\Elem{h}{i} \in \Real^h$ is a hidden state, $\Op{GRU}_{\EncParam}$ is a multi-layer GRU network, $\Matrix{V}_{\EncParam} \in \Real^{(k+2)\times h}$ and $\Vector{b}_{\EncParam} \in \Real^{k+2}$ are the weights of the softmax output layer, and $\Elem{o}{i}$ is an output distribution over all the possible node IDs. The process is initialized by setting $\Elem{s}{0} = \SOS$ and $\Elem{h}{0} = \Zeros$. We use teacher forcing, meaning that, at each step, the input of the network is the ground truth value taken from $\Vector{s}$, rather than the output predicted by the network. Lastly, the output $\Elem{o}{i}$ is compared to the ground truth value $\Elem{s}{i}$ using the \gls{ce} loss function. For the whole starting sequence, the computed loss is the following:
$$\Loss(\EncParam, \Vector{s}) = \frac{1}{|\Vector{s}|+1} \sum_{i=1}^{|\Vector{s}|+1} q_{\EncParam}(\Elem{s}{i} \given \Elem{s}{<i}) = \frac{1}{|\Vector{s}|+1} \sum_{i=1}^{|\Vector{s}|+1} \mathrm{CE}(\Elem{s}{i}, \Elem{o}{i}).$$
After the starting sequence $\Vector{s}$ has been processed by $q_{\EncParam}$, the control flow passes onto $p_{\DecParam}$. The function computed by $p_{\DecParam}$ is similar to the one implemented by the first \gls{rnn}. Specifically, it is the following:
\begin{align*}
    \Elem{h'}{i} &= \Op{GRU}_{\DecParam}(\Elem{h'}{i-1}, \Matrix{E}^{\Transpose}\Elem{s}{i})\\
    \Elem{o'}{i} &= \softmax(\Matrix{V}_{\DecParam}\Elem{h'}{i} + \Vector{b'}_{\DecParam}),
\end{align*}
where $\Elem{h'}{0} = \Elem{h}{|\tau_S|}$, \ie the second network is initialized with the last hidden state of the first. Moreover, $\Elem{h'}{i} \in \Real^h$ is a hidden state, $\Op{GRU}_{\DecParam}$ is a multi-layer GRU network, $\Matrix{V}_{\DecParam} \in \Real^{(k+2) \times h}$ and $\Vector{b'}_{\Param} \in \Real^{k+2}$ are the weights of the softmax output layer, $\Elem{o}{i}$ is an output distribution over all the possible node IDs, and $\Matrix{E}$ is the same embedding matrix used by $q_{\EncParam}$.  Lastly, the output $\Elem{o'}{i}$ is compared to the ground truth value $\Elem{e}{i}$ using the \gls{ce} loss function, similarly as before. For the whole ending sequence, the computed loss is the following:
$$\Loss(\DecParam, (\Vector{s}, \Vector{e})) = \frac{1}{|\Vector{e}|} \sum_{i=1}^{|\Vector{e}|} - \log p_{\DecParam}(\Elem{e}{i} \given \Elem{e}{i-1}) = \frac{1}{|\Vector{e}|} \sum_{i=1}^{|\Vector{e}|} \mathrm{CE}(\Elem{e}{i}, \Elem{o'}{i}).$$
The whole network is trained with \gls{mle} by minimizing the following objective function:
$$\argmin_{\Param} \frac{1}{n} \sum_{(\Vector{s},\Vector{e}) \in \Data} \Loss(\EncParam, \Vector{s}) + \Loss(\DecParam, (\Vector{s}, \Vector{e})),$$
where $\Param = (\DecParam, \EncParam)$. Figure \ref{fig:model-training} shows the architecture during training.

\begin{figure*}[h!]
    \centering
    \resizebox{.8\textwidth}{!}{\input{Figures/Chapter6/model-training.tex}}
    \caption{A depiction of the proposed architecture during training. We set $|\tau_S| = |\tau_E| = m$ to avoid cluttering. Notice that the embedding matrix is shared between the two networks.}
    \label{fig:model-training}
\end{figure*}

\subsection{Generation}
To generate an ordered edge sequence, one first generates a starting sequence $\Vector{\tilde{s}}$ from the first network in autoregressive sampling mode. The generative process is started by passing the $\SOS$ token to the network. The categorical distribution corresponding to all the possible Node IDs is sampled at each step, and the resulting node ID is used as input for the next step. The autoregressive sampling is interrupted once the $\EOS$ token is sampled. At this point, the sampled starting sequence $\Vector{\tilde{s}}$ is used as input for the second network, which predicts the ending sequence. This time, the output distribution is not sampled, but the token with the highest probability $\Elem{\hat{e}}{i}$ is chosen at each step. This method is also known as \emph{greedy sampling}, and corresponds to applying the $\argmax$ operator to the distribution vector, which gives the ID of the chosen node. After all elements have been predicted, we end up with a starting sequence $\Vector{\tilde{s}}$, and an ending sequence $\Vector{\hat{e}}$. The two sequences are paired together to obtain the ordered edge sequence of the generated graph. Figure \ref{fig:model-sampling} shows the generative process visually.
\begin{figure*}[h!]
    \centering
    \resizebox{.8\textwidth}{!}{\input{Figures/Chapter6/model-sampling.tex}}
    \caption{Graph generation using the proposed model. The output of the network are two sequences: one is a stochastic sequence $\Vector{\tilde{s}}$ representing the starting sequence of the graph. The other is a deterministic ending sequence $\Vector{\hat{e}}$, predicted using $\Vector{\tilde{s}}$ as input. The two sequences are ultimately combined to produce the ordered edge sequence of a novel graph (not shown).}
    \label{fig:model-sampling}
\end{figure*}

\subsection{Implementation details}\label{sec:implementation}
The model is implemented using the \texttt{PyTorch}\footnote{\url{https://github.com/marcopodda/grapher}} \cite{paszke2017pytorch} library; after an initial exploratory phase (not reported), some hyper-parameters (such as the number of recurrent layers) were fixed in advance. We use model selection with a 80\%-20\% splits to select the other hyper-parameters. We choose among 32 and 64 for the embedding dimension $d$, 128 and 256 for the recurrent state size $h$. We apply a dropout layer to the embedding layer, whose rate is chosen between 0.25 and 0.35. As recurrent cells, we use 2 layers of Gated Recurrent Units (GRU) \cite{cho2014gru}. As regards the learning parameters, we use the Adam \cite{kingma2015adam} optimizer with learning rate of 0.001, halved every 200 epochs. We train all models for a maximum of 2000 epochs, applying early stopping whenever the loss starts to plateau (we found a suitable empirical threshold after running a number of exploratory training instances).

\section{Experiments}
Following, we detail the experiments to assess our model, describing the datasets used for learning, the baselines we compared to, and the framework used for evaluation.

\subsection{Datasets}\label{sec:datasets}
In our experiments, we evaluate our model on 3 synthetic datasets and 2 real-world datasets of molecular graphs. Every synthetic dataset represents graphs with particular node/edge dependencies. The rationale is to assess whether our approach is general enough to give good performances on very different graph distributions. Concretely, we use the following datasets:

\begin{itemize}
    \item LADDERS, a synthetic dataset of ladder graphs. We generate all possible ladder graphs having number of nodes $2n, n = 2, \ldots, 20$, and replicate them 10 times, resulting in a total size of 180 graphs. This dataset is inspired from the grids dataset used in \citep{you2018graphrnn}. In our case, we use ladder graphs because they have a similar node degree distribution, while being computationally manageable. In this dataset, the model has to capture very strong dependencies: the nodes of a ladder graph have degree of 3, except nodes at the four corners, which have degree of two. Any graph that does not respect this dependencies is not a ladder graph. An example of ladder graph is shown in Figure \ref{fig:ladder};
    \item COMMUNITY, a synthetic dataset of graphs with two-community structure. Community graphs are composed of two densely connected clusters of nodes, which are weakly connected between themselves. Here, the model has to capture the community structure. Community dependencies are very common in biological settings: for example, densely connected communities in metabolic networks often represent functional groups (see \eg \citep{girvan2002commstructsocialbionet}). To create this dataset, we firstly generate two clique graphs of random size between $8 \leq N \leq 20$. We then remove some edges randomly from the two clusters with probability $0.4$, and then connect the two communities with 1 or 2 edges at random. The generated dataset is composed of 1000 graphs. This dataset is similar to the COMMUNITY dataset used in \citep{you2018graphrnn}, which unfortunately we could not reproduce. An example of community graph is shown in Figure \ref{fig:community};
    \item EGO, a dataset of ego networks extracted from the Citeseer dataset \citep{giles1998citeseer}. In this case, the model has to capture the presence of a focal node (the \quotes{ego} node), which is connected to the majority of nodes in the graph, while the other nodes (the \quotes{alter} nodes) have weaker connectivity. This dependency is typical of social networks, and can be modeled with the BA model for preferential attachment. To create the dataset, we extract all possible ego networks of radius 2 from the Citeseer dataset. Thus, the path length between the ego node and the alter nodes is at most 2. The total number of graphs in the dataset is 1719. An example of ego graph is shown in Figure \ref{fig:ego};
    \item ENZYMES, a subset of 436 graphs taken from the ENZYMES dataset \citep{schomburg2004enzymes} (see Section \ref{sec:datasets} for details);
    \item PROTEINS, a subset of 794 graphs taken from the Dobson and Doig dataset \citep{dobson2003dd} (see Section \ref{sec:datasets} for details). In these two last datasets, the model should capture patterns of connectivity typical of molecules, such as functional groups or aromatic rings.
\end{itemize}
All datasets have a number of nodes comprised between 4 and 40, and a number of edges comprised between 2 and 130. Before training, we held out a portion of the dataset from testing purposes. In particular, for all datasets except LADDERS, we held out 30\% of graphs, and used them for evaluation. This held-out set acts as a random sample drawn from the corresponding unknown graph distribution.
\begin{figure*}[h!]
    \begin{subfigure}[b]{0.25\linewidth}
        \centering
        \resizebox{.55\textwidth}{!}{\input{Figures/Chapter6/graph-ladder.tex}}
        \caption{Ladder graph.}
        \label{fig:ladder}
    \end{subfigure}
    \begin{subfigure}[b]{0.43\linewidth}
        \centering
        \resizebox{.8\textwidth}{!}{\input{Figures/Chapter6/graph-community.tex}}
        \caption{Community graph.}
        \label{fig:community}
    \end{subfigure}
    \begin{subfigure}[b]{0.30\linewidth}
        \centering
        \resizebox{.8\textwidth}{!}{\input{Figures/Chapter6/graph-ego.tex}}
        \caption{Ego graph.}
        \label{fig:ego}
    \end{subfigure}
    \caption{Three examples of synthetic graphs from the datasets used during the evaluation.}
    \label{fig:synthetic-graphs}
\end{figure*}
For the LADDERS dataset, we held out 10\% of graphs in a stratified fashion. In practice, the held-out set for LADDERS is composed of the same 18 unique ladder graphs found in the training set. To motivate this choice, let us clarify the role of the LADDERS dataset: its purpose is not to evaluate generalization capability, but to show that adaptive models can overfit hard node/edge dependencies among nodes, while non-adaptive models (such as the ER and BA models) cannot. The statistics of the datasets used for evaluation are presented in Table \ref{tab:generation-datasets}.

\input{Tables/Chapter6/generation-datasets.tex}

\subsection{Baselines}
We compare against four baseline models. Two of them are classical generative models of graphs coming from graph theory literature, namely the ER and BA models. The rationale behind this choice is to assess whether our model is able to perform better than random models that model graph connectivity independently (ER), or consider just one simple edge dependency (BA, where the probability of an edge is a function of the node degree).

The ER model has two parameters: $n$, the number of nodes of the graph to be generated, and $P$, the probability of adding an edge between a pair of nodes. The generative process of the ER model can be described informally as follows: first, pick $n$, then, for each possible pair of nodes, sample a Bernoulli random variable with probability $P$, and connect the two nodes according to the sampled value. We sample $n$ from the empirical distribution of the number of nodes in the datasets, and choose $P$ from a grid of values. The best value of $P$ is obtained by minimizing the earth mover distance between the empirical distributionand the generated distribution of graph properties

Similarly, the BA model has two parameters: $n$, the number of nodes of the graph to be generated, and $M$, a maximum number of nodes a node can be connected to. The generative process for a BA graph proceeds incrementally: given an already formed graph, add a new node and connect it (or not) to at most $M$ nodes with probability proportional to the nodes degree. In our experiments, the two parameters of the BA model are optimized in a similar fashion as the ER model.

Besides graph theory baslines, we also compare to a strong Deep-Learning based generative baseline. In particular, we choose the GraphRNN model of \citet{you2018graphrnn}, which holds state-of-the-art performances in the graph generation task. We implemented the model according to the original code repository provided by the authors, following their implementation and their hyper-parameters configuration closely.

Lastly, we introduce a third baseline model, a recurrent neural network with GRU cells that is trained to predict the adjacency matrix of the graph one entry at a time. We call this baseline GRU-B(aseline) from now on. It is arguably the simplest model one can come up with to model graph generation in a recurrent fashion, with the limitation that it has to sample $n(n-1)/2$ entries to fully specify the adjacency matrix of an undirected graph with $n$ nodes, making it susceptible to learning issues induced by long-term dependencies \citep{bengio1994learninglongtermdependenciesdifficult}. Clearly, even though Deep-Learning based, this baseline has purposedly limited expressiveness with respect of our model and GraphRNN.

\subsection{Evaluation Framework}
We assess our model against the baselines following the quantitative and qualitative evaluation principles described in Section \ref{sec:evaluation-generative-graphs}. The experiments are described as follows:
\begin{itemize}
    \item our first experiment consists in evaluating the model quantitatively. To be sure to detect overfitting, we generate large samples. Specifically, we generate two samples of size 1000 and 5000 from all the candidates. For each of sample, we measure the novelty and uniqueness of the generated graphs. Despite using chemical datasets (ENZYMES and PROTEINS), we do not evaluate chemical validity because we are concerned about generating unlabeled graphs; thus, chemical validity cannot be assessed in our case. Finally, we also measure the time that each model takes to generate the 5000 graph sample;
    \item the second experiment is of qualitative nature. In practice, we assess how much the generated samples resemble a random sample from the graph distribution of reference. To do so, we first generate from all the models a sample of equal size to the held-out test set. Then, on each sample, we evaluate the \gls{kld} on a set of graph statistics, collected in the generated sample and in the test sample. Specifically, we evaluate Average Degree Distribution (ADD), Average Clustering Coefficient (ACC), and Average Orbit Count (AOC). Note that, since each graph can have a different number of nodes, the vectors of statistics for each graph have different lengths in general. Therefore, for each of the two samples, we concatenate the graph statistics vectors into a unique vector, on which we fit an histogram of 100 bins. The \gls{kld} is calculated on the two resulting histograms. This process is repeated 10 times, each time drawing a novel sample from the models.
\end{itemize}
Lastly, we provide a study on how using different node orderings affects performances. To do so, we compare our proposed model with variants where graphs are preprocessed used different node orderings. The five analyzed variants are:
\begin{itemize}
    \item \emph{Random}: the node ordering is a random permutation of the nodes. This strategy is expected to perform very poorly, since the model does not see consistent connectivity patterns during training;
    \item \emph{BF Random}: the node ordering strategy proposed by \citet{you2018graphrnn}. Such strategy is very similar to ours (in fact, our strategy is inspired by it), but differs in that, each time a graph is picked from the training set at different epochs, the root node for the visit can be different. In principle, this means that the model is trained on every possible node permutation induced by a breadth-first (BF) visit of the graph. On one hand, this strategy acts as a regularizer, since at each epoch the training set changes completely. On the other hand, however, changing the node ordering at each epoch could in principle prevent our model from focusing on useful connectivity patterns, simply because they are "masked" away by different node permutations;
    \item \emph{DF Random}: the same strategy as BF random, but using depth-first (DF) visit instead;
    \item \emph{DF}: a variant of our proposed strategy which uses depth-first traversal instead of breadth-first;
    \item \emph{SMILES}: the node ordering imposed by the SMILES encoding of the graph. This strategy is available only in the PROTEINS and ENZYMES datasets, which consist of molecular graphs;
\end{itemize}
Our first experiment for this analysis consists in training the models with the alternative node ordering strategies for a large number of epochs without regularization. In practice, we try to overfit the dataset on purpose. The idea is to assess whether the alternative node ordering strategies are able to memorize connectivity patterns (hence, given proper regularization, are able to generalize to unseen ones). In our second experiment, we perform a qualitative analysis comparing the performances of our model to the different variants.

\section{Results}
Here, we present the results of our experiments. We divide the analysis of the results into a quantitative and qualitative sections for readability purposes.

\subsection{Quantitative Analysis}
Table \ref{tab:graph-quantitative} shows the results of our quantitative experiments. The best performances in terms of novelty and uniqueness are obtained consistently by the ER and BA models; this was expected, since by definition they produce random graphs, hence very likely to be different from the training set, as well as from the generated sample. One exception is the EGO dataset, where random models score poorly with respect to the competitors. We argue that this result is due to the nature of the EGO dataset, which is composed of graphs with very weak connectivity (except for the ego nodes) and a very low number of cycles. With such characteristics, it is easier for a random model to produce duplicates or overfit the training sample, just by setting the parameters that regulate connectivity to small values.
\input{Tables/Chapter6/graph-quantitative.tex}
In contrast, our model and GraphRNN consistently generate graphs with high novelty and uniqueness rates in almost all scenarios. The only exception to this trend is the ladders dataset, where both our model and GraphRNN overfit (they tend to generate the same graphs over and over), while random models do not. However, we remark the peculiarity of the LADDERS we mentioned in Section \ref{sec:datasets}. The main purpose of that particular dataset is to show that random models are unable to learn the strong edge dependency of ladder graphs. Thus, the analysis of novelty and uniqueness must be put into perspective by the qualitative analysis, to see whether structural dependencies have also been learned.

The GRU-B model greatly under-performs in all but the COMMUNITY dataset. From this result alone, one could legitimately infer that the model is not generalizing to unseen adjacency matrices. However, the qualitative analysis and the high value of the training loss (not shown) suggest that the model cannot perform any better. This provides evidence that a simple recurrent model such as GRU-B, at least in this form, is not well suited for the task of graph generation.

Our model obtainx excellent novelty and uniqueness scores in every dataset except LADDERS (for reasons explained above), with the notable mention of the PROTEINS dataset, where it obtains the best performances in every scored metric with a margin of 0.02 to 0.06 points with respect to the GraphRNN model. On the other hand, the GraphRNN model obtains a perfect score in the EGO dataset, beating our model with a margin of 0.3 to 0.1 points. However, our model is the most consistent across all the datsets, obtaining a score of above 0.87 in all considered scores in every dataset except LADDERS.

Note how all models obtain a perfect score in the COMMUNITY dataset. This can be explained by considering the nature of the COMMUNITY dataset, whose graphs are essentially composed by two random graphs weakly connected among each other. Thus, since generating a graph from that distribution is very similar to generating one at random, samples are highly likely to be different from each other.

As regards sampling time, random models are the fastest during generation; this was expected, since they have only 2 parameters, while all RNN-based models have a larger number of parameters. Among the RNN-based models, our model is the fastest at generating new samples. One exception is the COMMUNITY dataset, where however it elapses only 1.5 minutes more than the winning model, GRU-B. In contrast, the GraphRNN model has sampling times 5 to 20 times slower than our model. For completeness, however, we report that while we sampled from the GraphRNN model with batch size of 1 to achieve a fair comparison, its implementation allows to draw samples in batches, greatly speeding up the sampling process.

Table \ref{tab:graph-quantitative-rank} shows the mean rank of the models for each considered metric (except time), averaged over all datasets. More precisely, for a given metric, we collect the scores of all models on that particular metric, sort the corresponding vector in descending order from highest to lowest score. The order of the vector is the corresponding rank. The ranks are finally averaged across all datasets, to provide a measure of how models behave globally. Results show that the BA model has the highest mean rank as regards novelty on a sample size of 1000 (for the reasons discussed above), while GraphRNN has the highest mean rank as regards novelty on a sample of 5000. Our model obtains the highest mean rank on uniqueness, on both sample sizes.
\input{Tables/Chapter6/graph-quantitative-rank.tex}

\subsection{Qualitative Analysis}
Table \ref{tab:graph-qualitative} shows the results of the qualitative evaluation of the models. We remark that lower values are better.
It can be clearly seen how random models are unable to learn complex dependencies, scoring poorly on all datasets in every considered metric. One exception is the EGO dataset, where both the ER and BA models obtain the best KLD for the AOC metric (orbit counts) with 0.07 and 0.09, respectively (results are indistinguishable since the standard deviation intervals overlap). This result is in accordance with the argument made in the quantitative analysis, where we explained that random models fit the EGO with a small connectivity parameter. Since ego graphs have very low higher-order connectivity, in this case they are able to capture the correct orbit counts distribution. In the LADDERS dataset, the BA model obtains the best ACC (clustering coefficient), scoring $0$ in all experiments. A similar argument can be made in this case: in fact, the clustering coefficient of ladder graphs is 0 by construction. Thus, it can be easily emulated with a very small connectivity parameter value. In contrast, note how the BA model completely fails at generating graphs with the correct node degree distribution, scoring the worst among all datasets. A similar trend happens to the ER model, providing evidence that random models are suited to model only a small subset of graph distributions.
\input{Tables/Chapter6/graph-qualitative.tex}
As regards the GRU-B model, it struggles especially with the orbit count distributions: this is easily explained by the fact that higher-order dependencies cannot be learned using the adjacency matrix alone. This reinforces the argument made with the quantitative analysis, \ie that this model suffers from limited expressiveness.

Among all models, GraphRNN and ours perform consistently at the state of the art as regards the quality of generated graphs. In most cases, they perform indistinguishably. In one case, namely the EGO dataset, our model outperforms GraphRNN in all considered metrics. However, for the sake of fairness, we remind that results on the EGO dataset are probably slightly biased in favor of our model, which obtained lower uniqueness and novelty rates in that case. Thus, performances might be biased from repetitions of "good" graphs. While we did not investigate this issue any further, we also note that the margin by which our model outperforms GraphRNN is the highest among all datasets, with GraphRNN trailing by margins from 0.1 to 0.3 points. In contrast, note that whenever our model performs worse than GraphRNN, margins are instead noticeably narrower (the largest being 0.05 in the ENZYMES dataset on the AOC metric).

The global performances of the models across all datasets are summarized in Table \ref{tab:graph-qualitative-rank}, where the mean rank of all models is computed in a similar fashion as reported in the quantitative analysis. Our model obtains the highest mean ranking in all considered qualitative metrics.
\input{Tables/Chapter6/graph-qualitative-rank.tex}
In Figure \ref{fig:distributions} we compare the empirical distribution of the three metrics on the test sample, versus the same metrics in the generated sample. The plots show that, even in cases where the test empirical distribution is skewed or multimodal, the true and generated distributions match coherently.
\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{Figures/Chapter6/distributions.eps}
\caption{Plot of the three qualitative statistics of test samples compared to samples generated by our model. Metrics are displayed by column, dataset by rows. The distribution of the test sample is shown in blue, while the distribution of samples drawn from our model is shown in orange. Scales are omitted since they are normalized, hence not informative.}
\label{fig:distributions}
\end{figure}
Finally, in Figure \ref{fig:samples} we show graphs drawn from our model against real graphs, on three datasets. Indeed, visual inspection confirms that generated graphs display connectivity patterns typical of real graphs, for example ego nodes in EGO-like graphs, long chains in PROTEINS-like graphs, and dense clusters in the COMMUNITY-like graphs.
\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{Figures/Chapter6/samples.eps}
\caption{Five randomly chosen graphs, both from the training sample (left) and generated by our model (right), on each considered dataset (displayed by row). Notice how our generative model has picked up all the characterizing connectivity patterns.}
\label{fig:samples}
\end{figure}

\subsection{Effect of Node Ordering}
As explained before, our first experiment to assess the effect of the different node ordering is to verify if the dataset can be overfit using different ordering strategies. In Figure \ref{fig:loss}, we plot the loss obtained by every variant in the COMMUNITY, EGO and PROTEINS dataset. The figure shows that the models trained with our strategy, the variant of our strategy based on DFS, and the SMILES ordering are able to reach a lower loss than the alternative strategies, which in contrast plateau at higher loss values. This provides evidence that the chosen node ordering strategy is well coupled with the architecture of for our model, and it is sufficiently general to learn connectivity patterns from very different graph datasets. Note that the results of the SMILES ordering are similar to ours as expected. In fact, our node ordering procedure is similar to the way SMILES orders the atoms in a molecular graph. However, we also remark that the SMILES ordering cannot be applied in general, but it is only suitable for molecular graphs.

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{Figures/Chapter6/loss.eps}
\caption{Plot of the loss on dataset Protein of variants of our approach trained under different node orderings. Notice how the models trained with our proposed node ordering (blue), and SMILES (orange) are able to reach a lower training loss in a smaller number of epochs, compared to variants trained with random ordering (red) and BF random ordering (green).}
\label{fig:loss}
\end{figure}
We complement the discussion about node ordering with a qualitative analysis, whose results are detailed in Table \ref{tab:graph-ordering-qualitative}. It can be easily noticed that the our node ordering strategy yields superior results in every chosen metric, with respect to every other variant. The only competitive node ordering strategy is DF, which basically differs from ours in the way graph nodes are visited. However, despite being able to this strategy seems to underperform in some cases, for example the clustering coefficient in the COMMUNITY dataset and the orbit count in the EGO dataset. We hypothesize this is due to using DF traversal instead of BF, which is less suited to capture local dependencies. In conclusion, these results further extend the evidences supporting the robustness of our approach.
\begin{landscape}
\input{Tables/Chapter6/graph-ordering-qualitative.tex}
\end{landscape}


\section{Conclusions}
In this work, we have presented a novel generative model for graphs based on Recurrent Neural Networks, which is capable of generating high quality graphs from very different graph distributions. The key idea at the root of our work is to move from the domain of graphs to the one of sequences to simplify the learning task, while retaining most of the expressiveness. Our motivation to frame the generative process as learning problem on sequences is three-fold: (i) it is intuitive, (ii) it allows to work indirectly on smaller and informative portions of the exponentially large graph space, and (iii) it enables the use of the reliable and heavily optimized machinery of Recurrent Neural Networks to learn effectively.

With these ideas in mind, we developed a model which, first, orders nodes in a graph sequentially, then converts graphs into ordered sequences of edges, and finally learns these sequences using two RNNs. The first one predicts a sequence of source nodes, while the second uses the output of the first to predict all necessary endpoints to construct a novel graph. We tested our model against canonical random models from graph theory, a RNN baseline with limited expressiveness, and a state-of-the-art model on five different datasets. The evaluation was conducted on multiple levels, considering both quantitative performance measures such as novelty and uniqueness of generated graphs, as well as qualitative metrics to assess whether generated samples retain structural features of those in the training set. The results clearly show that our model performs at, and sometimes surpasses, the state of the art in the task.
We also conducted a study of how much the procedure chosen to superimpose an order on graph nodes is effective for the particular task of graph generation. More in detail, we compared with 5 variants of our model that were trained on different node orderings. Results show that with the ordering procedure of choice, the model can reach a lower training error, which is essential to learn the complex patterns needed to generate good quality graphs that resemble real-world ones.

The proposed model has also limitations. Even if it works empirically, the constraint imposed by the reliance on a specific ordering is unsatisfactory from a theoretical point of view. For this reason, one key step in future research is to study the node ordering problem more thoroughly, in order to relax and perhaps remove completely such constraint. Another limitation of our approach in its current form is the fact that it is not formulated to include node and edge labels, which are usually found in large classes of graph datasets such as molecular ones. While on one hand this limits the applicability of our approach on those domains, on the other we are also confident that extending the model to include node and edge labels might lead to improvements in generative tasks that deal with those kinds of graphs. Our belief is supported by the intuition that the ability of our model to well approximate structural features of graph distributions, coupled with a reliable mechanism to generate labels, could in perspective improve generalization, since conditioning the generative process not only on structure, but also on features, could help the model learn general connectivity patterns more reliably. This would also allow to compare our model to current molecular graph generator, and investigate the influence of domain specificity on this task.

As for other research directions, we mention the possibility to extend the model by adopting attention mechanisms \citep{bahdanau2015attention}, instead of conditioning the second network only on the last hidden state of the first. Another easily implementable extension of our approach is to model the generative process using a Variational Auto-Encoder in between the two recurrent neural networks, in order to constrain the latent space where the encoding of the first sequence is mapped to be normally distributed. This would have the advantage of cutting sampling time by approximately half, since the inference phase would only require to sample a point from latent space, and decode it using the second network. Lastly, a challenge for the near future is to test whether our approach is capable to scale to larger graphs, which would strengthen our positive results further, as well as expand its applicability to a broader class of problems.

\chapter{Deep Learning in Structured Domains} \label{ch:deep-learning-structures}
A \keyword{structured domain} is a data domain whose elements are formed by a set of atomic \emph{entities}, and the \emph{relations} between them. Structured data is common in several fields, such as biology, chemistry, finance, social networks, and many more. Typical examples are sequences such as time-series data, or graphs representing molecular structures. One distinctive characteristic of structured data is that it has \keyword{variable size}, meaning that the number of entities composing the datum is not fixed in general. This constitutes a serious limitation for traditional \gls{ml} models, which are designed to work with \quotes{flat} data, \ie collections of fixed-size vectors. In principle, they can be adapted to work with variable-sized data by incorporating the structure of the data to the input vectors as additional features. While useful to some extent, this approach requires to decide \apriori which features are needed to solve a task. This, in turn, requires a level of domain expertise that is not always available for many interesting problems. In contrast, \glspl{nn} (and Deep Learning models more so) are able to learn which features are useful to solve a task adaptively from data, without the need of feature engineering. Thus, the general idea is to provide the structured data directly as an input to the network, which automatically learns the needed features and the task, guided by the learning process. In this chapter, we present a class of \glspl{nn} that are able to handle variable-sized inputs for learning in structured domains.

\section{Graphs}\label{sec:graphs}
The elements of structured domains can be described in a compact and convenient notation using the general formalism of \keyword{graphs} \citep{bondy1976graph}. Informally, a graph is a collection of \emph{vertices} (the entities) connected through a collection of \emph{edges} (the relations). In the literature, vertices are sometimes called \emph{nodes}, while edges are also referred to as \emph{arcs} or \emph{links}. Formally, a graph with $n$ vertices is a pair
$$\Graph{g} =\langle \Nodes{g}, \Edges{g}\rangle,$$
where $\Nodes{g}= \Set{v_1, v_2, \ldots, v_n}$ is its set of vertices, and $\Edges{g} = \Set{\{u, v\} \mid u, v \in \Nodes{g}}$ is its set of edges. In a graph, $\Edges{g}$  specifies the graph \emph{structure}, that is, the way vertices are interconnected. Notice that the pair $\Set{u,v}$ is unordered: in this case, the graph is called \keyword{undirected}. Figure \ref{fig:undirected-graph} shows a visual representation of an undirected graph.
Given an edge $\Set{u, v} \in \Edges{g}$, $u$ and $v$ are called its \emph{endpoints}, and are said to be \emph{adjacent}. Alternatively, we say that $\Set{u, v}$ is \emph{incident} to $u$ and $v$. Edges of the form $\Set{v,v}$ that connect a vertex to itself are called \emph{self-loops}.
\begin{figure*}
    \begin{subfigure}[b]{0.38\linewidth}
        \centering
        \resizebox{.9\textwidth}{!}{\input{Figures/Chapter3/01a-undirected-graph.tex}}
        \caption{An undirected graph.}
        \label{fig:undirected-graph}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \resizebox{.9\textwidth}{!}{\input{Figures/Chapter3/01b-directed-graph.tex}}
        \caption{A directed graph.}
        \label{fig:directed-graph}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\linewidth}
        \centering
        \resizebox{.9\textwidth}{!}{\input{Figures/Chapter3/01c-bipartite-graph.tex}}
        \caption{A bipartite graph.}
        \label{fig:bipartite-graph}
    \end{subfigure}
    \caption{Three examples of graphs.}
\end{figure*}
Graphs where it is possible to have more than one edge between a pair of vertices are called \emph{multigraphs}. In this work, we restrict ourselves to the case where there is at most one possible edge between two vertices.

\paragraph{Directed Graphs}
A \keyword{directed graph} is one where he edges are ordered pairs of vertices, or equivalently one where $\Edges{g}\subseteq \Nodes{g}\times \Nodes{g}$. A directed edge is written as $(u, v)$, meaning that it goes from vertex $u$ to vertex $v$. An example of directed graph is shown in Figure \ref{fig:directed-graph}. Given a directed graph $\Graph{g}$ and one of its vertices $v$, the set of all vertices from which an edge reaches $v$ is called \emph{predecessors} set, and is defined as $\Pred(v) = \Set{u \in \Nodes{g} \mid (u,v) \in \Edges{g}}$. The cardinality of the predecessors set is called the \emph{in-degree} of the vertex, and we indicate it as $\Fun{degree}_{in}(v)$. Analogously, the set of all vertices reached by an edge from $v$ is called the \emph{successors} set, and is defined as $\Succ(v) = \Set{u \in \Nodes{g}\mid (v,u) \in \Edges{g}}$. Its cardinality is called the \emph{out-degree} of the vertex, and indicated as $\Fun{degree}_{out}(v)$. The \emph{neighborhood} (or \emph{adjacency set}) of a vertex $v$ is the union of the predecessors and successors sets: $\Neigh(v) = \Pred(v) \bigcup \Succ(v)$. Alternatively, one can view the neighborhood as a function $\Neigh: \Nodes{g}\shortrightarrow 2^{\Nodes{g}}$ from vertices to sets of vertices. The cardinality of the neighborhood is called the \keyword{degree} of the vertex, indicated as $\Fun{degree}(v)$.  In this work, we consider all graphs directed unless otherwise specified. Undirected graphs are thus implicitly transformed into directed graphs with the same vertices, where the set of edges contains the edges $(v,u)$ and $(u,v)$ if and only if $\{u,v\}$ is an edge of the undirected graph.

\paragraph{Bipartite Graphs}
A graph $\Graph{g}$ is called \keyword{bipartite} if we can split $\Nodes{g}$ in two disjoint subsets $\Nodes{g}^{+}$ and $\Nodes{g}^{-}$, such that $(u, v) \in \Edges{g}$ if and only if either $u \in \Nodes{g}^{+}$ and $v \in \Nodes{g}^{-}$, or $v \in \Nodes{g}^{+}$ and $u \in \Nodes{g}^{-}$. Figure \ref{fig:bipartite-graph} shows an example of bipartite graph, where
$\Nodes{g}^{+} = \Set{v_1, v_2, v_3}$ and $\Nodes{g}^{-} = \Set{v_4, v_5, v_6}$.

\paragraph{Walks, Paths, and Cycles}
Let $\Graph{g}$ be a graph. A \emph{walk} of length $l$ is any sequence of $l$ vertices $\Par{v_1, v_2, \ldots, v_l}$, where each pair of consecutive vertices is adjacent, \ie $\Par{v_i, v_{i+1}} \in \Edges{g}$, $\forall i= 1, \ldots, i-1$. A \emph{path} of length $l$ from vertex $u$ to vertex $v$ is a walk such that $v_1 = u$ and $v_l = v$, where each vertex appears exactly once in the sequence. If, given two vertices $u, v \in \Nodes{g}$ such that $u \neq v$, there exists a path between them, we say they are \emph{connected}, or that $v$ is reachable from $u$. Otherwise, we say they are \emph{disconnected}, or that $v$ is unreachable from $u$. A \emph{shortest path} from a node $u$ to a node $v$ is the path, among all paths from $u$ to $v$, with the smallest length. We indicate it with the notation $\Path{u}{g}{v}$. A graph is called \emph{connected} if every vertex is connected to any other vertex (ignoring the direction of the edges); otherwise it is called \emph{disconnected}. A \emph{cycle}, or \emph{loop}, of length $l$ is a walk where $v_1 = v_l$, and all the other vertices appear once in the sequence. Graphs that do not contain cycles are called \emph{acyclic}.

\paragraph{Trees and Sequences}
A graph $\Tree{t}$ is called a \keyword{tree} if its set of edges defines a \emph{partial order} over the set of vertices, implying that it is also connected and acyclic. The vertices of a tree are called \emph{nodes}. Given an edge $(u, v) \in \Edges{t}$, we call $u$ the \emph{parent} of $v$ and $v$ the \emph{child} of $u$. The set of children of a node $v$ is indicated with the notation $\Fun{ch}(v)$. In a tree, every node has exactly one parent, with the exception of a node called \emph{root} or \emph{supersource}, which has no parent node. A tree is \emph{positional} if we can distinguish among the children of a node, \ie if there exist a consistent ordering between them. Trees have a recursive structure: every node $v \in \Nodes{t}$ is itself the root of a tree, called \emph{sub-tree of} $\Tree{t}$ \emph{rooted at v}, and indicated as $\Tree{t}_v$. If $\Tree{t}_v$ contains only $v$, $v$ is called a \emph{leaf}. Trees encode \emph{hierarchical} relationships among nodes; an example of tree with five nodes is shown in Figure \ref{fig:tree}.

A graph $\Seq{s}$ with $n$ vertices is called a \emph{sequential graph}, or \keyword{sequence} of length $n$, if its set of edges defines a \emph{total order} over the set of vertices, which allows us to represent the set of vertices in an ordered fashion as $\Nodes{s} = (v_1, v_2, \ldots, v_n)$. In a sequence, the vertices are usually called \emph{elements}. A sequence can be viewed as a special case of tree with only one leaf. Sequences are useful to encode \emph{sequential} relationships among elements; Figure \ref{fig:sequence} shows an example of a sequence of four elements.
\begin{figure*}
    \begin{subfigure}[b]{0.60\linewidth}
        \centering
        \resizebox{.8\textwidth}{!}{\input{Figures/Chapter3/02a-sequence.tex}}
        \caption{A sequence.}
        \label{fig:sequence}
    \end{subfigure}
    \begin{subfigure}[b]{0.39\linewidth}
        \centering
        \resizebox{.70\textwidth}{!}{\input{Figures/Chapter3/02b-tree.tex}}
        \caption{A tree.}
        \label{fig:tree}
    \end{subfigure}
    \caption{Special classes of graphs.}
\end{figure*}

\paragraph{Subgraphs and Induced Subgraphs} A \keyword{subgraph}
$\Graph{h} = \langle \Nodes{h}, \Edges{h} \rangle$ of a graph $\Graph{g}$ is any graph for which $\Nodes{h} \subseteq \Nodes{g}$ and  $\Edges{h} \subseteq \Edges{g}$. If $\Edges{h} = (\Nodes{h} \times \Nodes{h}) \Inter \Edges{g}$, or equivalently, if $\Nodes{h}$ contains only vertices that are endpoints in $\Edges{g}$, the resulting subgraph is called \keyword{induced subgraph}. We indicate the subgraph of $\Graph{g}$ induced by $\Graph{h}$ with the notation $\InducedSubgraph{g}{\Graph{h}}$.

\subsection{Attributed Graphs} \label{sec:attr-graphs}
Real-world instances of graphs usually carry out other information besides structure, generally attached to their vertices or edges. As an example, consider the graph representation of a molecule, in which vertices are usually annotated with an atom type, and edges are annotated with a chemical bond type. Given a graph $\Graph{g}$ with $n$ vertices and $m$ edges, we define the associated graph with additional information content, and we call it an \keyword{attributed graph}, as a triplet $\langle \Graph{g}, \chi, \xi \rangle,$ where $\chi: \Nodes{g} \rightarrow \Real^d$ is a mapping from the space of vertices to a space of $d$-dimensional \emph{vertex features}, and $\xi: \Edges{g}\rightarrow \Real^{e}$, is a mapping from the space of edges to a space of $e$-dimensional \emph{edge features}. The values of these features can be either discrete (in which case the features are called \emph{labels} and encoded as one-hot vectors) or continuous vectors. In most cases, we omit to define $\chi$ and $\xi$ explicitly, and provide the vertex and edge features directly as sets, \eg $\Vector{x}_{\Graph{g}}= \Set{\Elem{x}{v} \in \Real^d \mid v \in \Nodes{g}}$ for the vertex features, and $\Vector{e}_{\Graph{g}} = \Set{\Elem{e}{u,v} \in \Real^e \mid (u, v) \in \Edges{g}}$ for the edge features. In this case, an attributed graph is a tuple:
$$\AttrGraph{g} = \VectorGraph{x}{g}.$$
If some ordering of the vertices and edges is assumed, we can represent equivalently $\chi$ as a matrix $\Matrix{X}_{\Graph{g}} \in \Real^{n \times d}$ where the $i$-th row contains the vertex features of the $i$-th vertex; analogously, we can define $\xi$ as a matrix of edge features $\Matrix{E}_{\Graph{g}} \in \Real^{m \times e}$. In this case, an attributed graph is a triple:
$$\AttrGraph{g} = \MatrixGraph{x}{g}.$$

\subsection{Isomorphisms, Automorphisms, and Canonization} \label{sec:isomorphisms}
An \keyword{isomorphism} between two graphs $\Graph{g}$ and $\Graph{h}$ is a bijection $\Isomorph: \Nodes{g}\rightarrow \Nodes{h}$ such that $(u,v) \in \Edges{g}$ if and only if $(\Isomorph(u),\Isomorph(v)) \in \Edges{h}$. Intuitively, graph isomorphism formalizes the notion of \emph{structural equivalence} between graphs, in the sense that two isomorphic graphs are structurally equivalent, regardless of the information they contain. Figure \ref{fig:isomorphism} shows two isomorphic graphs and their corresponding $\Isomorph$ bijection. An \keyword{automorphism} $\pi: \Nodes{g}\rightarrow \Nodes{g}$ is an isomorphism between $\Graph{g}$ and itself. Since $\pi$ is essentially a permutation of the vertex set, it follows that a graph always has at most $n!$ possible automorphisms. Intuitively, and similarly to graph isomorphism, graph automorphisms convey the notion that the structure of a graph is invariant to permutation of the vertices and edges. An automorphism $\pi$ on an example graph is shown
in Figure \ref{fig:automorphism}. Related to isomorphisms and automorphisms is the problem of \keyword{graph canonization}, where a canonical ordering (or form) of the graph vertices is sought, such that every graph $\Graph{h}$ isomorph to a given graph $\Graph{g}$ has the same canonical form. As we shall see, (approximate) graph canonization plays a role in the usage of graph within practical contexts; conversely, many techniques described in this work try to avoid representing graphs in canonical form, in favor of permutation-invariant representations.

\begin{figure*}
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \resizebox{.9\textwidth}{!}{\input{Figures/Chapter3/03a-isomorphism.tex}}
        \caption{Isomorphism.}
        \label{fig:isomorphism}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \resizebox{.9\textwidth}{!}{\input{Figures/Chapter3/03b-automorphism.tex}}
        \caption{Automorphism.}
        \label{fig:automorphism}
    \end{subfigure}
    \caption{An example of isomorphism and automorphism.}
\end{figure*}

\subsection{Graphs Matrices} \label{sec:adj-matrix}
One compact way to represent the structure of a graph is through its  \keyword{adjacency matrix}. Given a graph $\Graph{g}$ with $n$ vertices and $m$ edges, the entries $a_{ij}$ of its corresponding  adjacency matrix $\AdjMatrix{g} \in \Real^{n \times n}$ are defined as follows:
\[
a_{ij} =
    \begin{cases}
        1  & \text{if } (v_i, v_j) \in \Edges{g}\\
        0  & \text{otherwise.}
    \end{cases}
\]
Note that the diagonal entries $a_{ii}$ of the adjacency matrix specify the presence  (or absence) of self-loops. Another interesting property of the adjacency matrix is that  it is symmetric for undirected graphs, which implies $a_{ij} = a_{ji},$ $\forall\, i, j = 1, \ldots, n$. Adjacency matrices make some calculations of graph properties particularly convenient: for example, the in-degree and out-degree of a vertex $v_j \in \Nodes{g}$
can be obtained by performing row-wise and column-wise sums on $\AdjMatrix{g}$:
$$
\Fun{degree}_{in}(v_j) = \sum_{i=1}^n a_{ji} \quad \quad \Fun{degree}_{out}(v_j) = \sum_{i=1}^n a_{ij}.
$$
Adjacency matrices are also useful to understand concepts such as graph automorphisms: in fact, an automorphism of $\Graph{g}$ corresponds to a permutation of the columns or rows of the adjacency matrix (but not both). Other useful matrices to represent properties of graphs are the \emph{Laplacian matrix} $\LaplacianMatrix{g} \in \Real^{n \times n} = \DegreeMatrix{g} - \AdjMatrix{g}$, and the \emph{symmetric normalized Laplacian matrix} $\NormLaplacianMatrix{g} \in \Real^{n \times n} = \Identity - \DegreeMatrix{g}^{-\frac{1}{2}}\AdjMatrix{g}\DegreeMatrix{g}^{-\frac{1}{2}}$. In both definitions, the matrix  $\DegreeMatrix{g} \in \Real^{n \times n}$ is the \emph{degree matrix}, where all entries are zero except the diagonal entries, for which $d_{ii} = \Fun{degree}(v_i)$. These matrices provide information about the graph connectivity through their eigenvalues and eigenvectors.

\section{The Adaptive Processing of Structured Data}
The processing of structured data for learning purposes is carried out by a \keyword{structural transduction}, namely a function $\Transduction: \Cal{x} \shortrightarrow \Cal{y}$ where $\Cal{X}$ and $\Cal{Y}$ are structured domains. When the structural transduction is implemented by a (deep) \gls{nn}, it is \emph{adaptive}, \ie it is learned from data. A structural transduction can be decomposed as $\Transduction = \EncTrans \circ \OutTrans$, where:
\begin{itemize}
    \item $\EncTrans$ is called \emph{encoding function} or \emph{state transition function} that is applied separately to each element of the structure. The output of the encoding function is a structure isomorphic to that in input, where the elements are now \keyword{state vectors}. Intuitively, a state vector encodes the information of the element and of the elements it depends on;
    \item $\OutTrans$ is called \emph{output function}, which computes an output from the state vectors.
\end{itemize}
The output function of the structural transduction is task-dependent. Considering a supervised setting and a generic graph dataset $\Data$ consisting of $n$ training pairs, we distinguish two learning problems:
\begin{itemize}
    \item in \emph{structure-to-structure} tasks, the dataset is composed of pairs of attributed graphs $\Data = \{(\PatternGraph{g}{i}, \PatternGraph{h}{i})\}_{i=1}^n$, where $\AttrGraph{g} = \VectorGraph{x}{g}$ is an input graph, $\AttrGraph{h} = \VectorGraph{y}{h}$ is an output graph, and the two underlying unattributed graphs $\Graph{g}$ and $\Graph{h}$ are isomorphic under a bijection $\vartheta$. The task is to predict the target associated to an output graph vertex, given the corresponding features of its isomorphic vertex. The objective function minimized in these tasks is the following:
    $$\argmin_{\Param} \frac{1}{n} \sum_{i=1}^n -\log p(\PatternGraph{h}{i} \given \PatternGraph{g}{i}) = -\frac{1}{n} \sum_{i=1}^n \sum_{v \in \Nodes{g_{(i)}}} -\log p(\Elem{y}{\vartheta(v)} \given \Elem{x}{v});$$
    \item in \emph{structure-to-element} tasks, the dataset has the form  $\Data = \{(\PatternGraph{g}{i}, \PatternVector{y}{i})\}_{i=1}^n$, where $\AttrGraph{g} = \VectorGraph{x}{g}$ is an input graph and $\Vector{y} \in \Real^y$ is an output vector. The task is to predict a single output vector (or scalar) from the structure and the features of $\AttrGraph{g}$. The  objective function minimized in these tasks is the following:
    $$\argmin_{\Param} \frac{1}{n} \sum_{i=1}^n - \log p(\PatternVector{y}{i} \given \PatternGraph{g}{i}).$$
    To learn structure-to-element tasks, the output function must compress the states of each element of the structure into a global output vector representing the entire structure, which is compared to the target $\Vector{y}$. To do so, there are several strategies; in general, one could pick a single state vector as a representative for the whole structure, or compute a summary of the entire structure using all the available state vectors. The function that implements the latter strategy is usually termed \keyword{readout}.
\end{itemize}

As anticipated, one important issue that structural transductions need to address is how to deal with variable-sized inputs. The solution is to apply the same state transition function (that is, with the same adaptive parameters) \emph{locally} to every element in the structure, rather than to apply it one time to the overall structure. This process is similar to the localized processing of images performed by \glspl{cnn}, which works by considering a single pixel at a time, and combining it with some finite set of nearby pixels. This local property of the structural transduction is often referred to as \emph{stationarity}. An interesting byproduct of using stationary transductions is that they require a smaller number of parameters with respect to non-stationary ones, since the network weights are shared across the structure. At the same time, using stationary transductions also requires additional mechanisms to learn from the global structure of the datum (such as readouts in the case of structure-to-element tasks), rather than only locally.

In the following sections, we present three specific \gls{nn} architectures that implement transductions over structured data: recurrent neural networks, which process data represented as sequences; recursive neural networks, which process hierarchical data such as trees; and deep graph networks, which process general graphs.

\section{Recurrent Neural Networks}\label{sec:rnns}
A \gls{rnn} is a \gls{nn} architecture able to process sequences. Let $\Seq{S}$ be an attributed sequence of length $m$ with elements $\Nodes{s} = (v_1, v_2, \ldots, v_m)$, and let $\GraphFeatures{x}{s} = \Par{\Elem{x}{1}, \Elem{x}{2}, \ldots, \Elem{x}{m}}$ be its element features. Here, we slightly abuse the notation $\Elem{x}{v_t}$ in favor of $\Elem{x}{t}$, since sequence elements are ordered. In supervised settings, we also assume an isomorphic attributed sequence $\Graph{R}$ with targets $\GraphFeatures{y}{r} = \Par{\Elem{y}{1}, \Elem{y}{2}, \ldots, \Elem{y}{m}}$. The state transition function of a \gls{rnn}, applied locally to each sequence element, has the following general form:
\begin{align*}
    \Elem{h}{t} =
    \begin{cases}
        \boldsymbol{0} & \mathrm{if}\; t = 0\\
        \EncTrans(\Elem{x}{t}, \Elem{h}{t-1}) & \mathrm{otherwise},
    \end{cases}
\end{align*}
where $\Elem{h}{t} \in \Real^h$ is a state vector, also known as \keyword{hidden state}, and $\boldsymbol{0}$ is a zero vector. The calculation of the hidden state performed by the state transition function $\EncTrans$ is \emph{recursive}: to compute a hidden state for the $t$-th element of the sequence, the hidden state of the previous element must be known in advance. Thus, the state computation is a sequential process, where the input sequence is traversed in order one element at a time, and the hidden state is updated as a function of the current sequence element and the hidden state at the previous step. To avoid infinite recursion, the hidden state is initialized with the zero vector $\Elem{h}{0} = \boldsymbol{0}$. As the sequence is traversed, the hidden state maintains a \emph{memory} of the past elements of the sequence. The presence of a memory mechanism makes \glspl{rnn} very powerful: in fact, it has been proved that finite-size \glspl{rnn} can compute any function computable with a Turing machine \citep{siegelmann1995rnnturing}. As with \glspl{cnn}, the development of \glspl{rnn} started in the early '90s, and they have recently been rediscovered within the \gls{dl} framework after their success, especially in \gls{nlp}-related tasks.

\subsection{Training}
Given an attributed sequence $\Seq{S}$ with features $\Vector{x}_{\Seq{s}}$, the original implementation of the state transition function of a \gls{rnn} is defined as follows\footnote{For the rest of this chapter, biases are omitted for readability.}:
\begin{align*}
    \Elem{h}{t} &= \tanh\Par{\Matrix{W}\Elem{x}{t} + \Matrix{U}\Elem{h}{t-1}},\; \forall t=1, \ldots, m.
\end{align*}
The above is also called \keyword{recurrent layer}. The weight matrices $\Matrix{W} \in \Real^{d \times h}$ and $\Matrix{U} \in \Real^{h \times h}$, are shared among the sequence elements according to the stationarity property. For this reason, it is often said that the network is \emph{unrolled} over the sequence. In structure-to-structure tasks, once the states of the elements are calculated, an element-wise output is computed as:
\begin{align*}
    \Elem{o}{t} = g(\Elem{h}{t}),\; \forall t=1, \ldots, m,
\end{align*}
where $g$ can be any neural network such as one simple output layer or a more complex downstream network. Similarly, in structure-to-element tasks, a single output is computed from the last hidden state of the sequence:
\begin{align*}
    \Vector{o} = g(\Elem{h}{m}).
\end{align*}
Figure \ref{fig:rnn-unfold} shows a \gls{rnn} in compact form, as well as unrolled over a sequence of length $m$ for a structure-to-structure task. The error of the network during training is computed by comparing the output of the network for each sequence element $\Elem{o}{t}$ to the isomorphic sequence element $\Elem{y}{t}$ in the target sequence with the loss function $\Loss$, which is summed up over all the elements in the sequence. Notice that it is possible to stack multiple recurrent layers and create deep \glspl{rnn} by feeding the hidden state produced the recurrent layer to a subsequent recurrent layer, other than to the next step of the recurrence. In these cases, the output is computed after the last recurrent layer.
\begin{figure*}[h!]
    \begin{subfigure}[b]{0.39\linewidth}
        \centering
        \resizebox{.7\textwidth}{!}{\input{Figures/Chapter3/04a-rnn.tex}}
        \caption{}
        \label{fig:rnn}
    \end{subfigure}
    \begin{subfigure}[b]{0.59\linewidth}
        \centering
        \resizebox{.8\textwidth}{!}{\input{Figures/Chapter3/04b-rnn-unfold.tex}}
        \caption{}
        \label{fig:rnn-unfold}
    \end{subfigure}
    \caption{({\scriptsize A}): An example of recurrent neural network that can learn a structure-to-structure task. ({\scriptsize B}): the same network unfolded over a training pair of sequences of length $m$.}
    \label{fig:rnn-example}
\end{figure*}
\glspl{rnn} can be also adapted to learn structure-to-structure distributions of the kind $p(\AttrGraph{R} \given \AttrGraph{S})$, where the underlying unattributed graphs $\Seq{r}$ and $\Seq{s}$ are not isomorphic, \ie when the lengths of the input and target sequence do not match. The usual way to proceed in this case is to use two \glspl{rnn}: one acts as an encoder, computing a fixed-size representation of the input $\AttrGraph{S}$ (for example, its last hidden state as seen above); the other acts as a decoder of the target sequence $\AttrGraph{R}$, conditioned on the input representation. The conditioning is achieved by initializing the hidden state of the decoder \gls{rnn} with the encoding of the input computed by the encoder \gls{rnn}. These types of architectures are called \gls{s2s} models.

\glspl{rnn} are usually trained with \gls{bptt} \citep{werbos1988backpropthroughtime}, a variant of vanilla backpropagation that propagates the gradient both from the output layer to the recurrent layer, and backwards along the sequence elements. One \gls{bptt} update requires $\mathcal{O}(mb)$ computation, where $m$ is the sequence length and $b$ is the size of the mini-batch given to the optimizer. This can become computationally inconvenient for long sequences, and can lead to instabilities like gradient vanishing. Thus, in practical settings, faster \gls{bptt} variants are often used, such as truncated \gls{bptt} \citep{jaeger2002truncatedbptt}.

\subsection{Gated Recurrent Neural Networks}
Vanilla \glspl{rnn} struggle to learn with long sequences. This issue has been documented several times in the literature (see \eg \citep{bengio1994learninglongtermdependenciesdifficult}), and is mostly due to the gradient vanishing or exploding problems. While gradient exploding can be dealt with gradient clipping, gradient vanishing is more hard to tackle. Several workarounds have been proposed to overcome such limitation; the most adopted in practical settings exploits a form of information \emph{gating}, that is, by controlling the information flow inside the recurrent layer. In particular, it might be useful for the network to \emph{forget} useless information, or to \emph{reset} the hidden state when some kind of knowledge has already been processed. Gated mechanisms fulfill this purpose adaptively, driven by data. The most used \gls{rnn} variant that implements gating mechanisms is the \gls{lstm} \citep{hochreiter1997lstm}.
An \gls{lstm} is composed of a \emph{cell} $\Vector{c} \in \Real^h$, an \emph{input gate} $\Vector{i} \in \Real^h$, a \emph{forget gate} $\Vector{f} \in \Real^h$, and an \emph{output gate} $\Vector{g} \in \Real^h$. Assuming an input sequence element $\Elem{x}{t} \in \Real^d$, the hidden state $\Elem{h}{t} \in \Real^h$ of a \gls{lstm} is computed as follows:
\begin{align*}
    \Elem{f}{t} &= \sigmoid\Par{\Matrix{W}_{1}\Elem{x}{t} + \Matrix{U}_{1}\Elem{h}{t-1}}\\
    \Elem{i}{t} &= \sigmoid\Par{\Matrix{W}_{2}\Elem{x}{t} + \Matrix{U}_{2}\Elem{h}{t-1}}\\
    \Elem{g}{t} &= \sigmoid\Par{\Matrix{W}_{3}\Elem{x}{t} + \Matrix{U}_{3}\Elem{h}{t-1}}\\
    \Vector{\tilde{c}}_{(t)} &= \tanh\Par{\Matrix{W}_{4}\Elem{x}{t} + \Matrix{U}_{4}\Elem{h}{t-1}}\\
    \Elem{c}{t} &= \Elem{f}{t} \odot \Elem{c}{t-1} + \Elem{i}{t} \odot \Vector{\tilde{c}}_{(t)}\\
    \Elem{h}{t} &= \Elem{g}{t} \odot \tanh(\Elem{c}{t}),
\end{align*}
where $\odot$ is the Hadamard (element-wise) product between matrices. Notice that the weight matrices $\Matrix{W}_i \in \Real^{d \times h}$ and $\Matrix{U}_i \in \Real^{h \times h}$ with $i=1, \ldots, 4$ are all different. In short, the input gate controls how much of the input is kept, the forget gate controls how much information about previous elements is kept, and the output gate controls how much of the two should be used to compute the hidden state. While powerful, a single \gls{lstm} requires eight weight matrices; thus, it is computationally expensive to train. The \gls{gru} \citep{cho2014gru} gating mechanism is a lightweight alternative to \gls{lstm} which uses less parameters, though it is slightly less powerful \citep{gruber2020gruspecificlstm}. A \gls{gru} uses two gates, an \emph{update} gate $\Vector{u} \in \Real^h$ and a \emph{reset} gate $\Vector{r} \in \Real^h$, and computes the hidden state as follows:
\begin{align*}
    \Elem{u}{t} &= \sigmoid\Par{\Matrix{W}_{1}\Elem{x}{t} + \Matrix{U}_{1}\Elem{h}{t-1}}\\
    \Elem{r}{t} &= \sigmoid\Par{\Matrix{W}_{2}\Elem{x}{t} + \Matrix{U}_{2}\Elem{h}{t-1}}\\
    \Vector{\tilde{h}}_{(t)} &= \tanh\Par{\Matrix{W}_{3}\Elem{x}{t} + \Matrix{U}_{3}(\Elem{r}{t} \odot \Elem{h}{t-1})}\\
    \Elem{h}{t} &= (\Ones - \Elem{u}{t}) \odot \Elem{h}{t-1} + \Elem{u}{t} \odot \Vector{\tilde{h}}_{(t)},
\end{align*}
where $\Ones \in \Real^h$ is a vector of all ones. In practice, the reset gate controls how much information from previous sequence elements should be kept, and the hidden state is computed as a convex combination of this quantity and the previous hidden state, controlled by the update gate.

\subsection{Recurrent Neural Networks as Autoregressive Models}
Besides being used as supervised models, \glspl{rnn} can be also used as generative models of sequences \cite{graves2013generating}. Specifically, given a dataset $\Data = \{\PatternGraph{S}{i}\}_{i=1}^n$ of sequences, they can be trained to learn a model $p_{\Param}$ of the underlying distribution $p(\AttrGraph{s})$ as follows:
$$\argmin_{\Param} \frac{1}{n} \sum_{\AttrGraph{s} \in \Data} - \log p_{\Param}(\AttrGraph{s}) = \frac{1}{n} \sum_{\AttrGraph{s} \in \Data} \sum_{t=1}^{|\AttrGraph{s}|} - \log p_{\Param}(\Elem{x}{t} \given \Elem{x}{t-1}, \Elem{h}{t-1}),$$
where $|\AttrGraph{S}|$ indicates the sequence length, $\Elem{x}{0}$ is a special \emph{start of sequence symbol} $\SOS$, and $\Elem{h}{0} = \Zeros$ as usual. Once the network is trained, a sequence can be generated one element at a time. The process is initialized by feeding the start of sequence token $\Elem{x}{0}$ and the initial hidden state $\Elem{h}{0}$, and proceeds repeating the following instructions until an \emph{end of sequence token} $\Tuple{\mathsf{E}}$ is predicted by the network:
\begin{itemize}
    \item update the current state $\Elem{h}{t}$ with the \gls{rnn} and predict an output $\Elem{o}{t}$, which corresponds to a conditional distribution over the possible sequence elements. Sample a sequence element $\Elem{\tilde{x}}{t}$ according to this distribution;
    \item feed the sampled element $\Elem{\tilde{x}}{t}$ and the updated state $\Elem{h}{t}$ to the network and repeat.
\end{itemize}
We call this process \emph{autoregressive sampling mode}. The process is depicted in Figure \ref{fig:rnn-autoregressive}.
\begin{figure*}[h!]
    \centering
    \resizebox{.7\textwidth}{!}{\input{Figures/Chapter3/05a-autoregressive}}
    \caption{An example of training a recurrent neural network for learning an autoregressive distribution. The dashed arrows indicate non-differentiable operations.}
    \label{fig:rnn-autoregressive}
\end{figure*}
During training, the sampling process breaks the differentiability of the model. Hence, one resorts to reparameterization \citep{jang2017gumbel} or differentiates the output $\Elem{o}{t}$ instead of the hard sample \citep{bengio2013straighttrough}. Another option during training is \emph{teacher forcing} \citep{williams1989teacherforcing}: in this case, the knowledge of the elements of the sequence during training is exploited, by feeding the ground truth sequence element (instead of the sampled value) as the input for the next sequence. Both strategies have advantages and disadvantages: teacher forcing learns faster initially, but does not expose the network to its own errors, thus it can be less precise at generation time. Often, a combination of the two is used.
\begin{figure*}[h!]
    \centering
    \resizebox{.7\textwidth}{!}{\input{Figures/Chapter3/05b-teacher-forcing.tex}}
    \caption{The teacher forcing strategy for training a sequence generator with RNNs.}
    \label{fig:teacher-forcing}
\end{figure*}

\section{Recursive Neural Networks} \label{sec:recnns}
A \gls{recnn} \citep{sperduti1997generalizedneuron,frasconi1998general} is a \gls{nn} architecture that can adaptively process hierarchical data. Using trees as an example, let $\Tree{T}$ be an attributed tree with $m$ nodes $\Nodes{t} = \Set{v_1, v_2, \ldots, v_m}$, and $\Vector{x}_{\Tree{t}} = \Set{\Elem{x}{v} \in \Real^d \mid v \in \Nodes{t}}$ be its set node of features. The state transition function of a \gls{recnn}, applied locally to each node $v \in \Nodes{t}$, is the following:
\begin{align*}
    \Elem{h}{v}=
    \begin{cases}
        \boldsymbol{0} & \mathrm{if}\; \Fun{ch}(v) = \emptyset\\
        \EncTrans(\Elem{x}{v}, \Elem{h}{\Tree{t}_v}) & \mathrm{otherwise},
    \end{cases}
\end{align*}
where $\Elem{h}{\Tree{t}_v} \in \Real^h$ is the hidden state of the sub-tree rooted at $v$. As with \glspl{rnn}, the state transition function $\EncTrans$ is recursive, but this time the recursion is defined over the tree structure. Specifically, to compute the hidden state of a node, the hidden state of all its children must be known in advance. The state computation starts at the leaves of the tree (where the state is initialized beforehand to make the recursion well-defined), and proceeds bottom-up until the root node is reached.
The development of \glspl{recnn} started in the middle '90s, with the introduction of the notion of generalized recursive neuron \citep{sperduti1997generalizedneuron} and the development of a general framework for learning with tree-structured data \citep{frasconi1998general}, which was later extended to more expressive classes of structures such as \glspl{dag} and \glspl{dpag}. Since then, they have been applied fruitfully in several fields, including among others cheminformatics \citep{micheli2007introductionrecnncheminformatics,baldi2013recursiveneuralnets}, \gls{nlp} \citep{costa2000recnns,costa2003recnns,sturt2003recnns,socher2013recnnsentiment} and scene parsing \citep{socher2011parsingscenes}. Interestingly, \glspl{recnn} are also backed up by strong theoretical results, which support the generality of the structural transduction and characterize the kind of functions they can learn. Specifically, universal approximation theorems showing that \glspl{recnn} can approximate arbitrarily well any function from labeled trees to real values \citep{hammer1999recnn}, and from labeled \glspl{dpag} to real values \citep{hammer2005universal} have been proved.

\subsection{Training}
Using the binary tree of Figure \ref{fig:tree} and a structure-to-element task as an example, one possible implementation of the state transition function of a \gls{recnn} is the following:
\begin{align*}
    \Elem{h}{v} &= \sigma\Par{\Matrix{W}\Elem{x}{v} + \Matrix{U}_\Fun{l}\Elem{h}{\Fun{l}(v)} + \Matrix{U}_\Fun{r}\Elem{h}{\Fun{r}(v)}},\; \forall v \in \Nodes{t}.
\end{align*}
In the above formula, $g$ can be any hidden activation function, and $\Matrix{w} \in \Real^{d \times h}$, $\Matrix{u}_\Fun{l}$, and $\Matrix{u}_\Fun{r} \in \Real^{h \times h}$, are weight matrices shared across the structure. Notice that the two weight matrices on the node children are positional, meaning that they are applied to a certain node according to its position. In the example case of a binary tree, the two functions $\Fun{l}(v)$ and $\Fun{r}(v)$ select the left and right child of a node $v$, respectively, if they exist. Figure \ref{fig:recnn} shows the unfolded \gls{recnn} over the tree, where the final output of the entire structure is obtained using the hidden state of the root node $v_1$ as:
$$\Vector{o} = g\Par{\Elem{h}{v_1}},$$
where $\Vector{o} \in \Real^y$ is the output of the network and $g$ can be any downstream network such as a simple output layer, or a more complex neural network. For structure-to-structure tasks, the output is calculated node-wise as follows:
$$\Elem{o}{v} = g\Par{\Elem{h}{v}},\; \forall v \in \Nodes{t}.$$
Notably, the order by which the hidden states need to be calculated (the numbers at the left of the hidden states in the figure) must be respected to ensure that the recursive process is well-defined. The states of nodes with the same ordering can be calculated in parallel according to the tree structure, which makes \glspl{recnn} more efficient than \glspl{rnn} when compared on structures with the same number of elements.
\begin{figure*}[h!]
    \centering
    \resizebox{.5\textwidth}{!}{\input{Figures/Chapter3/06-recnn}}
    \caption{A recursive neural network unfolded over the tree of Figure \ref{fig:tree} for a structure-to-element task. The number at the left of the hidden states indicates the order in which they are calculated. Notice the initialization of the hidden states at the leaves (indicated by dashed boxes).}
    \label{fig:recnn}
\end{figure*}
More in general, \glspl{recnn} are analogous to \glspl{rnn} as to how they can be trained with \gls{mle}, and as what kinds of conditional distributions they can learn (even though in practical cases the structure-to-element scenario is more common).

\section{Deep Graph Networks} \label{sec:dgns}
The \gls{rnn} and \gls{recnn} models presented in Sections \ref{sec:rnns} and Section \ref{sec:recnns} share the idea that the state transition function is applied locally and recursively on the structure to compute the state vectors. Extending it to arbitrary graphs (which can have cycles) would require to apply the state transition function recursively to the neighbors of a vertex. However, this approach is not applicable to general graphs. In fact, the presence of cycles creates \emph{mutual dependencies}, which are difficult to model recursively and may lead to infinite loops when computing the states of vertices in parallel. While this issue can be overcome by resorting to canonization techniques that provide an ordering between the vertices, it is not feasible in many practical cases. \glspl{dgn} are a class of \glspl{nn} which can process arbitrary graphs, even in presence of cycles. The solution adopted by \glspl{dgn} to the problem of modelling mutual dependencies is to update the state of the vertices according to an \emph{iterative} scheme. Specifically, the hidden state of a vertex is updated as a function of the hidden state of the same vertex at the previous iteration. Given an attributed graph $\AttrGraph{g}$ with vertex features $\GraphFeatures{x}{g} = \Set{\Elem{x}{v} \mid v \in \Nodes{g}}$, the state transition function computed by a \gls{dgn}, applied locally to each vertex of $v \in \Nodes{g}$, has the following form:
\begin{align*}
    \StateVector{v}{\ell}=
    \begin{cases}
        \Elem{x}{v} & \mathrm{if}\; \ell = 0\\
        \EncTrans^{(\ell)}(\Elem{x}{v}, \StateVector{v}{\ell-1}) & \mathrm{otherwise},
    \end{cases}
\end{align*}
where $\StateVector{v}{\ell} \in \Real^h$ is now the state of vertex $v$ at iteration $\ell$, and $\EncTrans = \EncTrans^{1} \circ \EncTrans^{2} \circ \ldots \circ \EncTrans^{(\ell)}$. Notice how the value of the state vector does not depend on the value of the neighboring state vectors, but to the same state vector at the previous iteration. Following, we slightly change terminology and refer to the vertices of a graph as nodes, in accordance to the terminology currently used in the literature. For the same reason, we shall use the following terminology as regards the supervised tasks that can be learned with \glspl{dgn}:
\begin{itemize}
    \item structure-to-structure tasks shall now be termed \keyword{node classification} tasks if the targets are discrete node labels, or \keyword{node regression} tasks if the targets associated to the nodes are continuous vectors or scalars. We further distinguish among \emph{inductive} node classification (respectively, regression) tasks, if the prediction concerns unseen graphs; and \emph{transductive} node classification (respectively, regression) tasks, if the structure of the graph is fixed (\ie, the dataset is composed of one single graph), and the task is to predict from a subset of nodes for whose target is not known. The transductive setting is often referred to as semi-supervised node classification (respectively, regression);
    \item structure-to-element tasks shall now be termed \keyword{graph classification} tasks if the target associated to the graph is a discrete label, or \keyword{graph regression} tasks if the targets are continuous vectors (or scalars).
\end{itemize}

\subsection{Contextual Processing of Graph Information}
Besides solving the problem of mutual dependencies in the state computations, the iterative scheme has another important purpose, that of propagating the local information of a node to the other nodes of the graph. This process is known under several names, such as \keyword{context diffusion}. Informally, the context of a node is the set of nodes that directly or indirectly contribute to determine its hidden state; for a more formal characterization of the context, see \citep{micheli2009nn4g}. Context diffusion in a graph is obtained through \keyword{message passing}, \ie by repeatedly applying the following procedures:
\begin{itemize}
    \item each node constructs a \emph{message} vector using its hidden state, which is sent to the immediate neighbors according to the graph structure;
    \item each node receives messages from its neighbors, which are used to update its current hidden state through the state transition function.
\end{itemize}
Message passing is bootstrapped by initializing the hidden state of the nodes appropriately, so that an initial message can be created. Usually, this initial message is the vector of node features. Using the example graph of Figure \ref{fig:context-diffusion} as reference, we now explain how the context flows through the nodes as message passing is iterated. At iteration $\ell=2$, the vertex $v$ receives a single message from its only neighbor, $u$. The incoming message was constructed using information about the state of $u$ at $\ell=1$, which in turn was obtained through the state of neighbors of $u$ at $\ell=0$ (including $v$ itself). Thus, the context of $v$ at iteration $\ell=2$ includes $u$ as well as the neighbors of $u$. It is clear that, for this particular case, at iteration $\ell=3$ the context of $v$ would include all the nodes in the graph. Clearly, by iterating message passing, the nodes are able to acquire information from other nodes farther away in the graph.
\begin{figure*}[h!]
    \centering
    \resizebox{.8\textwidth}{!}{\input{Figures/Chapter4/01-context-diffusion}}
    \caption{Context diffusion through message passing. Directed edges represent messages (\eg from node $u$ to $v$ at iteration $\ell=2$). Dashed arrows represent the implicit contextual information received by a node (in dark grey) through the messages from its neighbors (in light grey). Focusing on node $v$, its context at iteration $\ell=2$ is composed all the dark grey nodes (including $v$ itself).}
    \label{fig:context-diffusion}
\end{figure*}
In the literature, we distinguish three different approaches by which iterative context diffusion is implemented in practice, which we describe in the following.

\subsubsection*{Recursive Approaches}
In the recursive approach to context diffusion, message passing is formulated as a dynamical system. In this case, the state transition function is recursive, meaning that $\EncTrans = \EncTrans^{(1)} = \EncTrans^{(2)} = \ldots = \EncTrans^{(\ell)}$. Practically speaking, the mutual dependencies between hidden states are modeled with a single recurrent layer, which is run indefinitely until convergence. Some well-known representatives of this paradigm are the Graph Neural Network \citep{scarselli2009gnn}, the Graph Echo State Network \citep{gallicchio2010graphesn}, and the more recent Fast and Deep Graph Neural Network \citep{gallicchio2020fastdeepgnn}. To ensure convergence, these approaches impose contractive dynamics on the state transition function. While the Graph Neural Network enforces such constraints in the (supervised) loss function, the other two inherit convergence from the contractivity of (untrained) reservoir dynamics. Another example is the Gated Graph Neural Network \citep{li2016gatedgnn}, where, differently from \citet{scarselli2009gnn}, the number of iterations is fixed \apriori regardless of whether convergence is reached or not. Another approach based on \emph{collective inference}, which adopts the same strategy but does not rely on any particular convergence criteria, has been introduced in \citep{macskassy2007classificationnetworkdata}.

\subsubsection*{Feed-Forward Approaches}
The feed-forward approach is based on stacking multiple layers to compose the local context learned at each message passing iteration. As a result, the mutual dependencies between the hidden states are handled separately via differently parameterized layers, without the need of constraints to ensure the convergence of the state transition function. In practice, the state transition function is no more recursive, but changes at every layer. Thus, in the feed-forward case, the symbol $\ell$ indicates the layer that handles the corresponding message passing iteration. The effectiveness of the compositionality induced by the introduction of layers has been demonstrated in \citep{micheli2009nn4g}, where it is shown formally that the context of a node increases as a function of the network depth, up to including all the other nodes in the graph. Feed-forward approaches are nowadays the main paradigm to design \glspl{dgn}, due to their simplicity, efficiency, and performance on many different tasks. However, deep networks for graphs suffer from the same gradient-related problems as other deep \glspl{nn}, especially when associated with an \quotes{end-to-end} learning process running through the whole architecture \citep{bengio1994learninglongtermdependenciesdifficult,li2018deeperinsightgraphconvsemisupervised}. For the rest of this thesis, all the \glspl{dgn} used shall be feed-forward.

\subsubsection*{Constructive Approaches}
Constructive approaches are a special case of feed-forward models, in which training is performed layer-wise. The major benefit of constructive architectures is that deep networks do not incur the vanishing/exploding gradient problem by design. In supervised scenarios, the constructive technique can learn the number of layers needed to solve a task \citep{fahlman1990cascor,marquez2018deepcascade,bianucci2000cascorchemistry}. In other words, constructive \glspl{dgn} can determine automatically how much context is most beneficial to perform well, according to the specific task at hand. Another feature of constructive models is that they solve a problem in a \emph{divide-et-impera} fashion, rather than using \quotes{end-to-end} training, by incrementally splitting the task into manageable sub-tasks. Each layer solves its own sub-problem, and subsequent layers use their results to improve further on their own, addressing the global task progressively. Among the constructive approaches, we mention the Neural Network for Graphs \citep{micheli2009nn4g}, which was the first to propose a feed-forward architecture for graphs. Among recent models, another related approach which tackles the problem from a probabilistic point of view is the Contextual Graph Markov Model \citep{bacciu2018contgraphmarkov}.

\subsection{Building Blocks of Deep Graph Networks}
\glspl{dgn} are built from several architectural components, which we cover in detail in this section. In short, a \gls{dgn} can be decomposed into a collection of layers that process the graph structure, and a downstream predictor (either a classifier or a regressor) that computes a task-dependent output. The whole network is trained in an end-to-end fashion. In this section, we focus on the former components, the ones whose role is to carry out the processing of an input graph.

\subsubsection*{Graph Convolutional Layers}\label{sec:graph-conv-layers}
A \gls{gcl} is essentially a neural network layer that performs message passing. The term \quotes{convolutional} is used to remark that the local processing performed by the state transition function is a generalization of the convolutional layer for images to graph domains with variable-size neighborhoods. Given an attributed graph $\AttrGraph{g}$ with $n$ nodes, and its node attributes $\GraphFeatures{x}{g} = \Set{\Elem{x}{v} \mid v \in \Nodes{g}}$, one general formulation of a \gls{gcl} is the following:
\begin{align}
    \label{eq:simple-aggregation}
    \StateVector{v}{\ell} = U \Par{\StateVector{v}{\ell-1}, A\Par{\Set{\,T(\StateVector{u}{\ell-1}) \mid u \in \Neigh(v)}}}, \; \forall v \in \Nodes{g},
\end{align}
where $\StateVector{v}{\ell} \in \Real^{h_\ell}$ is the hidden state of the node at layer $\ell$, $\,\StateVector{v}{\ell-1} \in \Real^{h_{\ell-1}}$ is the hidden state of the node at the previous layer $\ell-1$, and by convention $\StateVector{v}{0} = \Elem{x}{v}$. Notice that the neighborhood function $\Neigh$ is also implicitly passed as input of the layer, so that the connectivity of each node is known. We can identify three key functions inside a \gls{gcl}:
\begin{itemize}
    \item $T: \Real^{h_{\ell-1}} \shortrightarrow \Real^{h_{\ell-1}}$ is a \emph{transform} function that applies some transformation to the hidden states of neighbors of node $v$ at layer $\ell-1$. This can be any function, either fixed or adaptive (implemented by a neural network);
    \item $A: (\Real^{h_{\ell-1}} \times \Real^{h_{\ell-1}} \times \ldots) \shortrightarrow \Real^{h_{\ell-1}}$ is an \emph{aggregation} function that maps a \emph{multiset}\footnote{Given a set $\Cal{B}$, a multiset $\Multiset(\Cal{B})$ is a tuple $\Tuple{\Cal{B}, \varrho}$ where $\varrho: \Cal{B} \shortrightarrow \Natural_+$ gives the multiplicity of each element in $\Cal{B}$.} of transformed neighbors of $v$ to a unique \emph{neighborhood state vector}. In practice, $A$ is a \emph{permutation invariant} function, meaning that its output does not change upon reordering of the arguments. For this reason, the computation of the neighborhood state vector is often referred to as \emph{neighborhood aggregation};
    \item $U: (\Real^{h_{\ell-1}} \times \Real^{h_{\ell-1}}) \shortrightarrow \Real^{h_{\ell}}$ is an \emph{update} function that takes the hidden state of a node at layer $\ell-1$ and the aggregated vector, and combines them to produce the new hidden state of the node at layer $\ell$. Similarly to $T$, $U$ can also be fixed or adaptive.
\end{itemize}
The usage of a permutation invariant function to compute the state of the neighbors is crucial, as it allows to acquire information from nearby nodes in a non-positional fashion, which is often the case with real-world graphs. From this general formulation, several implementations can be realized. As an example, we report the well-known formulation of \citet{kipf2017semisupervisedgcn}, corresponding to the \gls{gcn} model:
\begin{align}
    \label{eq:convolutional}
    \StateVector{v}{\ell} = \sigmoid\Par{ \LayerMatrix{w}{\ell} \sum_{u \in \Neigh(v)} \tilde{l}_{uv}\StateVector{u}{\ell-1}}, \; \forall v \in \Nodes{g},
\end{align}
where $\tilde{l}_{uv}$ is the entry of the symmetric normalized graph Laplacian $\NormLaplacianMatrix{g}$ related to nodes $u$ and $v$, and:
\begin{align}
    \label{eq:transform}
    T(\StateVector{u}{\ell-1}) &= \GenStateVector{t}{v}{\ell-1}  = \tilde{l}_{uv}\,\StateVector{u}{\ell-1}\\
    \label{eq:aggregate}
    A\Par{\Set{\GenStateVector{t}{v}{\ell-1} \mid u \in \Neigh(v)}} &= \GenStateVector{n}{v}{\ell-1} = \sum_{u \in \Neigh(v)} \GenStateVector{t}{v}{\ell-1}\\
    \label{eq:update}
    U(\StateVector{v}{\ell-1}, \GenStateVector{n}{v}{\ell-1}) &= \StateVector{v}{\ell} =  \sigmoid\Par{\LayerMatrix{w}{\ell}\, \GenStateVector{n}{v}{\ell-1}}.
\end{align}
In this case, the aggregation function is the sum function. Other examples of permutation invariant functions used in practical contexts are the mean, the max, or other general functions which work on multisets \citep{zaheer2017deepsets}. Notice that a \gls{gcl} can be applied simultaneously to all the nodes in the graph, corresponding to visiting the graph nodes in parallel, with no predefined ordering. This contrasts with \glspl{rnn} and \glspl{recnn}, where parallelism in the state calculations is not possible or limited, respectively.

The generic \gls{gcl} can be rewritten in matrix form as some variation of the following:
$$\LayerStateMat{g}{\ell} = \Fun{GCL}(\AdjMatrix{g}, \LayerStateMat{g}{\ell-1}) = g\Par{\AdjMatrix{g}\LayerStateMat{g}{\ell-1}\LayerMatrix{w}{\ell}} \in \Real^{n \times h^{(\ell)}},$$
where $g$ is a generic activation function, $\AdjMatrix{g} \in \Real^{n \times n}$ is the adjacency matrix of the graph, $\LayerStateMat{g}{\ell-1} \in \Real^{n \times h}$ are the hidden states computed at layer $\ell-1$ where by convention $\LayerStateMat{g}{0} = \FeatureMatrix{g} \in \Real^{n \times d}$ is the matrix of node features, and $\LayerMatrix{w}{\ell} \in \Real^{h_{(\ell-1)} \times h_{(\ell)}}$ is the matrix of trainable layer-wise weights. Here, the adjacency matrix substitutes the neighborhood function $\Neigh$, and the node adjacencies are inferred by its rows and columns. With this formulation, the \gls{gcl} can be vectorized, which allows to run the state computation in fast hardware such as \glspl{gpu}.

\paragraph{Handling Edges}
In certain tasks, including information about the edge features to the message passing algorithm can be beneficial to performances. Here, we describe how this can be achieved, focusing on the case where the edge features are discrete values out of a set of $k$ possible choices. Specifically, given an attributed graph $\AttrGraph{g}$, we assume a set of edge features of the form $\GraphFeatures{e}{g} = \Set{\Elem{e}{u,v} \in \Cal{C} \mid (u, v) \in \Edges{g}}$, with $\Cal{C} = \Set{c_i}_{i=1}^k$. To account for different edge types, two modifications to the message passing algorithm are required. One is to replace the standard neighborhood function in the aggregation function with the following \emph{edge-aware} neighborhood function of a node $v$:
$$\Neigh_c(v) = \Set{u \in \Neigh(v) \mid \Indicator{\Elem{e}{u, v} = c}},$$
which selects only neighbors of $v$ with edge type $c$. The other modification requires to change the update function for handling the different edge types. Taking again the \gls{gcn} implementation as an example, Eq. \ref{eq:update} is modified as follows:
\begin{align*}
    \StateVector{v}{\ell} = \sigmoid\Par{\sum_{c \in \Cal{C}} \LayerMatrix{w}{\ell}_c \sum_{u \in \Neigh_c(v)} \tilde{l}_{uv}\,\Elem{h}{v}^{(\ell-1)}},\; \forall v \in \Nodes{g},
\end{align*}
where the weight matrices $\LayerMatrix{w}{\ell}_c $ are now edge-specific, so that the contributions of the different edge types are weighted adaptively \citep{micheli2009nn4g,schlichtkrull2018relationaldatagcn}. In practice, the above procedure corresponds to performing $k$ different aggregations weighted separately to compute the state of the node. Other approaches to include edge information in the message passing scheme require to extend the transform function, such that the edges between the processed node and its neighbors are included in the transformation (for example, by concatenating the edge feature to the hidden state vector).

\paragraph{Node Attention}
Attention mechanisms \citep{bahdanau2015attention} are a widely used tool in Deep Learning to get importance scores out of arbitrary sets of items. Thus, they are naturally applicable within the \gls{dgn} framework to measure the contribution of the different nodes during neighborhood aggregation. Specifically, to introduce attention mechanisms in the neighborhood aggregation, we weigh the contribution of the transformed nodes in the neighborhood by a scalar $a^{(\ell)}_{uv} \in \Real$, called \emph{attention score} as follows:
$$A(\Set{\,a^{(\ell)}_{uv}\,T(\StateVector{u}{\ell-1}) \mid u \in \Neigh(v)}).$$
The attention scores are derived from \emph{attention coefficients} $w_{vu}^{(\ell)}$, which are essentially similarity scores between the neighbor and the current node, calculated as follows:
$$w_{vu}^{(\ell)} = F(\StateVector{v}{\ell}, \StateVector{u}{\ell}),$$
where $F$ is an arbitrary function (generally a neural network). Different attention mechanisms are defined based on how $F$ is implemented. Finally, the coefficients are normalized into attention scores by a softmax function, effectively defining a probability distribution among them. The attention mechanism can be generalized to \emph{multi-head} attention, where multiple attention scores for each node are calculated and concatenated together to obtain an attention vector, rather than a score. Figure \ref{fig:attention} shows an example of attention computed on an example graph. We remark that node attention is unrelated to weighting the connection between nodes, which is an operation that involves the edge features. Here, similarity between nodes is calculated relying solely on the hidden states of the involved node and its neighbors.

\begin{figure*}[h!]
    \centering
    \resizebox{.35\textwidth}{!}{\input{Figures/Chapter4/02-attention}}
    \caption{An example of node attention. Note that edge thickness is not related to the strength of the connection between node $v$ (in dark grey) and its neighbors (in light grey), but it represents the degree of similarity between the node states, quantified in a probabilistic sense by the attention score. Dashed edges connect nodes that are not involved in the attention score computation.}
    \label{fig:attention}
\end{figure*}

\paragraph{Node Sampling}
Node sampling is a technique used when learning on large graphs to ensure computational efficiency. When the number of nodes in a graph is large, and nodes are densely connected among themselves, computing neighborhood aggregation may become very expensive or even intractable. The most straightforward method to address this issue is to randomly sample a predefined number of nodes to aggregate, rather than using the whole neighborhood. This basic strategy can be refined by using more sophisticated techniques such as importance sampling \citep{gallicchio2020fastdeepgnn}, or even extended to sampling a bounded number of nodes which are not necessarily in the immediate neighborhood of the current node \citep{hamilton2017graphsage}. The latter requires to add fictitious edges between the current node and nodes at farther distances, in order to treat them as standard neighbors. This way, global information about the graph can be incorporated more directly, as compared to message passing.

\begin{figure*}[h!]
    \centering
    \resizebox{.35\textwidth}{!}{\input{Figures/Chapter4/03-sampling}}
    \caption{An example of node sampling. We focus on a node $v$ (in dark grey), and its neighbors (in light grey). Dashed edges connect nodes that are not involved in the neighborhood aggregation; notice how nodes $u_1$ and $u_2$ are excluded from the neighborhood aggregation, even though they are effectively neighbors of $v$.}
    \label{fig:sampling}
\end{figure*}

\subsubsection*{Readout Layers}
As we have seen, the application of $L$ \gls{dgn} layers to a graph $\AttrGraph{G}$ yields $L$ hidden states per node, each one composed with a progressively broad context. In node classification or regression tasks, these are combined by a \emph{hidden state readout} function to obtain a unique hidden state to use as input of the output function, which emits a prediction for every node. Specifically, a hidden state readout function $\Fun{R'}$ computes a \keyword{node representation} (or \keyword{node embedding}) $\StateVector{v}{*}$ for each node as follows:
$$\StateVector{v}{*} = \Op{R}_{v}\Par{\Set{\StateVector{v}{\ell} \mid \ell = 1, \ldots, L}},\;  \forall v \in \Nodes{g}. $$
Notice that, when aggregating hidden states, one can exploit the fact that the number of layers is fixed beforehand in feed-forward \gls{dgn} architectures, and that the hidden states are ordered depth-wise. Thus, the aggregation need not to be permutation-invariant. Usual choices of $\Op{R}_{v}$ include concatenation, weighted average (where the mixing weights can also be learned), \gls{rnn}s, or just selecting the hidden state at the last layer. The node representations are then fed to an output layer or downstream network, which computes node-wise outputs:
$$\Elem{o}{v} = g(\StateVector{v}{*}),\; \forall v \in \Nodes{g},$$
where $\Elem{o}{v} \in \Real^y$ and $g$ can be any arbitrarily complex neural network as usual. A visual example of the process for a single node is shown in Figure \ref{fig:node-readout}.
\begin{figure*}[h!]
    \centering
    \resizebox{.8\textwidth}{!}{\input{Figures/Chapter4/04-node-readout}}
    \caption{The role of hidden state readout function in a node classification/regression task. Here, we focus on a single node $\StateVector{v}{\ell}$, where we drop the subscripts to avoid visual cluttering. Grey nodes replace the $[v]$ subscript. The hidden state readout function creates a node representation $\StateVector{v}{*}$ by combining its three hidden states (one for each layer). Successively, the node representation is turned into an output by an output layer, and compared by the loss function to the same node $\Elem{y}{v}$ in the isomorphic target graph. This operation is repeated for every node in the graph.}
    \label{fig:node-readout}
\end{figure*}
In graph classification or regression tasks, the node representations computed by a node readout function are aggregated once more by a \emph{graph readout} function, to compute a \keyword{graph representation} (or \keyword{graph embedding}) $\GraphRepr{g}$, \ie a vector representing the entire graph. Differently from the hidden state readout, the readout function must necessarily be permutation-invariant, since there are no guarantees about the number of graph nodes. Specifically, a graph readout function $\Fun{R}$ computes the embedding of graph $\AttrGraph{g}$ as follows:
$$\GraphRepr{g} = \GraphReadout{g}\Par{\Set{\StateVector{v}{*} \mid v \in \Nodes{g}}}.$$
Typical readouts for \glspl{dgn} include simple functions such sum, mean, max, or more complex aggregators such as deep sets models \citep{zaheer2017deepsets}. Finally, the graph embedding is fed to an output layer or a downstream network to compute the associated output:
$$\Vector{o} = g(\GraphRepr{g}).$$
\begin{figure*}[h!]
    \centering
    \resizebox{.6\textwidth}{!}{\input{Figures/Chapter4/05-graph-readout}}
    \caption{A graph readout on an example graph for a graph classification/regression task. Here, we assume the node representations $\StateVector{v}{*}$ have already been obtained by a node readout (not shown).}
    \label{fig:graph-readout}
\end{figure*}
An graph readout applied to an example graph is shown in Figure \ref{fig:graph-readout}.

\subsubsection*{Graph Pooling Layers}\label{sec:pooling}
Similarly to the layer used by \glspl{cnn} for computer vision, pooling is also applicable to \glspl{dgn} for graph classification (or regression) tasks. In \gls{dgn} architectures, pooling is usually placed after a graph convolutional layer, and serves a three-fold purpose: it is used to detect communities in the graph, \ie clusters of nodes with very higher connectivity among themselves than the rest of the graph; to augment the information content of the hidden states with this knowledge; and to reduce the number of nodes (and consequently, the number of parameters) needed by the network in later stages of computation. An example of a graph pooling layer is shown in Figure \ref{fig:pooling}, where nearby nodes are pooled into a single node in the reduced graph according to some strategy. Graph pooling methods are developed according to two strategies: \emph{adaptive} and \emph{tolopogical}. Adaptive methods pool nodes in a differentiable manner, so that the optimal clustering of the nodes for the task at hand is learned by the end-to-end network. One example of adaptive pooling is DiffPool, developed in \citep{ying2018diffpool}. Given a graph $\Graph{g}$ with $n$ nodes, and assuming the $\ell-1$ \glspl{gcl} have been applied, DiffPool computes two matrices:
\begin{align*}
    \GenGraphLayerMat{z}{\ell-1} &= \Fun{DGN}_e(\LayerAdjMat{\ell-1},\LayerStateMat{g}{\ell-1}) \in \Real^{n \times h}\\
    \GenGraphLayerMat{s}{\ell-1} &= \softmax\Par{\Fun{DGN}_p(\LayerAdjMat{\ell-1},\LayerStateMat{g}{\ell-1})} \in \Real^{n \times k},
\end{align*}
where $\Fun{DGN}_e$ and $\Fun{DGN}_p$ is a stack of graph convolutional layers. The matrix $\Matrix{S}$ computes a soft-assignment to each node to one of $k$ clusters with a softmax output function. These two matrices are then combined with the current hidden states to produce a novel adjacency matrix and its corresponding matrix of hidden states as follows:
\begin{align*}
    \LayerStateMat{g}{\ell} &= \GenGraphLayerMat{s}{\ell-1}\GenGraphLayerMat{z}{\ell-1} \in \Real^{k \times h}\\
    \LayerAdjMat{\ell} &= \GenGraphLayerMat{s}{\ell-1}\LayerAdjMat{\ell-1}\GenGraphLayerMat{s}{\ell-1} \in \Real^{k \times k}
\end{align*}
where $\LayerAdjMat{0} = \AdjMatrix{g}$ and $\LayerStateMat{g}{0} = \FeatureMatrix{g}$. Thus, after applying the DiffPool layer, the size of the graph is reduced progressively from $n$ to $k$ nodes.

\begin{figure*}[h!]
    \centering
    \resizebox{.8\textwidth}{!}{\input{Figures/Chapter4/06-pooling}}
    \caption{A visual example of a graph pooling layer.}
    \label{fig:pooling}
\end{figure*}
Topological pooling, on the other hand,  uses not-differentiable methods which leverage the global structure of the graph, and the communities beneath it. These methods work by grouping nodes according to well known graph theory tools, such as spectral clustering (see \eg \citet{vonluxburg2007tutorialspectralclustering, dhillon2007weightedgraphcuts}).

\subsection{Regularization}
\glspl{dgn} are trained with regular losses, such as \gls{ce} for classification and \gls{mse} for regression. Besides standard regularization techniques, the objective function is often regularized through unsupervised loss functions, which impose priors on which kinds of
structures the network should preferably learn. The regularized objective function for supervised tasks\footnote{We use a generic target $y$ to imply that this formulation is task-independent.} has the form:
$$\argmin_{\Param} \Loss(\Param, (\Vector{x}_{\Graph{g}}, y)) + \lambda \sum_{\ell=1}^L \Psi(\LayerStateMat{g}{\ell}),$$
were $\Psi$ is a regularization function weighted by a regularization coefficient $\lambda$, that is applied at each layer $\ell$ to the set of hidden states of the nodes, represented as a matrix $\Matrix{h}_{{\Graph{g}}}^{(\ell)} \in \Real^{n \times h}$. An example of regularization widely employed in practical settings is the \emph{link prediction} unsupervised loss, defined as:
\begin{align}
    \Psi(\LayerStateMat{g}{\ell}) = \sum_{u,v} \Norm{\StateVector{v}{\ell} - \StateVector{u}{\ell}}_2,
\end{align}
where the summation ranges over all possible combinations of nodes. Basically, when this loss is minimized, it biases the network towards producing node representations that are more similar for nodes connected by an edge. Notably, this loss can be also used in isolation to tackle link prediction tasks, \ie tasks where the downstream network must predict unseen links between the nodes.

\section{Deep Generative Learning on Graphs}
In this section, we discuss the graph generation problem. The term \quotes{graph generation} is purposedly kept broad, to include a variety of methodologies which learn processes that generate graphs from a set of training examples. This research field originates from generative models of graphs for the theoretical understanding of graph properties, which have been studied thoroughly since the '50s in the field of graph theory. The first known generative model is the  Erd\"{o}s-R\'{e}nyi (ER) model \cite{erdos1959randomgraphs}. The ER model studies random graphs, where the connectivity among graph nodes is modeled independently. This model is useful to study theoretical properties of graphs, such as how the global connectivity of the graph evolves as its size grows. Another historical model of graphs is the  Watts-Strogatz (WS)  model \cite{watts1998smallworld}. The WS model concerns \quotes{small world} graphs, \ie graphs where it is possible to reach any other node in the graph with very short paths, regardless of their ize. This property arises in several real-world graphs such as social and electrical networks. Lastly, the Barab{\'a}si-Albert (BA) model \cite{barabasi1999prefatt}, which models the so-called \quotes{preferential attachment} property, where the connectivity potential of a node is directly proportional to its number of neighbors. While being useful to the study and understanding of graph properties, these models usually fail to generalize to real-world graph distributions, because they can only model one or a limited number of graph properties, and that their parameters cannot be learned from data in general. Other methods such as stochastic block models \citep{airoldi2008mixedstochasticblock}, exponential random graphs \citep{robins2007exponentialrandomgraphs}, and Kronecker graphs \citep{leskovec2010kronecker} can approximate more complex graph distributions such as graphs with communities, but are still limited to specific kinds of graphs and scale poorly to large datasets.
These limitations clearly highlight that, in order to generate graphs for practical applications, a more powerful class of learning models is needed. In this chapter, we review a variety of generative models of graphs based on Deep Learning approaches. The advantages of such models are the possibility to approximate complex distributions efficiently and effectively, and the flexibility to use different generative paradigms to adapt to the task at hand.

\subsection{The Challenges of Graph Generation}
The problem of generating graph structures from arbitrary distributions is arguably harder than their predictive modeling. Some of the challenges that need to be addressed when designing deep generative models of graphs are related to the complexity of graph spaces. In particular, we report the following:
\begin{itemize}
    \item \emph{size of graphs spaces}. Graphs spaces are combinatorial, and thus very large. For example, the size of the space of undirected graphs with $m$ nodes is ${m \choose 2} = \frac{m(m-1)}{2}$, which becomes very large even for graphs with a moderate number of nodes. Thus, trivial approaches such as exhaustive enumeration are generally intractable. Moreover, real-world graph distributions are usually defined over attributed graphs with variable size, which makes the search space even larger;
    \item \emph{discreteness of graphs spaces}. Graphs are discrete objects. This contrasts with the nature of neural networks, which can backpropagate only through continuous and differentiable objects. Hence, the learning process must be accomodated to work with discrete structures;
    \item \emph{sparsity of graphs spaces}. For most generative tasks, only very small subsets of graph space contain graphs with non-zero probability. Thus, any generative method should be designed to focus on regions where interesting graphs are contained, to avoid efficiency issues;
    \item \emph{complex dependencies}. Most real-world graph have hard structural constraints that are very difficult to enforce on a generative model (which is inherently stochastic). For example, cycle graphs are such that if only one edge is missplaced, the graph is not a cycle anymore;
    \item \emph{non-unique representations}. In general, graphs are invariant under node permutation. Thus, the same graph can be potentially represented by up to $n!$ possible adjacency matrices, depending on the node permutation. This poses constraints on the kind of graph representations a generative model can learn. For example, generative methods that map graphs into a latent space must take into account that different node permutations of the same graph must map to the same latent vector. While invariance to node permutation can be addressed by assuming an order of the graph nodes, this introduces the necessity to maintain order consistency among different graphs.
\end{itemize}

\subsection{Generative Tasks}
The family of \glspl{dgm} of graphs is flexible enough to model different kinds of generative tasks. Loosely following the taxonomy proposed by \citet{guo2020systematicreviewgenerativegraphs}, we distinguish two main tasks related to graph generation:
\begin{itemize}
    \item \emph{unconditional generation}, where the task is to explicitly learn a distribution $p(\Graph{g})$ over graphs, or some parametrized function that produces samples from it. Here, the term \quotes{unconditional} refers to the fact that the generation starts with drawing a vector $\Vector{z} \in \Real^z$ from some easy to sample prior distribution $p(\Vector{z})$, which is usually assumed to be an isotropic Gaussian or a uniform distribution;
    \item \emph{conditional generation}, where the aim is to learn a conditional distribution $p(\Graph{g} \given \Vector{y})$ with $\Vector{y} \in \Real^y$, or the corresponding parameterized sampling mechanism. The purpose of the conditioning vector is to drive the generative process towards producing a graph with desired characteristics. For example, one might want to generate a graph whose structure resembles that of a graph given as input to the \gls{dgm}. In this case, $\Vector{y}$ is the representation of the conditioning graph.
\end{itemize}
Hereafter, we consider the task of unconditional generation, where we assume access to a dataset of graphs $\Data = \Set{\Graph{g}_i}_{i=1}^n$. Notice from the notation that, at least for the moment, we focus on unattributed graphs. Broadly speaking, defining a \gls{dgm} of graphs requires to specify two components: a \emph{graph decoder}, which takes care of generating a graph, and an end-to-end generative framework used to optimize the model parameters. As regards the latter, common generative frameworks include \glspl{vae}, \glspl{gan}, and flow-based models \citep{rezende2015normalizingflows}. Here, we only describe approaches based on the first two.

\subsection{Graph Decoders}
The graph decoder is the architectural component that outputs some conditional distribution, which can be sampled to generate a graph. If the framwork in which the decoder is placed allows for inference (such as the \gls{vae}), the conditional is also learned with maximum likelihood; otherwise, (such as with \glspl{gan}) it is used only for sampling, and its parameters are optimized with adversarial training. Ideally, graph decoders should generate permutation invariant graphs; however, this is rarely the case. The major hurdle to devise permutation invariant graph decoders is their computational cost; even though some methods do exist \citep{ermon2020permutationinvariantgraphgeneration}, they are still too inefficient to be deployed in real world scenarios. Thus, in the following, we assume non-invariance.
One particular caveat that needs to be addressed during the training phase of a graph decoder is maintaining the differentiability of the architecture while still generating hard graph samples. This is critical especially in \gls{gan}-like architectures, where the discriminator must be trained with actual graphs. As with sequence generation with \glspl{rnn}, the same techniques (straight-trough gradient estimation, reparameterization, or even reinforcement learning-based techniques \cite{williams1992reinforce}) can be used for this purpose. There are two main paradigms to implement adaptive graph decoders, which we detail in the following.

\subsubsection*{One-shot Decoders}
This class of graph decoders outputs a dense probabilistic adjacency matrix $\ProbAdjMatrix{g} \in \Real^{n \times n}$, where $n$ is the maximum number of nodes allowed. The probabilistic matrix is sampled entry by entry to produce an actual adjacency matrix. The entries of the matrix are modeled as independent Bernoulli variables, which indicate the presence or absence of an edge. In some cases, the elements in the adjacency matrix $a_{ii}$ are modeled as independent Bernoulli variables that specify if a node belongs to the graph or not. In practice, the entries of the matrix are produced by a neural network with sigmoid outputs that predicts an $n \times n$ vector. Thus, the adjacency matrix can be sampled in parallel (hence the term \quotes{one-shot}). Two possible approaches to specify a one-shot decoder are:

\begin{itemize}
    \item \emph{graph-based} decoders, which require a graph representation $\Vector{z}$. In this case, the decoder models the conditional as follows:
    $$p(\ProbAdjMatrix{g} \given \Vector{z}) \approx \prod_{i=1}^n \prod_{j=1}^n p_{\Param}(a_{ij} \given \Vector{z}),$$
    where $p_{\Param}$ is a neural network that predicts the matrix from the graph representation;
    \item \emph{node-based} decoders, which require a matrix of node representations $\Matrix{Z} \in \Real^{n \times z}$. In this case, the decoder models the conditional as follows:
    $$p(\ProbAdjMatrix{g} \given \Matrix{z}) \approx \prod_{i=1}^n \prod_{j=1}^n p_{\Param}(a_{ij} \given \Vector{z}_i, \Vector{z}_j),$$
    where $i$ and $j$ range over the matrix rows. In this case, $p_{\Param}$ is a neural network that takes as input pair of node representations, and applies a sigmoid function to their dot product. The idea is that nodes that are close in representation space should be more likely to be connected.
\end{itemize}
One-shot approaches are usually fast to train and to take samples from. However, they are too simplistic, in that they assume the edges are generated independently (which is usually not the case for real-world graphs). Furthermore, the maximum number of nodes must be pre-specified, which makes them unable to generalize to larger graphs.

\subsubsection*{Autoregressive Decoders}
Autoregressive decoders assume that graphs are generated by some sequential process that involves its set of nodes. Specifically, the generative process is the following:
$$p(\Graph{s}) = \int p(\Graph{s}, \pi)\, d\pi = \int p(\Graph{s} \given \pi)\, q(\pi)\, d\pi,$$
where $\Graph{s}$ are sequences that generate graphs one component at a time, and the order of generation is given directly or indirectly by a nodes permutation drawn from a prior $q(\pi)$. The idea is to decompose the generating sequence autoregressively as follows:
$$p(\Graph{s}) = \int p(\Graph{s}_i \given \Graph{s}_{[<i]}, \pi)\, q(\pi)\, d\pi.$$
However, this requires to integrate over all $n!$ possible node permutations, which becomes intractable for moderately large graphs. An approximate solution to this issue is to assume some ordering and create the sequences before training, as a preprocessing step. Once the sequence are fixed, the chain-rule decomposition becomes tractable:
$$p(\Graph{s}) \approx p_{\Param}^{\pi}(\Graph{s}) = p_{\Param}^{\pi}(\Graph{s}_i \given \Graph{s}_{[<i]}).$$
Depending on the nature of the graph generating sequence, we distinguish four possible approaches to develop autoregressive graph decoders:
\begin{itemize}
    \item \emph{node-based} approaches decompose the graph as a sequence of actions performed on an initially empty graph. These action correspond to decisions such as whether to add a node to the existing graph, and which nodes it must be connected to. In this case, $\Graph{s}_{[<i]}$ is a vector that represents the current state of the graph. For all these models, one has two options as to how to implement the autoregressive network. One approach is to use a hierarchy of \glspl{rnn}: one keeps track of the state of the nodes added to the graph, and the other is responsible to connect newly added nodes to the current graph, given the state of the first \citep{you2018graphrnn}. The other choice is to update the state of the current graph with a \gls{dgn}, which is passed to the networks responsible of deciding which action to perform \citep{li2018learningdeepgmg};
    \item \emph{edge-based} approaches decompose the graph as a sequence of edges. To produce an ordered sequence of edges, one must first order its nodes, then label the nodes with progressive integers, and then sort its set of edges in lexicographic order. In this case, $\Graph{s}_{[<i]}$ represents the state of the graph indirectly, by keeping memory of the edges of the sequence already generated. These approaches are mostly implemented with \glspl{rnn} \citep{goyal2020graphgen,bacciu2019edgegraphgenrnn};
    \item \emph{rule-based} approaches can be applied in cases where the graph generation can be decomposed in a sequence of production rules over some known grammar (\eg molecules or computer programs) \citep{kusner2017grammarvae,dai2018sdvae}. In this case, the model generates a sequence of production rules to construct a desired graph;
    \item \emph{motif-based} approaches decompose the graph as a sequence (or even a tree) of \emph{motifs}, \ie very small and manageable subgraphs, which are combined together adaptively. Here, the challenge is mainly how to decompose the graphs into sequences of motifs,
\end{itemize}
A special kind of sequential decomposition for graphs is the \gls{smiles} linearization applied to molecules. We shall define the SMILES more precisely in Section \ref{subsec:smiles}; for the moment, it is sufficient to say that the SMILES encoding of a molecule is a string of ASCII characters that represents its structure. When a domain-specific linearization techniques such as SMILES are not available, the sequences representing the graph generative process are constructed based on some node ordering strategy. One general strategy to do so is to choose one node at random, then visit the graph nodes with a depth-first or breadth-first traversal. The order by which the nodes are visited is used to determine the order of the elements in the sequences. Clearly, this approach is not optimal since it heavily depends on the starting node, and may produce very different sequences for different starting node choices. However, it has been shown to work empirically \citep{you2018graphrnn,li2018learningdeepgmg,bacciu2019edgegraphgenrnn,goyal2020graphgen}. The pros and cons of autoregressive decoders are orthogonal to those of one-shot decoders: briefly, they allow to generate variable-sized graphs seamlessly, and they can model dependencies between nodes and edges by means of the autoregressive property. However, both training and sampling processes are slower in terms of computational time, because the graphs are reconstructed one sequence element at a time and not in parallel.

\subsection{Performance Evaluation}\label{sec:evaluation-generative-graphs}
The desired end result of training a generative model is that the structure of samples generated by the network should resemble that of graphs in the training set, without being identical. This is a very different setting with respect to predictive tasks, since the samples of a generative model do not exist until they are generated. Hence, one does not have access to held-out ground truth values to measure generalization. Furthermore, the model can very easily obtain deceptive-looking performances by learning to replicate training graphs exactly, or repeating the same graph typology over and over. Thus, one critical aspect of using generative models of graphs is how to assess performances. Below, we define two broad classes of metrics that allow the evaluation of graph models, assuming the availability of a training sample $\TrainingSample \subseteq \Data = \Set{\Graph{g}}_{i=1}^n$ , and a collection\footnote{Here, we use the term \quotes{collection} to indicate a multiset, meaning that it can possibly contain duplicate elements.} $\GeneratedSample = \Set{\Graph{g'}}_{i=1}^m$ of samples generated by the model.

\subsubsection*{Quantitative Metrics}
\emph{Quantitative} metrics measure the rate at which the generative model produces diverse and heterogeneous graphs, without taking into account structural similarity. The three main quantitative matrics considered in the literature are:
\begin{itemize}
    \item \emph{novelty} measures the ratio of generated samples that are not training samples. A high novelty indicates that the model has not learned to replicate training graphs. Formally, it is measured as $1 - \frac{|\GeneratedSample \Inter \TrainingSample|}{|\TrainingSample|}$;
    \item \emph{uniqueness} measures the ratio of unique graphs with respect to the total number of graphs generated. A low uniqueness rate might indicate that the model has overfit one specific typology of graph. To calculate uniqueness, one first checks every graph for isomorphism with every other graph in the generated sample, removing them. If the resulting set of unique graphs is indicated by $\UniqueSample$, uniqueness can be simply calculated as $\frac{|\UniqueSample|}{|\GeneratedSample|}$.
    \item \emph{validity} measures the ratio of generated graphs that respect some validity constraint, out of the total number of graphs generated. To calculate validity, one needs to check wheter every generated graph satisfies some structural constraint or not (\eg the presence of a cycle). If $\ValidSample$ is the collection of graphs that satisfy the structural constraints, validity is calculated as $\frac{|\ValidSample|}{|\GeneratedSample|}$. This metric is particularly useful in molecular generation tasks, since chemically invalid molecules are useless. When assessing validity is required, novelty and uniqueness are usually conditioned on validity first, meaning that the all the invalid graphs are removed from $\GeneratedSample$ before calculating these two metrics.
\end{itemize}

\subsubsection*{Qualitative Metrics}
Quantitative metrics give only one side of the spectrum relatively to how a generative model is performing. For example, assessing novelty alone might be misleading, since a high novelty rate can also be associated to underfitting (meaning that the model generates graphs very different from the training sample, which are trivially novel). Thus, a proper evaluation of generative models must also include a series of metrics that consider the structural properties of the generated graphs. We call such metrics \emph{qualitative}. The framework under which qualitative metrics are assessed consists of comparing the empirical distribution of a certain graph property in the training sample, to the empirical distribution on the generated sample. Given a generic graph with $d$ nodes, coming from one of the two samples indifferently, a relevant subset of such properties includes:
\begin{itemize}
    \item node degree distribution, that is, an $d$-dimensional vector where each position contains the degree of the corresponding node. Notice the the length of the vector may differ across different graphs, since their number of nodes may change;
    \item clustering coefficient distribution. The clustering coefficient of a node $v$ is defined as the ratio between the number of actual connections between neighbors of $v$ out of the total number of possible connections. In other words, it is a relative measure of how many \quotes{closed triangles} (fully connected graphs with three nodes) the node is part of. Similarly to the node degree distribution, it consists of an $d$-dimensional vector where each position contains the clustering coefficient of the corresponding node;
    \item number of nodes of the graph, which is a single integer;
    \item number of edges of the graph, which is again a single integer;
    \item average orbit counts. Orbits are subgraph with 4 nodes. Counting orbits in a graph can be viewed as a generalization of the clustering coefficient to 4-node subgraphs instead of triangles. In practice, it consists in a $dk$-dimensional vector, where $k$ is the number of orbits considered;
    \item \gls{nspdk} \citep{costa2010nspdk}, which measures the similarity between two graphs by counting the number of matching induced subgraphs between them. The subgraphs are derived node-wise, by considering neighborhoods of a node comprising nodes at increasing path lengths. Differently from the other qualitative metrics, the \gls{nspdk} provides a global measure of similarity between graphs, since it is based on multiple subgraph matchings. In practice, it is a vector of length $m-1$ ($n-1$, respectively), where each position measures the similarity of the graph with another graph in the sample.
\end{itemize}
Once the graph properties are calculated for each graph in the sample, there are two options to measure the distance between the empirical distribution of the training sample versus the generated sample, based on their respective number of elements:
\begin{itemize}
    \item if $n = m$, one can compute their empirical \gls{kld} as follows:
    $$\EKLD{\Prop(\GeneratedSample)}{\Prop(\TrainingSample)} = \frac{1}{n} \sum_{i=1}^{n} \Prop(\Graph{g}_{(i)}) \log \Par{\frac{\Prop(\Graph{g'}_{(i)})}{\Prop(\Graph{g}_{(i)})}},$$
    where $\Prop$ is one of the properties mentioned above;
    \item if $n \neq m$, one can either concatenate all the values of the property for each each in the sample, and then fit a histogram with an equal number of bins to make their length match in order to apply the empirical \gls{kld}. Another, more general, approach to compare distribution when the two samples have different lengths is to compute their \gls{mmd} \citep{gretton2012mmdkernel}. Intuitively, the \gls{mmd} measures the distance between two distributions as the sum of the distances of between their matching moments. The computation of these distances can be generalized to an infinite space of moments by applying a kernel trick \citep{smola2008kernels}.
\end{itemize}
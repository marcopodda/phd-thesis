\chapter{Deep Learning in Structured Domains} \label{ch:deep-learning-structures}
A \keyword{structured domain} is a data domain whose elements are formed by a set of atomic \emph{entities}, and the \emph{relations} between them. Structured data is common in several fields, such as biology, chemistry, finance, social networks, and many more. Typical examples are sequences such as time-series data, or graphs representing molecular structures. One distinctive characteristic of structured data is that it has \keyword{variable size}, meaning that the number of entities composing the datum is not fixed in general. This constitutes a serious limitation for traditional \gls{ml} models, which are designed to work with \quotes{flat} data, \ie collections of fixed-size vectors. In principle, they can be adapted to work with variable-sized data by incorporating the structure of the data to the input vectors as additional features. While useful to some extent, this approach requires to decide \apriori which features are needed to solve a task. This, in turn, requires a level of domain expertise that is not always available for many interesting problems. In contrast, \glspl{nn} (and Deep Learning models more so) are able to learn which features are useful to solve a task adaptively from data, without the need of feature engineering. Thus, the general idea is to provide the structured data directly as an input to the network, which automatically learns the needed features and the task, guided by the learning process. In this chapter, we present a class of \glspl{nn} that are able to handle variable-sized inputs for learning in structured domains.

\section{Graphs}\label{sec:graphs}
The elements of structured domains can be described in a compact and convenient notation using the general formalism of \keyword{graphs} \citep{bondy1976graph}. Informally, a graph is a collection of \emph{vertices} (the entities) connected through a collection of \emph{edges} (the relations). In the literature, vertices are sometimes called \emph{nodes}, while edges are also referred to as \emph{arcs} or \emph{links}. Formally, a graph with $n$ vertices is a pair
$$\Graph{g} =\langle \Nodes{g}, \Edges{g}\rangle,$$
where $\Nodes{g}= \Set{v_1, v_2, \ldots, v_n}$ is its set of vertices, and $\Edges{g} = \Set{\{u, v\} \mid u, v \in \Nodes{g}}$ is its set of edges. In a graph, $\Edges{g}$  specifies the graph \emph{structure}, that is, the way vertices are interconnected. Notice that the pair $\Set{u,v}$ is unordered: in this case, the graph is called \keyword{undirected}. Figure \ref{fig:undirected-graph} shows a visual representation of an undirected graph.
Given an edge $\Set{u, v} \in \Edges{g}$, $u$ and $v$ are called its \emph{endpoints}, and are said to be \emph{adjacent}. Alternatively, we say that $\Set{u, v}$ is \emph{incident} to $u$ and $v$. Edges of the form $\Set{v,v}$ that connect a vertex to itself are called \emph{self-loops}.
\begin{figure*}
    \begin{subfigure}[b]{0.38\linewidth}
        \centering
        \resizebox{.9\textwidth}{!}{\input{Figures/Chapter3/01a-undirected-graph.tex}}
        \caption{An undirected graph.}
        \label{fig:undirected-graph}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \resizebox{.9\textwidth}{!}{\input{Figures/Chapter3/01b-directed-graph.tex}}
        \caption{A directed graph.}
        \label{fig:directed-graph}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\linewidth}
        \centering
        \resizebox{.9\textwidth}{!}{\input{Figures/Chapter3/01c-bipartite-graph.tex}}
        \caption{A bipartite graph.}
        \label{fig:bipartite-graph}
    \end{subfigure}
    \caption{Three examples of graphs.}
\end{figure*}
Graphs where it is possible to have more than one edge between a pair of vertices are called \emph{multigraphs}. In this work, we restrict ourselves to the case where there is at most one possible edge between two vertices.

\paragraph{Directed Graphs}
A \keyword{directed graph} is one where he edges are ordered pairs of vertices, or equivalently one where $\Edges{g}\subseteq \Nodes{g}\times \Nodes{g}$. A directed edge is written as $(u, v)$, meaning that it goes from vertex $u$ to vertex $v$. An example of directed graph is shown in Figure \ref{fig:directed-graph}. Given a directed graph $\Graph{g}$ and one of its vertices $v$, the set of all vertices from which an edge reaches $v$ is called \emph{predecessors} set, and is defined as $\Pred(v) = \Set{u \in \Nodes{g} \mid (u,v) \in \Edges{g}}$. The cardinality of the predecessors set is called the \emph{in-degree} of the vertex, and we indicate it as $\Fun{degree}_{in}(v)$. Analogously, the set of all vertices reached by an edge from $v$ is called the \emph{successors} set, and is defined as $\Succ(v) = \Set{u \in \Nodes{g}\mid (v,u) \in \Edges{g}}$. Its cardinality is called the \emph{out-degree} of the vertex, and indicated as $\Fun{degree}_{out}(v)$. The \emph{neighborhood} (or \emph{adjacency set}) of a vertex $v$ is the union of the predecessors and successors sets: $\Neigh(v) = \Pred(v) \bigcup \Succ(v)$. Alternatively, one can view the neighborhood as a function $\Neigh: \Nodes{g}\shortrightarrow 2^{\Nodes{g}}$ from vertices to sets of vertices. The cardinality of the neighborhood is called the \keyword{degree} of the vertex, indicated as $\Fun{degree}(v)$.  In this work, we consider all graphs directed unless otherwise specified. Undirected graphs are thus implicitly transformed into directed graphs with the same vertices, where the set of edges contains the edges $(v,u)$ and $(u,v)$ if and only if $\{u,v\}$ is an edge of the undirected graph.

\paragraph{Bipartite Graphs}
A graph $\Graph{g}$ is called \keyword{bipartite} if we can split $\Nodes{g}$ in two disjoint subsets $\Nodes{g}^{+}$ and $\Nodes{g}^{-}$, such that $(u, v) \in \Edges{g}$ if and only if either $u \in \Nodes{g}^{+}$ and $v \in \Nodes{g}^{-}$, or $v \in \Nodes{g}^{+}$ and $u \in \Nodes{g}^{-}$. Figure \ref{fig:bipartite-graph} shows an example of bipartite graph, where
$\Nodes{g}^{+} = \Set{v_1, v_2, v_3}$ and $\Nodes{g}^{-} = \Set{v_4, v_5, v_6}$.

\paragraph{Walks, Paths, and Cycles}
Let $\Graph{g}$ be a graph. A \emph{walk} of length $l$ is any sequence of $l$ vertices $\Par{v_1, v_2, \ldots, v_l}$, where each pair of consecutive vertices is adjacent, \ie $\Par{v_i, v_{i+1}} \in \Edges{g}$, $\forall i= 1, \ldots, i-1$. A \emph{path} of length $l$ from vertex $u$ to vertex $v$ is a walk such that $v_1 = u$ and $v_l = v$, where each vertex appears exactly once in the sequence. If, given two vertices $u, v \in \Nodes{g}$ such that $u \neq v$, there exists a path between them, we say they are \emph{connected}, or that $v$ is reachable from $u$. Otherwise, we say they are \emph{disconnected}, or that $v$ is unreachable from $u$. A \emph{shortest path} from a node $u$ to a node $v$ is the path, among all paths from $u$ to $v$, with the smallest length. We indicate it with the notation $\Path{u}{g}{v}$. A graph is called \emph{connected} if every vertex is connected to any other vertex (ignoring the direction of the edges); otherwise it is called \emph{disconnected}. A \emph{cycle}, or \emph{loop}, of length $l$ is a walk where $v_1 = v_l$, and all the other vertices appear once in the sequence. Graphs that do not contain cycles are called \emph{acyclic}.

\paragraph{Trees and Sequences}
A graph $\Tree{t}$ is called a \keyword{tree} if its set of edges defines a \emph{partial order} over the set of vertices, implying that it is also connected and acyclic. The vertices of a tree are called \emph{nodes}. Given an edge $(u, v) \in \Edges{t}$, we call $u$ the \emph{parent} of $v$ and $v$ the \emph{child} of $u$. The set of children of a node $v$ is indicated with the notation $\Fun{ch}(v)$. In a tree, every node has exactly one parent, with the exception of a node called \emph{root} or \emph{supersource}, which has no parent node. A tree is \emph{positional} if we can distinguish among the children of a node, \ie if there exist a consistent ordering between them. Trees have a recursive structure: every node $v \in \Nodes{t}$ is itself the root of a tree, called \emph{sub-tree of} $\Tree{t}$ \emph{rooted at v}, and indicated as $\Tree{t}_v$. If $\Tree{t}_v$ contains only $v$, $v$ is called a \emph{leaf}. Trees encode \emph{hierarchical} relationships among nodes; an example of tree with five nodes is shown in Figure \ref{fig:tree}.

A graph $\Seq{s}$ with $n$ vertices is called a \emph{sequential graph}, or \keyword{sequence} of length $n$, if its set of edges defines a \emph{total order} over the set of vertices, which allows us to represent the set of vertices in an ordered fashion as $\Nodes{s} = (v_1, v_2, \ldots, v_n)$. In a sequence, the vertices are usually called \emph{elements}. A sequence can be viewed as a special case of tree with only one leaf. Sequences are useful to encode \emph{sequential} relationships among elements; Figure \ref{fig:sequence} shows an example of a sequence of four elements.
\begin{figure*}
    \begin{subfigure}[b]{0.60\linewidth}
        \centering
        \resizebox{.8\textwidth}{!}{\input{Figures/Chapter3/02a-sequence.tex}}
        \caption{A sequence.}
        \label{fig:sequence}
    \end{subfigure}
    \begin{subfigure}[b]{0.39\linewidth}
        \centering
        \resizebox{.70\textwidth}{!}{\input{Figures/Chapter3/02b-tree.tex}}
        \caption{A tree.}
        \label{fig:tree}
    \end{subfigure}
    \caption{Special classes of graphs.}
\end{figure*}

\paragraph{Subgraphs and Induced Subgraphs} A \keyword{subgraph}
$\Graph{h} = \langle \Nodes{h}, \Edges{h} \rangle$ of a graph $\Graph{g}$ is any graph for which $\Nodes{h} \subseteq \Nodes{g}$ and  $\Edges{h} \subseteq \Edges{g}$. If $\Edges{h} = (\Nodes{h} \times \Nodes{h}) \Inter \Edges{g}$, or equivalently, if $\Nodes{h}$ contains only vertices that are endpoints in $\Edges{g}$, the resulting subgraph is called \keyword{induced subgraph}. We indicate the subgraph of $\Graph{g}$ induced by $\Graph{h}$ with the notation $\InducedSubgraph{g}{\Graph{h}}$.

\subsection{Attributed Graphs} \label{sec:attr-graphs}
Real-world instances of graphs usually carry out other information besides structure, generally attached to their vertices or edges. As an example, consider the graph representation of a molecule, in which vertices are usually annotated with an atom type, and edges are annotated with a chemical bond type. Given a graph $\Graph{g}$ with $n$ vertices and $m$ edges, we define the associated graph with additional information content, and we call it an \keyword{attributed graph}, as a triplet $\langle \Graph{g}, \chi, \xi \rangle,$ where $\chi: \Nodes{g} \rightarrow \Real^d$ is a mapping from the space of vertices to a space of $d$-dimensional \emph{vertex features}, and $\xi: \Edges{g}\rightarrow \Real^{e}$, is a mapping from the space of edges to a space of $e$-dimensional \emph{edge features}. The values of these features can be either discrete (in which case the features are called \emph{labels} and encoded as one-hot vectors) or continuous vectors. In most cases, we omit to define $\chi$ and $\xi$ explicitly, and provide the vertex and edge features directly as sets, \eg $\Vector{x}_{\Graph{g}}= \Set{\Elem{x}{v} \in \Real^d \mid v \in \Nodes{g}}$ for the vertex features, and $\Vector{e}_{\Graph{g}} = \Set{\Elem{e}{u,v} \in \Real^e \mid (u, v) \in \Edges{g}}$ for the edge features. In this case, an attributed graph is a tuple:
$$\AttrGraph{g} = \VectorGraph{x}{g}.$$
If some ordering of the vertices and edges is assumed, we can represent equivalently $\chi$ as a matrix $\Matrix{X}_{\Graph{g}} \in \Real^{n \times d}$ where the $i$-th row contains the vertex features of the $i$-th vertex; analogously, we can define $\xi$ as a matrix of edge features $\Matrix{E}_{\Graph{g}} \in \Real^{m \times e}$. In this case, an attributed graph is a triple:
$$\AttrGraph{g} = \MatrixGraph{x}{g}.$$

\subsection{Isomorphisms, Automorphisms, and Canonization} \label{sec:isomorphisms}
An \keyword{isomorphism} between two graphs $\Graph{g}$ and $\Graph{h}$ is a bijection $\Isomorph: \Nodes{g}\rightarrow \Nodes{h}$ such that $(u,v) \in \Edges{g}$ if and only if $(\Isomorph(u),\Isomorph(v)) \in \Edges{h}$. Intuitively, graph isomorphism formalizes the notion of \emph{structural equivalence} between graphs, in the sense that two isomorphic graphs are structurally equivalent, regardless of the information they contain. Figure \ref{fig:isomorphism} shows two isomorphic graphs and their corresponding $\Isomorph$ bijection. An \keyword{automorphism} $\pi: \Nodes{g}\rightarrow \Nodes{g}$ is an isomorphism between $\Graph{g}$ and itself. Since $\pi$ is essentially a permutation of the vertex set, it follows that a graph always has at most $n!$ possible automorphisms. Intuitively, and similarly to graph isomorphism, graph automorphisms convey the notion that the structure of a graph is invariant to permutation of the vertices and edges. An automorphism $\pi$ on an example graph is shown
in Figure \ref{fig:automorphism}. Related to isomorphisms and automorphisms is the problem of \keyword{graph canonization}, where a canonical ordering (or form) of the graph vertices is sought, such that every graph $\Graph{h}$ isomorph to a given graph $\Graph{g}$ has the same canonical form. As we shall see, (approximate) graph canonization plays a role in the usage of graph within practical contexts; conversely, many techniques described in this work try to avoid representing graphs in canonical form, in favor of permutation-invariant representations.

\begin{figure*}
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \resizebox{.9\textwidth}{!}{\input{Figures/Chapter3/03a-isomorphism.tex}}
        \caption{Isomorphism.}
        \label{fig:isomorphism}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \resizebox{.9\textwidth}{!}{\input{Figures/Chapter3/03b-automorphism.tex}}
        \caption{Automorphism.}
        \label{fig:automorphism}
    \end{subfigure}
    \caption{An example of isomorphism and automorphism.}
\end{figure*}

\subsection{Graphs Matrices} \label{sec:adj-matrix}
One compact way to represent the structure of a graph is through its  \keyword{adjacency matrix}. Given a graph $\Graph{g}$ with $n$ vertices and $m$ edges, the entries $a_{ij}$ of its corresponding  adjacency matrix $\AdjMatrix{g} \in \Real^{n \times n}$ are defined as follows:
\[
a_{ij} =
    \begin{cases}
        1  & \text{if } (v_i, v_j) \in \Edges{g}\\
        0  & \text{otherwise.}
    \end{cases}
\]
Note that the diagonal entries $a_{ii}$ of the adjacency matrix specify the presence  (or absence) of self-loops. Another interesting property of the adjacency matrix is that  it is symmetric for undirected graphs, which implies $a_{ij} = a_{ji},$ $\forall\, i, j = 1, \ldots, n$. Adjacency matrices make some calculations of graph properties particularly convenient: for example, the in-degree and out-degree of a vertex $v_j \in \Nodes{g}$
can be obtained by performing row-wise and column-wise sums on $\AdjMatrix{g}$:
$$
\Fun{degree}_{in}(v_j) = \sum_{i=1}^n a_{ji} \quad \quad \Fun{degree}_{out}(v_j) = \sum_{i=1}^n a_{ij}.
$$
Adjacency matrices are also useful to understand concepts such as graph automorphisms: in fact, an automorphism of $\Graph{g}$ corresponds to a permutation of the columns or rows of the adjacency matrix (but not both). Other useful matrices to represent properties of graphs are the \emph{Laplacian matrix} $\LaplacianMatrix{g} \in \Real^{n \times n} = \DegreeMatrix{g} - \AdjMatrix{g}$, and the \emph{symmetric normalized Laplacian matrix} $\NormLaplacianMatrix{g} \in \Real^{n \times n} = \Identity - \DegreeMatrix{g}^{-\frac{1}{2}}\AdjMatrix{g}\DegreeMatrix{g}^{-\frac{1}{2}}$. In both definitions, the matrix  $\DegreeMatrix{g} \in \Real^{n \times n}$ is the \emph{degree matrix}, where all entries are zero except the diagonal entries, for which $d_{ii} = \Fun{degree}(v_i)$. These matrices provide information about the graph connectivity through their eigenvalues and eigenvectors.

\section{The Adaptive Processing of Structured Data}
The processing of structured data for learning purposes is carried out by a \keyword{structural transduction}, namely a function $\Transduction: \Cal{x} \shortrightarrow \Cal{y}$ where $\Cal{X}$ and $\Cal{Y}$ are structured domains. When the structural transduction is implemented by a (deep) \gls{nn}, it is \emph{adaptive}, \ie it is learned from data. A structural transduction can be decomposed as $\Transduction = \EncTrans \circ \OutTrans$, where:
\begin{itemize}
    \item $\EncTrans$ is called \emph{encoding function} or \emph{state transition function} that is applied separately to each element of the structure. The output of the encoding function is a structure isomorphic to that in input, where the elements are now \keyword{state vectors}. Intuitively, a state vector encodes the information of the element and of the elements it depends on;
    \item $\OutTrans$ is called \emph{output function}, which computes an output from the state vectors.
\end{itemize}
The output function of the structural transduction is task-dependent. Considering a supervised setting and a generic graph dataset $\Data$ consisting of $n$ training pairs, we distinguish two learning problems:
\begin{itemize}
    \item in \emph{structure-to-structure} tasks, the dataset is composed of pairs of attributed graphs $\Data = \{(\PatternGraph{g}{i}, \PatternGraph{h}{i})\}_{i=1}^n$, where $\AttrGraph{g} = \VectorGraph{x}{g}$ is an input graph, $\AttrGraph{h} = \VectorGraph{y}{h}$ is an output graph, and the two underlying unattributed graphs $\Graph{g}$ and $\Graph{h}$ are isomorphic under a bijection $\vartheta$. The task is to predict the target associated to an output graph vertex, given the corresponding features of its isomorphic vertex. The objective function minimized in these tasks is the following:
    $$\argmin_{\Param} \frac{1}{n} \sum_{i=1}^n -\log p(\PatternGraph{h}{i} \given \PatternGraph{g}{i}) = -\frac{1}{n} \sum_{i=1}^n \sum_{v \in \Nodes{g_{(i)}}} -\log p(\Elem{y}{\vartheta(v)} \given \Elem{x}{v});$$
    \item in \emph{structure-to-element} tasks, the dataset has the form  $\Data = \{(\PatternGraph{g}{i}, \PatternVector{y}{i})\}_{i=1}^n$, where $\AttrGraph{g} = \VectorGraph{x}{g}$ is an input graph and $\Vector{y} \in \Real^y$ is an output vector. The task is to predict a single output vector (or scalar) from the structure and the features of $\AttrGraph{g}$. The  objective function minimized in these tasks is the following:
    $$\argmin_{\Param} \frac{1}{n} \sum_{i=1}^n - \log p(\PatternVector{y}{i} \given \PatternGraph{g}{i}).$$
    To learn structure-to-element tasks, the output function must compress the states of each element of the structure into a global output vector representing the entire structure, which is compared to the target $\Vector{y}$. To do so, there are several strategies; in general, one could pick a single state vector as a representative for the whole structure, or compute a summary of the entire structure using all the available state vectors. The function that implements the latter strategy is usually termed \keyword{readout}.
\end{itemize}

As anticipated, one important issue that structural transductions need to address is how to deal with variable-sized inputs. The solution is to apply the same state transition function (that is, with the same adaptive parameters) \emph{locally} to every element in the structure, rather than to apply it one time to the overall structure. This process is similar to the localized processing of images performed by \glspl{cnn}, which works by considering a single pixel at a time, and combining it with some finite set of nearby pixels. This local property of the structural transduction is often referred to as \emph{stationarity}. An interesting byproduct of using stationary transductions is that they require a smaller number of parameters with respect to non-stationary ones, since the network weights are shared across the structure. At the same time, using stationary transductions also requires additional mechanisms to learn from the global structure of the datum (such as readouts in the case of structure-to-element tasks), rather than only locally.

In the following sections, we present three specific \gls{nn} architectures that implement transductions over structured data: recurrent neural networks, which process data represented as sequences; recursive neural networks, which process hierarchical data such as trees; and deep graph networks, which process general graphs.

\section{Recurrent Neural Networks}\label{sec:rnns}
A \gls{rnn} is a \gls{nn} architecture able to process sequences. Let $\Seq{S}$ be an attributed sequence of length $m$ with elements $\Nodes{s} = (v_1, v_2, \ldots, v_m)$, and let $\GraphFeatures{x}{s} = \Par{\Elem{x}{1}, \Elem{x}{2}, \ldots, \Elem{x}{m}}$ be its element features. Here, we slightly abuse the notation $\Elem{x}{v_t}$ in favor of $\Elem{x}{t}$, since sequence elements are ordered. In supervised settings, we also assume an isomorphic attributed sequence $\Graph{R}$ with targets $\GraphFeatures{y}{r} = \Par{\Elem{y}{1}, \Elem{y}{2}, \ldots, \Elem{y}{m}}$. The state transition function of a \gls{rnn}, applied locally to each sequence element, has the following general form:
\begin{align*}
    \Elem{h}{t} =
    \begin{cases}
        \boldsymbol{0} & \mathrm{if}\; t = 0\\
        \EncTrans(\Elem{x}{t}, \Elem{h}{t-1}) & \mathrm{otherwise},
    \end{cases}
\end{align*}
where $\Elem{h}{t} \in \Real^h$ is a state vector, also known as \keyword{hidden state}, and $\boldsymbol{0}$ is a zero vector. The calculation of the hidden state performed by the state transition function $\EncTrans$ is \emph{recursive}: to compute a hidden state for the $t$-th element of the sequence, the hidden state of the previous element must be known in advance. Thus, the state computation is a sequential process, where the input sequence is traversed in order one element at a time, and the hidden state is updated as a function of the current sequence element and the hidden state at the previous step. To avoid infinite recursion, the hidden state is initialized with the zero vector $\Elem{h}{0} = \boldsymbol{0}$. As the sequence is traversed, the hidden state maintains a \emph{memory} of the past elements of the sequence. The presence of a memory mechanism makes \glspl{rnn} very powerful: in fact, it has been proved that finite-size \glspl{rnn} can compute any function computable with a Turing machine \citep{siegelmann1995rnnturing}. As with \glspl{cnn}, the development of \glspl{rnn} started in the early '90s, and they have recently been rediscovered within the \gls{dl} framework after their success, especially in \gls{nlp}-related tasks.

\subsection{Training}
Given an attributed sequence $\Seq{S}$ with features $\Vector{x}_{\Seq{s}}$, the original implementation of the state transition function of a \gls{rnn} is defined as follows\footnote{For the rest of this chapter, biases are omitted for readability.}:
\begin{align*}
    \Elem{h}{t} &= \tanh\Par{\Matrix{W}\Elem{x}{t} + \Matrix{U}\Elem{h}{t-1}},\; \forall t=1, \ldots, m.
\end{align*}
The above is also called \keyword{recurrent layer}. The weight matrices $\Matrix{W} \in \Real^{d \times h}$ and $\Matrix{U} \in \Real^{h \times h}$, are shared among the sequence elements according to the stationarity property. For this reason, it is often said that the network is \emph{unrolled} over the sequence. In structure-to-structure tasks, once the states of the elements are calculated, an element-wise output is computed as:
\begin{align*}
    \Elem{o}{t} = g(\Elem{h}{t}),\; \forall t=1, \ldots, m,
\end{align*}
where $g$ can be any neural network such as one simple output layer or a more complex downstream network. Similarly, in structure-to-element tasks, a single output is computed from the last hidden state of the sequence:
\begin{align*}
    \Vector{o} = g(\Elem{h}{m}).
\end{align*}
Figure \ref{fig:rnn-unfold} shows a \gls{rnn} in compact form, as well as unrolled over a sequence of length $m$ for a structure-to-structure task. The error of the network during training is computed by comparing the output of the network for each sequence element $\Elem{o}{t}$ to the isomorphic sequence element $\Elem{y}{t}$ in the target sequence with the loss function $\Loss$, which is summed up over all the elements in the sequence. Notice that it is possible to stack multiple recurrent layers and create deep \glspl{rnn} by feeding the hidden state produced the recurrent layer to a subsequent recurrent layer, other than to the next step of the recurrence. In these cases, the output is computed after the last recurrent layer.
\begin{figure*}[h!]
    \begin{subfigure}[b]{0.39\linewidth}
        \centering
        \resizebox{.7\textwidth}{!}{\input{Figures/Chapter3/04a-rnn.tex}}
        \caption{}
        \label{fig:rnn}
    \end{subfigure}
    \begin{subfigure}[b]{0.59\linewidth}
        \centering
        \resizebox{.8\textwidth}{!}{\input{Figures/Chapter3/04b-rnn-unfold.tex}}
        \caption{}
        \label{fig:rnn-unfold}
    \end{subfigure}
    \caption{({\scriptsize A}): An example of recurrent neural network that can learn a structure-to-structure task. ({\scriptsize B}): the same network unfolded over a training pair of sequences of length $m$.}
    \label{fig:rnn-example}
\end{figure*}
\glspl{rnn} can be also adapted to learn structure-to-structure distributions of the kind $p(\AttrGraph{R} \given \AttrGraph{S})$, where the underlying unattributed graphs $\Seq{r}$ and $\Seq{s}$ are not isomorphic, \ie when the lengths of the input and target sequence do not match. The usual way to proceed in this case is to use two \glspl{rnn}: one acts as an encoder, computing a fixed-size representation of the input $\AttrGraph{S}$ (for example, its last hidden state as seen above); the other acts as a decoder of the target sequence $\AttrGraph{R}$, conditioned on the input representation. The conditioning is achieved by initializing the hidden state of the decoder \gls{rnn} with the encoding of the input computed by the encoder \gls{rnn}. These types of architectures are called \gls{s2s} models.

\glspl{rnn} are usually trained with \gls{bptt} \citep{werbos1988backpropthroughtime}, a variant of vanilla backpropagation that propagates the gradient both from the output layer to the recurrent layer, and backwards along the sequence elements. One \gls{bptt} update requires $\mathcal{O}(mb)$ computation, where $m$ is the sequence length and $b$ is the size of the mini-batch given to the optimizer. This can become computationally inconvenient for long sequences, and can lead to instabilities like gradient vanishing. Thus, in practical settings, faster \gls{bptt} variants are often used, such as truncated \gls{bptt} \citep{jaeger2002truncatedbptt}.

\subsection{Gated Recurrent Neural Networks}
Vanilla \glspl{rnn} struggle to learn with long sequences. This issue has been documented several times in the literature (see \eg \citep{bengio1994learninglongtermdependenciesdifficult}), and is mostly due to the gradient vanishing or exploding problems. While gradient exploding can be dealt with gradient clipping, gradient vanishing is more hard to tackle. Several workarounds have been proposed to overcome such limitation; the most adopted in practical settings exploits a form of information \emph{gating}, that is, by controlling the information flow inside the recurrent layer. In particular, it might be useful for the network to \emph{forget} useless information, or to \emph{reset} the hidden state when some kind of knowledge has already been processed. Gated mechanisms fulfill this purpose adaptively, driven by data. The most used \gls{rnn} variant that implements gating mechanisms is the \gls{lstm} \citep{hochreiter1997lstm}.
An \gls{lstm} is composed of a \emph{cell} $\Vector{c} \in \Real^h$, an \emph{input gate} $\Vector{i} \in \Real^h$, a \emph{forget gate} $\Vector{f} \in \Real^h$, and an \emph{output gate} $\Vector{g} \in \Real^h$. Assuming an input sequence element $\Elem{x}{t} \in \Real^d$, the hidden state $\Elem{h}{t} \in \Real^h$ of a \gls{lstm} is computed as follows:
\begin{align*}
    \Elem{f}{t} &= \sigmoid\Par{\Matrix{W}_{1}\Elem{x}{t} + \Matrix{U}_{1}\Elem{h}{t-1}}\\
    \Elem{i}{t} &= \sigmoid\Par{\Matrix{W}_{2}\Elem{x}{t} + \Matrix{U}_{2}\Elem{h}{t-1}}\\
    \Elem{g}{t} &= \sigmoid\Par{\Matrix{W}_{3}\Elem{x}{t} + \Matrix{U}_{3}\Elem{h}{t-1}}\\
    \Vector{\tilde{c}}_{(t)} &= \tanh\Par{\Matrix{W}_{4}\Elem{x}{t} + \Matrix{U}_{4}\Elem{h}{t-1}}\\
    \Elem{c}{t} &= \Elem{f}{t} \odot \Elem{c}{t-1} + \Elem{i}{t} \odot \Vector{\tilde{c}}_{(t)}\\
    \Elem{h}{t} &= \Elem{g}{t} \odot \tanh(\Elem{c}{t}),
\end{align*}
where $\odot$ is the Hadamard (element-wise) product between matrices. Notice that the weight matrices $\Matrix{W}_i \in \Real^{d \times h}$ and $\Matrix{U}_i \in \Real^{h \times h}$ with $i=1, \ldots, 4$ are all different. In short, the input gate controls how much of the input is kept, the forget gate controls how much information about previous elements is kept, and the output gate controls how much of the two should be used to compute the hidden state. While powerful, a single \gls{lstm} requires eight weight matrices; thus, it is computationally expensive to train. The \gls{gru} \citep{cho2014gru} gating mechanism is a lightweight alternative to \gls{lstm} which uses less parameters, though it is slightly less powerful \citep{gruber2020gruspecificlstm}. A \gls{gru} uses two gates, an \emph{update} gate $\Vector{u} \in \Real^h$ and a \emph{reset} gate $\Vector{r} \in \Real^h$, and computes the hidden state as follows:
\begin{align*}
    \Elem{u}{t} &= \sigmoid\Par{\Matrix{W}_{1}\Elem{x}{t} + \Matrix{U}_{1}\Elem{h}{t-1}}\\
    \Elem{r}{t} &= \sigmoid\Par{\Matrix{W}_{2}\Elem{x}{t} + \Matrix{U}_{2}\Elem{h}{t-1}}\\
    \Vector{\tilde{h}}_{(t)} &= \tanh\Par{\Matrix{W}_{3}\Elem{x}{t} + \Matrix{U}_{3}(\Elem{r}{t} \odot \Elem{h}{t-1})}\\
    \Elem{h}{t} &= (\Ones - \Elem{u}{t}) \odot \Elem{h}{t-1} + \Elem{u}{t} \odot \Vector{\tilde{h}}_{(t)},
\end{align*}
where $\Ones \in \Real^h$ is a vector of all ones. In practice, the reset gate controls how much information from previous sequence elements should be kept, and the hidden state is computed as a convex combination of this quantity and the previous hidden state, controlled by the update gate.

\subsection{Recurrent Neural Networks as Autoregressive Models}
Besides being used as supervised models, \glspl{rnn} can be also used as generative models of sequences \cite{graves2013generating}. Specifically, given a dataset $\Data = \{\PatternGraph{S}{i}\}_{i=1}^n$ of sequences, they can be trained to learn a model $p_{\Param}$ of the underlying distribution $p(\AttrGraph{s})$ as follows:
$$\argmin_{\Param} \frac{1}{n} \sum_{\AttrGraph{s} \in \Data} - \log p_{\Param}(\AttrGraph{s}) = \frac{1}{n} \sum_{\AttrGraph{s} \in \Data} \sum_{t=1}^{|\AttrGraph{s}|} - \log p_{\Param}(\Elem{x}{t} \given \Elem{x}{t-1}, \Elem{h}{t-1}),$$
where $|\AttrGraph{S}|$ indicates the sequence length, $\Elem{x}{0}$ is a special \emph{start of sequence symbol} $\SOS$, and $\Elem{h}{0} = \Zeros$ as usual. Once the network is trained, a sequence can be generated one element at a time. The process is initialized by feeding the start of sequence token $\Elem{x}{0}$ and the initial hidden state $\Elem{h}{0}$, and proceeds repeating the following instructions until an \emph{end of sequence token} $\Tuple{\mathsf{E}}$ is predicted by the network:
\begin{itemize}
    \item update the current state $\Elem{h}{t}$ with the \gls{rnn} and predict an output $\Elem{o}{t}$, which corresponds to a conditional distribution over the possible sequence elements. Sample a sequence element $\Elem{\tilde{x}}{t}$ according to this distribution;
    \item feed the sampled element $\Elem{\tilde{x}}{t}$ and the updated state $\Elem{h}{t}$ to the network and repeat.
\end{itemize}
We call this process \emph{autoregressive sampling mode}. The process is depicted in Figure \ref{fig:rnn-autoregressive}.
\begin{figure*}[h!]
    \centering
    \resizebox{.7\textwidth}{!}{\input{Figures/Chapter3/05a-autoregressive}}
    \caption{An example of training a recurrent neural network for learning an autoregressive distribution. The dashed arrows indicate non-differentiable operations.}
    \label{fig:rnn-autoregressive}
\end{figure*}
During training, the sampling process breaks the differentiability of the model. Hence, one resorts to reparameterization \citep{jang2017gumbel} or differentiates the output $\Elem{o}{t}$ instead of the hard sample \citep{bengio2013straighttrough}. Another option during training is \emph{teacher forcing} \citep{williams1989teacherforcing}: in this case, the knowledge of the elements of the sequence during training is exploited, by feeding the ground truth sequence element (instead of the sampled value) as the input for the next sequence. Both strategies have advantages and disadvantages: teacher forcing learns faster initially, but does not expose the network to its own errors, thus it can be less precise at generation time. Often, a combination of the two is used.
\begin{figure*}[h!]
    \centering
    \resizebox{.7\textwidth}{!}{\input{Figures/Chapter3/05b-teacher-forcing.tex}}
    \caption{The teacher forcing strategy for training a sequence generator with RNNs.}
    \label{fig:teacher-forcing}
\end{figure*}

\section{Recursive Neural Networks} \label{sec:recnns}
A \gls{recnn} \citep{sperduti1997generalizedneuron,frasconi1998general} is a \gls{nn} architecture that can adaptively process hierarchical data. Using trees as an example, let $\Tree{T}$ be an attributed tree with $m$ nodes $\Nodes{t} = \Set{v_1, v_2, \ldots, v_m}$, and $\Vector{x}_{\Tree{t}} = \Set{\Elem{x}{v} \in \Real^d \mid v \in \Nodes{t}}$ be its set node of features. The state transition function of a \gls{recnn}, applied locally to each node $v \in \Nodes{t}$, is the following:
\begin{align*}
    \Elem{h}{v}=
    \begin{cases}
        \boldsymbol{0} & \mathrm{if}\; \Fun{ch}(v) = \emptyset\\
        \EncTrans(\Elem{x}{v}, \Elem{h}{\Tree{t}_v}) & \mathrm{otherwise},
    \end{cases}
\end{align*}
where $\Elem{h}{\Tree{t}_v} \in \Real^h$ is the hidden state of the sub-tree rooted at $v$. As with \glspl{rnn}, the state transition function $\EncTrans$ is recursive, but this time the recursion is defined over the tree structure. Specifically, to compute the hidden state of a node, the hidden state of all its children must be known in advance. The state computation starts at the leaves of the tree (where the state is initialized beforehand to make the recursion well-defined), and proceeds bottom-up until the root node is reached.
The development of \glspl{recnn} started in the middle '90s, with the introduction of the notion of generalized recursive neuron \citep{sperduti1997generalizedneuron} and the development of a general framework for learning with tree-structured data \citep{frasconi1998general}, which was later extended to more expressive classes of structures such as \glspl{dag} and \glspl{dpag}. Since then, they have applied fruitfully in several fields, including among others cheminformatics \citep{micheli2007introductionrecnncheminformatics,baldi2013recursiveneuralnets}, \gls{nlp} \citep{costa2000recnns,costa2003recnns,sturt2003recnns,socher2013recnnsentiment} and scene parsing \citep{socher2011parsingscenes}. Interestingly, \glspl{recnn} are also backed up by strong theoretical results, which support the generality of the structural transduction and characterize the kind of functions they can learn. Specifically, universal approximation theorems showing that \glspl{recnn} can approximate arbitrarily well any function from labeled trees to real values \citep{hammer1999recnn}, and from labeled \glspl{dpag} to real values \citep{hammer2005universal} have been proved.

\subsection{Training}
Using the binary tree of Figure \ref{fig:tree} and a structure-to-element task as an example, one possible implementation of the state transition function of a \gls{recnn} is the following:
\begin{align*}
    \Elem{h}{v} &= \sigma\Par{\Matrix{W}\Elem{x}{v} + \Matrix{U}_\Fun{l}\Elem{h}{\Fun{l}(v)} + \Matrix{U}_\Fun{r}\Elem{h}{\Fun{r}(v)}},\; \forall v \in \Nodes{t}.
\end{align*}
In the above formula, $g$ can be any hidden activation function, and $\Matrix{w} \in \Real^{d \times h}$, $\Matrix{u}_\Fun{l}$, and $\Matrix{u}_\Fun{r} \in \Real^{h \times h}$, are weight matrices shared across the structure. Notice that the two weight matrices on the node children are positional, meaning that they are applied to a certain node according to its position. In the example case of a binary tree, the two functions $\Fun{l}(v)$ and $\Fun{r}(v)$ select the left and right child of a node $v$, respectively, if they exist. Figure \ref{fig:recnn} shows the unfolded \gls{recnn} over the tree, where the final output of the entire structure is obtained using the hidden state of the root node $v_1$ as:
$$\Vector{o} = g\Par{\Elem{h}{v_1}},$$
where $\Vector{o} \in \Real^y$ is the output of the network and $g$ can be any downstream network such as a simple output layer, or a more complex neural network. For structure-to-structure tasks, the output is calculated node-wise as follows:
$$\Elem{o}{v} = g\Par{\Elem{h}{v}},\; \forall v \in \Nodes{t}.$$
Notably, the order by which the hidden states need to be calculated (the numbers at the left of the hidden states in the figure) must be respected to ensure that the recursive process is well-defined. The states of nodes with the same ordering can be calculated in parallel according to the tree structure, which makes \glspl{recnn} more efficient than \glspl{rnn} when compared on structures with the same number of elements.
\begin{figure*}[h!]
    \centering
    \resizebox{.5\textwidth}{!}{\input{Figures/Chapter3/06-recnn}}
    \caption{A recursive neural network unfolded over the tree of Figure \ref{fig:tree} for a structure-to-element task. The number at the left of the hidden states indicates the order in which they are calculated. Notice the initialization of the hidden states at the leaves (indicated by dashed boxes).}
    \label{fig:recnn}
\end{figure*}
More in general, \glspl{recnn} are analogous to \glspl{rnn} as to how they can be trained with \gls{mle}, and as what kinds of conditional distributions they can learn (even though in practical cases the structure-to-element scenario is more common).

\section{Deep Graph Networks} \label{sec:dgns}
The \gls{rnn} and \gls{recnn} models presented in Sections \ref{sec:rnns} and Section \ref{sec:recnns} share the idea that the state transition function is applied locally and recursively on the structure to compute the state vectors. Extending it to arbitrary graphs (which can have cycles) would require to apply the state transition function recursively to the neighbors of a vertex. However, this approach is not applicable to general graphs. In fact, the presence of cycles creates \emph{mutual dependencies}, which are difficult to model recursively and may lead to infinite loops when computing the states of vertices in parallel. While this issue can be overcome by resorting to canonization techniques that provide an ordering between the vertices, it is not feasible in many practical cases. \glspl{dgn} are a class of \glspl{nn} which can process arbitrary graphs, even in presence of cycles. The solution adopted by \glspl{dgn} to the problem of modelling mutual dependencies is to update the state of the vertices according to an \emph{iterative} scheme. Specifically, the hidden state of a vertex is updated as a function of the hidden state of the same vertex at the previous iteration. Given an attributed graph $\AttrGraph{g}$ with vertex features $\GraphFeatures{x}{g} = \Set{\Elem{x}{v} \mid v \in \Nodes{g}}$, the state transition function computed by a \gls{dgn}, applied locally to each vertex of $v \in \Nodes{g}$, has the following form:
\begin{align*}
    \StateVector{v}{\ell}=
    \begin{cases}
        \Elem{x}{v} & \mathrm{if}\; \ell = 0\\
        \EncTrans^{(\ell)}(\Elem{x}{v}, \StateVector{v}{\ell-1}) & \mathrm{otherwise},
    \end{cases}
\end{align*}
where $\StateVector{v}{\ell} \in \Real^h$ is now the state of vertex $v$ at iteration $\ell$, and $\EncTrans = \EncTrans^{1} \circ \EncTrans^{2} \circ \ldots \circ \EncTrans^{(\ell)}$. Notice how the value of the state vector does not depend on the value of the neighboring state vectors, but to the same state vector at the previous iteration. Following, we slightly change terminology and refer to the vertices of a graph as nodes, in accordance to the terminology currently used in the literature. For the same reason, we shall use the following terminology as regards the supervised tasks that can be learned with \glspl{dgn}:
\begin{itemize}
    \item structure-to-structure tasks shall now be termed \keyword{node classification} tasks if the targets are discrete node labels, or \keyword{node regression} tasks if the targets associated to the nodes are continuous vectors or scalars. We further distinguish among \emph{inductive} node classification (respectively, regression) tasks, if the prediction concerns unseen graphs; and \emph{transductive} node classification (respectively, regression) tasks, if the structure of the graph is fixed (\ie, the dataset is composed of one single graph), and the task is to predict from a subset of nodes for whose target is not known. The transductive setting is often referred to as semi-supervised node classification (respectively, regression);
    \item structure-to-element tasks shall now be termed \keyword{graph classification} tasks if the target associated to the graph is a discrete label, or \keyword{graph regression} tasks if the targets are continuous vectors (or scalars).
\end{itemize}

\subsection{Contextual Processing of Graph Information}
Besides solving the problem of mutual dependencies in the state computations, the iterative scheme has another important purpose, that of propagating the local information of a node to the other nodes of the graph. This process is known under several names, such as \keyword{context diffusion}. Informally, the context of a node is the set of nodes that directly or indirectly contribute to determine its hidden state; for a more formal characterization of the context, see \citep{micheli2009nn4g}. Context diffusion in a graph is obtained through \keyword{message passing}, \ie by repeatedly applying the following procedures:
\begin{itemize}
    \item each node constructs a \emph{message} vector using its hidden state, which is sent to the immediate neighbors according to the graph structure;
    \item each node receives messages from its neighbors, which are used to update its current hidden state through the state transition function.
\end{itemize}
Message passing is bootstrapped by initializing the hidden state of the nodes appropriately, so that an initial message can be created. Usually, this initial message is the vector of node features. Using the example graph of Figure \ref{fig:context-diffusion} as reference, we now explain how the context flows through the nodes as message passing is iterated. At iteration $\ell=2$, the vertex $v$ receives a single message from its only neighbor, $u$. The incoming message was constructed using information about the state of $u$ at $\ell=1$, which in turn was obtained through the state of neighbors of $u$ at $\ell=0$ (including $v$ itself). Thus, the context of $v$ at iteration $\ell=2$ includes $u$ as well as the neighbors of $u$. It is clear that, for this particular case, at iteration $\ell=3$ the context of $v$ would include all the nodes in the graph. Clearly, by iterating message passing, the nodes are able to acquire information from other nodes farther away in the graph.
\begin{figure*}[h!]
    \centering
    \resizebox{.8\textwidth}{!}{\input{Figures/Chapter4/01-context-diffusion}}
    \caption{Context diffusion through message passing. Directed edges represent messages (\eg from node $u$ to $v$ at iteration $\ell=2$). Dashed arrows represent the implicit contextual information received by a node (in dark grey) through the messages from its neighbors (in light grey). Focusing on node $v$, its context at iteration $\ell=2$ is composed all the dark grey nodes (including $v$ itself).}
    \label{fig:context-diffusion}
\end{figure*}
In the literature, we distinguish three different approaches by which iterative context diffusion is implemented in practice, which we describe in the following.

\subsubsection*{Recursive Approaches}
In the recursive approach to context diffusion, message passing is formulated as a dynamical system. In this case, the state transition function is recursive, meaning that $\EncTrans = \EncTrans^{(1)} = \EncTrans^{(2)} = \ldots = \EncTrans^{(\ell)}$. Practically speaking, the mutual dependencies between hidden states are modeled with a single recurrent layer, which is run indefinitely until convergence. Some well-known representatives of this paradigm are the Graph Neural Network \citep{scarselli2009gnn}, the Graph Echo State Network \citep{gallicchio2010graphesn}, and the more recent Fast and Deep Graph Neural Network \citep{gallicchio2020fastdeepgnn}. To ensure convergence, these approaches impose contractive dynamics on the state transition function. While the Graph Neural Network enforces such constraints in the (supervised) loss function, the other two inherit convergence from the contractivity of (untrained) reservoir dynamics. Another example is the Gated Graph Neural Network \citep{li2016gatedgnn}, where, differently from \citet{scarselli2009gnn}, the number of iterations is fixed \apriori regardless of whether convergence is reached or not. Another approach based on \emph{collective inference}, which adopts the same strategy but does not rely on any particular convergence criteria, has been introduced in \citep{macskassy2007classificationnetworkdata}.

\subsubsection*{Feed-Forward Approaches}
The feed-forward approach is based on stacking multiple layers to compose the local context learned at each message passing iteration. As a result, the mutual dependencies between the hidden states are handled separately via differently parameterized layers, without the need of constraints to ensure the convergence of the state transition function. In practice, the state transition function is no more recursive, but changes at every layer. Thus, in the feed-forward case, the symbol $\ell$ indicates the layer that handles the corresponding message passing iteration. The effectiveness of the compositionality induced by the introduction of layers has been demonstrated in \citep{micheli2009nn4g}, where it is shown formally that the context of a node increases as a function of the network depth, up to including all the other nodes in the graph. Feed-forward approaches are nowadays the main paradigm to design \glspl{dgn}, due to their simplicity, efficiency, and performance on many different tasks. However, deep networks for graphs suffer from the same gradient-related problems as other deep \glspl{nn}, especially when associated with an \quotes{end-to-end} learning process running through the whole architecture \citep{bengio1994learninglongtermdependenciesdifficult,li2018deeperinsightgraphconvsemisupervised}. For the rest of this thesis, all the \glspl{dgn} used shall be feed-forward.

\subsubsection*{Constructive Approaches}
Constructive approaches are a special case of feed-forward models, in which training is performed layer-wise. The major benefit of constructive architectures is that deep networks do not incur the vanishing/exploding gradient problem by design. In supervised scenarios, the constructive technique can learn the number of layers needed to solve a task \citep{fahlman1990cascor,marquez2018deepcascade,bianucci2000cascorchemistry}. In other words, constructive \glspl{dgn} can determine automatically how much context is most beneficial to perform well, according to the specific task at hand. Another feature of constructive models is that they solve a problem in a \emph{divide-et-impera} fashion, rather than using \quotes{end-to-end} training, by incrementally splitting the task into manageable sub-tasks. Each layer solves its own sub-problem, and subsequent layers use their results to improve further on their own, addressing the global task progressively. Among the constructive approaches, we mention the Neural Network for Graphs \citep{micheli2009nn4g}, which was the first to propose a feed-forward architecture for graphs. Among recent models, another related approach which tackles the problem from a probabilistic point of view is the Contextual Graph Markov Model \citep{bacciu2018contgraphmarkov}.

\subsection{Building Blocks of Deep Graph Networks}
\glspl{dgn} are built from several architectural components, which we cover in detail in this section. In short, a \gls{dgn} can be decomposed into a collection of layers that process the graph structure, and a downstream predictor (either a classifier or a regressor) that computes a task-dependent output. The whole network is trained in an end-to-end fashion. In this section, we focus on the former components, the ones whose role is to carry out the processing of an input graph.

\subsubsection{Graph Convolutional Layers}\label{sec:graph-conv-layers}
A \gls{gcl} is essentially a neural network layer that performs message passing. The term \quotes{convolutional} is used to remark that the local processing performed by the state transition function is a generalization of the convolutional layer for images to graph domains with variable-size neighborhoods. Given an attributed graph $\AttrGraph{g}$ with $n$ nodes, and its node attributes $\GraphFeatures{x}{g} = \Set{\Elem{x}{v} \mid v \in \Nodes{g}}$, one general formulation of a \gls{gcl} is the following:
\begin{align}
    \label{eq:simple-aggregation}
    \StateVector{v}{\ell} = U \Par{\StateVector{v}{\ell-1}, A\Par{\Set{\,T(\StateVector{u}{\ell-1}) \mid u \in \Neigh(v)}}}, \; \forall v \in \Nodes{g},
\end{align}
where $\StateVector{v}{\ell} \in \Real^{h_\ell}$ is the hidden state of the node at layer $\ell$, $\,\StateVector{v}{\ell-1} \in \Real^{h_{\ell-1}}$ is the hidden state of the node at the previous layer $\ell-1$, and by convention $\StateVector{v}{0} = \Elem{x}{v}$. Notice that the neighborhood function $\Neigh$ is also implicitly passed as input of the layer, so that the connectivity of each node is known. We can identify three key functions inside a \gls{gcl}:
\begin{itemize}
    \item $T: \Real^{h_{\ell-1}} \shortrightarrow \Real^{h_{\ell-1}}$ is a \emph{transform} function that applies some transformation to the hidden states of neighbors of node $v$ at layer $\ell-1$. This can be any function, either fixed or adaptive (implemented by a neural network);
    \item $A: (\Real^{h_{\ell-1}} \times \Real^{h_{\ell-1}} \times \ldots) \shortrightarrow \Real^{h_{\ell-1}}$ is an \emph{aggregation} function that maps a \emph{multiset}\footnote{Given a set $\Cal{B}$, a multiset $\Multiset(\Cal{B})$ is a tuple $\Tuple{\Cal{B}, \varrho}$ where $\varrho: \Cal{B} \shortrightarrow \Natural_+$ gives the multiplicity of each element in $\Cal{B}$.} of transformed neighbors of $v$ to a unique \emph{neighborhood state vector}. In practice, $A$ is a \emph{permutation invariant} function, meaning that its output does not change upon reordering of the arguments. For this reason, the computation of the neighborhood state vector is often referred to as \emph{neighborhood aggregation};
    \item $U: (\Real^{h_{\ell-1}} \times \Real^{h_{\ell-1}}) \shortrightarrow \Real^{h_{\ell}}$ is an \emph{update} function that takes the hidden state of a node at layer $\ell-1$ and the aggregated vector, and combines them to produce the new hidden state of the node at layer $\ell$. Similarly to $T$, $U$ can also be fixed or adaptive.
\end{itemize}
The usage of a permutation invariant function to compute the state of the neighbors is crucial, as it allows to acquire information from nearby nodes in a non-positional fashion, which is often the case with real-world graphs. From this general formulation, several implementations can be realized. As an example, we report the well-known formulation of \citet{kipf2017semisupervisedgcn}, corresponding to the \gls{gcn} model:
\begin{align}
    \label{eq:convolutional}
    \StateVector{v}{\ell} = \sigmoid\Par{ \LayerMatrix{w}{\ell} \sum_{u \in \Neigh(v)} \tilde{l}_{uv}\StateVector{u}{\ell-1}}, \; \forall v \in \Nodes{g},
\end{align}
where $\tilde{l}_{uv}$ is the entry of the symmetric normalized graph Laplacian $\NormLaplacianMatrix{g}$ related to nodes $u$ and $v$, and:
\begin{align}
    \label{eq:transform}
    T(\StateVector{u}{\ell-1}) &= \GenStateVector{t}{v}{\ell-1}  = \tilde{l}_{uv}\,\StateVector{u}{\ell-1}\\
    \label{eq:aggregate}
    A\Par{\Set{\GenStateVector{t}{v}{\ell-1} \mid u \in \Neigh(v)}} &= \GenStateVector{n}{v}{\ell-1} = \sum_{u \in \Neigh(v)} \GenStateVector{t}{v}{\ell-1}\\
    \label{eq:update}
    U(\StateVector{v}{\ell-1}, \GenStateVector{n}{v}{\ell-1}) &= \StateVector{v}{\ell} =  \sigmoid\Par{\LayerMatrix{w}{\ell}\, \GenStateVector{n}{v}{\ell-1}}.
\end{align}
In this case, the aggregation function is the sum function. Other examples of permutation invariant functions used in practical contexts are the mean, the max, or other general functions which work on multisets \citep{zaheer2017deepsets}. Notice that a \gls{gcl} can be applied simultaneously to all the nodes in the graph, corresponding to visiting the graph nodes in parallel, with no predefined ordering. This contrasts with \glspl{rnn} and \gls{recnn}, where parallelism in the state calculations is not possible or limited, respectively.

The generic \gls{gcl} can be rewritten in matrix form as some variation of the following:
$$\LayerStateMat{g}{\ell} = \Fun{GCL}(\AdjMatrix{g}, \LayerStateMat{g}{\ell-1}) = g\Par{\AdjMatrix{g}\LayerStateMat{g}{\ell-1}\LayerMatrix{w}{\ell}} \in \Real^{n \times h^{(\ell)}},$$
where $g$ is a generic activation function, $\AdjMatrix{g} \in \Real^{n \times n}$ is the adjacency matrix of the graph, $\LayerStateMat{g}{\ell-1} \in \Real^{n \times h}$ are the hidden states computed at layer $\ell-1$ where by convention $\LayerStateMat{g}{0} = \FeatureMatrix{g} \in \Real^{n \times d}$ is the matrix of node features, and $\LayerMatrix{w}{\ell} \in \Real^{h_{(\ell-1)} \times h_{(\ell)}}$ is the matrix of trainable layer-wise weights. Here, the adjacency matrix substitutes the neighborhood function $\Neigh$, and the node adjacencies are inferred by its rows and columns. With this formulation, the \gls{gcl} can be vectorized, which allows to run the state computation in fast hardware such as \glspl{gpu}.

\paragraph{Handling Edges}
In certain tasks, including information about the edge features to the message passing algorithm can be beneficial to performances. Here, we describe how this can be achieved, focusing on the case where the edge features are discrete values out of a set of $k$ possible choices. Specifically, given a graph $\Graph{g}$, we assume a set of edge features of the form $\GraphFeatures{e}{g} = \Set{\Elem{e}{u,v} \in \Cal{C} \mid (u, v) \in \Edges{g}}$, with $\Cal{C} = \Set{c_i}_{i=1}^k$. To account for different edge types, two modifications to the message passing algorithm are required. One is to replace the standard neighborhood function in the aggregation function with the following \emph{edge-aware} neighborhood function of a node $v$:
$$\Neigh_c(v) = \Set{u \in \Neigh(v) \mid \Indicator{\Elem{e}{u, v} = c}},$$
which selects only neighbors of $v$ with edge type $c$. The other modification requires to change the update function for handling the different edge types. Taking again the \gls{gcn} implementation as an example, Eq. \ref{eq:update} is modified as follows:
\begin{align*}
    \StateVector{v}{\ell} = \sigmoid\Par{\sum_{c \in \Cal{C}} \LayerMatrix{w}{\ell}_c \sum_{u \in \Neigh_c(v)} \tilde{l}_{uv}\,\Elem{h}{v}^{(\ell-1)}},\; \forall v \in \Nodes{g},
\end{align*}
where the weight matrices $\LayerMatrix{w}{\ell}_c $ are now edge-specific, so that the contributions of the different edge types are weighted adaptively \citep{micheli2009nn4g,schlichtkrull2018relationaldatagcn}. In practice, the above procedure corresponds to performing $k$ different aggregations weighted separately to compute the state of the node. Other approaches to include edge information in the message passing scheme require to extend the transform function, such that the edges between the processed node and its neighbors are included in the transformation (for example, by concatenating the edge feature to the hidden state vector).

\paragraph{Node Attention}
Attention mechanisms \citep{bahdanau2015attention} are a widely used tool in Deep Learning to get importance scores out of arbitrary sets of items. Thus, they are naturally applicable within the \gls{dgn} framework to measure the contribution of the different nodes during neighborhood aggregation. Specifically, to introduce attention mechanisms in the neighborhood aggregation, we weigh the contribution of the transformed nodes in the neighborhood by a scalar $a^{(\ell)}_{uv} \in \Real$, called \emph{attention score} as follows:
$$A(\Set{\,a^{(\ell)}_{uv}\,T(\StateVector{u}{\ell-1}) \mid u \in \Neigh(v)}).$$
The attention scores are derived from \emph{attention coefficients} $w_{vu}^{(\ell)}$, which are essentially similarity scores between the neighbor and the current node, calculated as follows:
$$w_{vu}^{(\ell)} = F(\StateVector{v}{\ell}, \StateVector{u}{\ell}),$$
where $F$ is an arbitrary function (generally a neural network). Different attention mechanisms are defined based on how $F$ is implemented. Finally, the coefficients are normalized into attention scores by a softmax function, effectively defining a probability distribution among them. The attention mechanism can be generalized to \emph{multi-head} attention, where multiple attention scores for each node are calculated and concatenated together to obtain an attention vector, rather than a score. Figure \ref{fig:attention} shows an example of attention computed on an example graph. We remark that node attention is unrelated to weighting the connection between nodes, which is an operation that involves the edge features. Here, similarity between nodes is calculated relying solely on the hidden states of the involved node and its neighbors.

\begin{figure*}[h!]
    \centering
    \resizebox{.35\textwidth}{!}{\input{Figures/Chapter4/02-attention}}
    \caption{An example of node attention. Note that edge thickness is not related to the strength of the connection between node $v$ (in dark grey) and its neighbors (in light grey), but it represents the degree of similarity between the node states, quantified in a probabilistic sense by the attention score. Dashed edges connect nodes that are not involved in the attention score computation.}
    \label{fig:attention}
\end{figure*}

\paragraph{Node Sampling}
Node sampling is a technique used when learning on large graphs to ensure computational efficiency. When the number of nodes in a graph is large, and nodes are densely connected among themselves, computing neighborhood aggregation may become very expensive or even intractable. The most straightforward method to address this issue is to randomly sample a predefined number of nodes to aggregate, rather than using the whole neighborhood. This basic strategy can be refined by using more sophisticated techniques such as importance sampling \citep{gallicchio2020fastdeepgnn}, or even extended to sampling a bounded number of nodes which are not necessarily in the immediate neighborhood of the current node \citep{hamilton2017graphsage}. The latter requires to add fictitious edges between the current node and nodes at farther distances, in order to treat them as standard neighbors. This way, global information about the graph can be incorporated more directly, as compared to message passing.

\begin{figure*}[h!]
    \centering
    \resizebox{.35\textwidth}{!}{\input{Figures/Chapter4/03-sampling}}
    \caption{An example of node sampling. We focus on a node $v$ (in dark grey), and its neighbors (in light grey). Dashed edges connect nodes that are not involved in the neighborhood aggregation; notice how nodes $u_1$ and $u_2$ are excluded from the neighborhood aggregation, even though they are effectively neighbors of $v$.}
    \label{fig:sampling}
\end{figure*}

\subsubsection{Readout Layers}
As we have seen, the application of $L$ \gls{dgn} layers to a graph $\AttrGraph{G}$ yields $L$ hidden states per node, each one composed with a progressively broad context. In node classification or regression tasks, these are combined by a \emph{hidden state readout} function to obtain a unique hidden state to use as input of the output function, which emits a prediction for every node. Specifically, a hidden state readout function $\Fun{R'}$ computes a \keyword{node representation} (or \keyword{node embedding}) $\StateVector{v}{*}$ for each node as follows:
$$\StateVector{v}{*} = \Op{R}_{v}\Par{\Set{\StateVector{v}{\ell} \mid \ell = 1, \ldots, L}},\;  \forall v \in \Nodes{g}. $$
Notice that, when aggregating hidden states, one can exploit the fact that the number of layers is fixed beforehand in feed-forward \gls{dgn} architectures, and that the hidden states are ordered depth-wise. Thus, the aggregation need not to be permutation-invariant. Usual choices of $\Op{R}_{v}$ include concatenation, weighted average (where the mixing weights can also be learned), \gls{rnn}s, or just selecting the hidden state at the last layer. The node representations are then fed to an output layer or downstream network, which computes node-wise outputs:
$$\Elem{o}{v} = g(\StateVector{v}{*}),\; \forall v \in \Nodes{g},$$
where $\Elem{o}{v} \in \Real^y$ and $g$ can be any arbitrarily complex neural network as usual. A visual example of the process for a single node is shown in Figure \ref{fig:node-readout}.
\begin{figure*}[h!]
    \centering
    \resizebox{.8\textwidth}{!}{\input{Figures/Chapter4/04-node-readout}}
    \caption{The role of hidden state readout function in a node classification/regression task. Here, we focus on a single node $\StateVector{v}{\ell}$, where we drop the subscripts to avoid visual cluttering. Grey nodes replace the $[v]$ subscript. The hidden state readout function creates a node representation $\StateVector{v}{*}$ by combining its three hidden states (one for each layer). Successively, the node representation is turned into an output by an output layer, and compared by the loss function to the same node $\Elem{y}{v}$ in the isomorphic target graph. This operation is repeated for every node in the graph.}
    \label{fig:node-readout}
\end{figure*}
In graph classification or regression tasks, the node representations computed by a node readout function are aggregated once more by a \emph{graph readout} function, to compute a \keyword{graph representation} (or \keyword{graph embedding}) $\GraphRepr{g}$, \ie a vector representing the entire graph. Differently from the hidden state readout, the readout function must necessarily be permutation-invariant, since there are no guarantees about the number of graph nodes. Specifically, a graph readout function $\Fun{R}$ computes the embedding of graph $\AttrGraph{g}$ as follows:
$$\GraphRepr{g} = \GraphReadout{g}\Par{\Set{\StateVector{v}{*} \mid v \in \Nodes{g}}}.$$
Typical readouts for \glspl{dgn} include simple functions such sum, mean, max, or more complex aggregators such as deep sets models \citep{zaheer2017deepsets}. Finally, the graph embedding is fed to an output layer or a downstream network to compute the associated output:
$$\Vector{o} = g(\GraphRepr{g}).$$
\begin{figure*}[h!]
    \centering
    \resizebox{.6\textwidth}{!}{\input{Figures/Chapter4/05-graph-readout}}
    \caption{A graph readout on an example graph for a graph classification/regression task. Here, we assume the node representations $\StateVector{v}{*}$ have already been obtained by a node readout (not shown).}
    \label{fig:graph-readout}
\end{figure*}
An graph readout applied to an example graph is shown in Figure \ref{fig:graph-readout}.

\subsubsection{Graph Pooling Layers}\label{sec:pooling}
Similarly to the layer used by \glspl{cnn} for computer vision, pooling is also applicable to \glspl{dgn} for graph classification (or regression) tasks. In \gls{dgn} architectures, pooling is usually placed after a graph convolutional layer, and serves a three-fold purpose: it is used to detect communities in the graph, \ie clusters of nodes with very higher connectivity among themselves than the rest of the graph; to augment the information content of the hidden states with this knowledge; and to reduce the number of nodes (and consequently, the number of parameters) needed by the network in later stages of computation. An example of a graph pooling layer is shown in Figure \ref{fig:pooling}, where nearby nodes are pooled into a single node in the reduced graph according to some strategy. Graph pooling methods are developed according to two strategies: \emph{adaptive} and \emph{tolopogical}. Adaptive methods pool nodes in a differentiable manner, so that the optimal clustering of the nodes for the task at hand is learned by the end-to-end network. One example of adaptive pooling is DiffPool, developed in \citep{ying2018diffpool}. Given a graph $\Graph{g}$ with $n$ nodes, and assuming the $\ell-1$ \glspl{gcl} have been applied, DiffPool computes two matrices:
\begin{align*}
    \GenGraphLayerMat{z}{\ell-1} &= \Fun{DGN}_e(\LayerAdjMat{\ell-1},\LayerStateMat{g}{\ell-1}) \in \Real^{n \times h}\\
    \GenGraphLayerMat{s}{\ell-1} &= \softmax\Par{\Fun{DGN}_p(\LayerAdjMat{\ell-1},\LayerStateMat{g}{\ell-1})} \in \Real^{n \times k},
\end{align*}
where $\Fun{DGN}_e$ and $\Fun{DGN}_p$ is a stack of graph convolutional layers. The matrix $\Matrix{S}$ computes a soft-assignment to each node to one of $k$ clusters with a softmax output function. These two matrices are then combined with the current hidden states to produce a novel adjacency matrix and its corresponding matrix of hidden states as follows:
\begin{align*}
    \LayerStateMat{g}{\ell} &= \GenGraphLayerMat{s}{\ell-1}\GenGraphLayerMat{z}{\ell-1} \in \Real^{k \times h}\\
    \LayerAdjMat{\ell} &= \GenGraphLayerMat{s}{\ell-1}\LayerAdjMat{\ell-1}\GenGraphLayerMat{s}{\ell-1} \in \Real^{k \times k}
\end{align*}
where $\LayerAdjMat{0} = \AdjMatrix{g}$ and $\LayerStateMat{g}{0} = \FeatureMatrix{g}$. Thus, after applying the DiffPool layer, the size of the graph is reduced progressively from $n$ to $k$ nodes.

\begin{figure*}[h!]
    \centering
    \resizebox{.8\textwidth}{!}{\input{Figures/Chapter4/06-pooling}}
    \caption{A visual example of a graph pooling layer.}
    \label{fig:pooling}
\end{figure*}
Topological pooling, on the other hand,  uses not-differentiable methods which leverage the global structure of the graph, and the communities beneath it. These methods work by grouping nodes according to well known graph theory tools, such as spectral clustering (see \eg \citet{vonluxburg2007tutorialspectralclustering, dhillon2007weightedgraphcuts}).

\subsection{Regularization}
\glspl{dgn} are trained with regular losses, such as \gls{ce} for classification and \gls{mse} for regression. Besides standard regularization techniques, the objective function is often regularized through unsupervised loss functions, which impose priors on which kinds of
structures the network should preferably learn. The regularized objective function for supervised tasks\footnote{We use a generic target $y$ to imply that this formulation is task-independent.} has the form:
$$\argmin_{\Param} \Loss(\Param, (\Vector{x}_{\Graph{g}}, y)) + \lambda \sum_{\ell=1}^L \Psi(\LayerStateMat{g}{\ell}),$$
were $\Psi$ is a regularization function weighted by a regularization coefficient $\lambda$, that is applied at each layer $\ell$ to the set of hidden states of the nodes, represented as a matrix $\Matrix{h}_{{\Graph{g}}}^{(\ell)} \in \Real^{n \times h}$. An example of regularization widely employed in practical settings is the \emph{link prediction} unsupervised loss, defined as:
\begin{align}
    \Psi(\LayerStateMat{g}{\ell}) = \sum_{u,v} \Norm{\StateVector{v}{\ell} - \StateVector{u}{\ell}}_2,
\end{align}
where the summation ranges over all possible combinations of nodes. Basically, when this loss is minimized, it biases the network towards producing node representations that are more similar for nodes connected by an edge. Notably, this loss can be also used in isolation to tackle link prediction tasks, \ie tasks where the downstream network must predict unseen links between the nodes.
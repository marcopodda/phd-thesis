\chapter{Machine Learning and Neural Networks}\label{ch:neural-networks} 
In this chapter, we provide an overview of Machine Learning and neural networks. We start with a short introduction to Machine Learning, laying out the foundations on how learning from data is possible, as well as more practical aspects about the selection and evaluation of machine learning models. In Section \ref{sec:nn}, we briefly review the core concepts about \glspl{nn}, such as the procedure by which they are trained, the loss functions they optimize, and how they are regularized.

\section{Machine Learning}\label{sec:ml}
Many real-world phenomena are not clearly understood, or cannot be characterized in terms of simple mathematical equations. However, one can usually collect quantitative and/or qualitative measurements about them, and observe their effects in the environment where they manifest. \gls{ml} is a branch of Artificial Intelligence that studies algorithms and provides tools to \emph{learn} these unknown processes from data. According to Mitchell, \keyword{learning} is defined as improving at some task from experience \citep{mitchell}. To start off, let us first briefly describe the key components of a learning system in detail, introducing other useful notation, definitions and concepts along the way:

\begin{itemize}
    \item the \keyword{experience}, in the context of \gls{ml}, refers to
    the data which is available to the learner. Data is provided in the form
    of \emph{examples}, (also called \emph{observations} or \emph{data points}). Each example is a set of qualitative or quantitative measurements about some phenomenon of interest;
    \item the \keyword{task} refers to some function $f$ (called \emph{target function}) that the learner needs to estimate from data. \gls{ml} tasks are multiple, and of potentially very different nature. Three well-known examples of tasks are are \emph{classification}, where $f$ is a function that assigns a categorical \emph{label} to a data point; \emph{regression}, where $f$ associates a desired numerical quantity to a given example; and \emph{density estimation}, where $f$ is the probability density (or mass, for the discrete case) function from which the examples are drawn from;
    \item the \keyword{performance} is a function which returns a quantitative measurement of how \quotes{well} the task is being learned. For example, in a classification task, one can measure improvement measuring the \emph{accuracy} of the learner, \ie the proportion of correctly classified examples out of the total number of available examples. Clearly, higher accuracies indicate that a task is being learned.
\end{itemize}

Machine Learning can be broadly divided in three main areas: supervised, unsupervised, and reinforcement learning. In \keyword{supervised learning}, the learner is given a set of input-output pairs, and the goal is to to learn the relationship between the inputs and the outputs. Classification and regression fall into the supervised paradigm. In \keyword{unsupervised learning}, data only consists of inputs, and the goal is to learn some property of the distribution that generates the data. Typical unsupervised tasks include clustering and density estimation. \keyword{Reinforcement learning} is about learning to act in an environment, where the actions performed yield rewards or penalties. This work exclusively deals with supervised and unsupervised learning. 

Regardless of the learning paradigm, the central issue in \gls{ml} is \keyword{generalization}, that is, the learned function should be such that it works correctly on previously unseen examples, not used during the learning process. As it turns out, generalization is possible if the learning process is carefully crafted.

\section{Learning and Generalization}\label{sec:learning}
Informally, learning can be thought of as finding a \quotes{good} approximation of the target function in some space of candidate functions. The search process is often referred to as \emph{training}. During training, several candidates are evaluated until one that best approximates the target function is found. A program that implements this learning process is called \keyword{learning algorithm}.

The training process is driven by the data. Given a task, the learning algorithm has access to a \keyword{dataset} $\Data_n = \Set{\Pattern{z}{i}}_{i=1}^n$, a set of $n$ \gls{iid} samples drawn from some data domain $\Cal{Z}$ according to a fixed but unknown distribution $p(z)$. The nature of $\Cal{Z}$ depends on the learning paradigm. In the supervised case, we have $\Cal{Z} = \Cal{X} \times \Cal{Y}$, where $\Cal{X}$ is called \emph{input space} and $\Cal{Y}$ \emph{output space}. The elements $z \in \Cal{z}$ are pairs $(x, y)$, where $y$ is called \emph{label}. The output space $\Cal{Y}$ is tightly coupled to the task to be learned; for example, in classification tasks, $\Cal{Y}$ is a discrete set; in regression tasks, $\Cal{Y} = \Real$. In unsupervised learning, $\Cal{Z}$ is just the input space $\Cal{X}$. For this reason, data used in unsupervised tasks is often called \emph{unlabelled}. For the moment, we assume that the input space $\Cal{X}$ is the set of $d$-dimensional real-valued vectors $\Real^d$. We call elements $\Vector{x} \in \Real^d$  \emph{feature vectors}, and their elements \keyword{features}. Given a vector $\Vector{x}$, we use the notation $\Vector{x}_i$ to indicate its $i$-th feature. Later on, we shall generalize the notion of input space to more complex spaces than just vectors, to apply \gls{ml} to more complex objects.

The space of candidate functions explored by the learning algorithm is called \keyword{hypotheses space}. Formally, a hypotheses space is a set of functions $\HypSpace = \Set{h_{\omega} \mid \omega \in \Omega}$ whose members are called \emph{hypotheses}. The hypotheses space is used indirectly by the learning algorithm, which rather operates in \emph{parameter space} $\Omega$. The elements of the parameter space are called \emph{parameters}, and each parameter $\omega \in \Omega$ uniquely identifies a hypotheses $h_{\omega} \in \HypSpace$. Usually, the parameters $\omega$ correspond to a set of real-valued vectors. 

Besides the dataset, the learning algorithm is also given a \keyword{loss function} $L: \Omega \times \Cal{Z} \shortrightarrow \Real_+$. The role of the loss function is to provide a non-negative value to measure the error committed in approximating the unknown function $f$ with a hypothesis $h_{\omega}$. The objective of the learning algorithm is thus to find the optimal hypotheses $h_{\omega^*}$ whose error approximating $f$ is the lowest. This can be achieved by solving the following optimization problem:
$$h_{\omega^*} = \argmin_{\omega \in \Omega} R(h_{\omega}) \defeq \int \Loss(\omega, z)\, dz,$$
where $R$ is called \emph{risk functional}, or \keyword{generalization error}. However, the above objective function is intractable, since the learning algorithm only has access to the specific portion of $\Cal{Z}$ represented by the dataset $\Data_n$, randomly sampled from $p(z)$. Therefore, a tractable optimization problem is used instead:

$$h_{\hat{\omega}} = \argmin_{\omega \in \Omega} R_{\Data_n}(h_{\omega}) \defeq \frac{1}{n} \sum_{i=1}^n \Loss(\omega, \Pattern{z}{i}),$$
where $R_{\Data_n}$ is called \emph{empirical risk functional}, or
\keyword{training error}, and corresponds to selecting the hypothesis
whose average loss computed on the dataset is the lowest. We call the hypothesis $h_{\hat{\omega}}$ given as output by the learning algorithm a \keyword{model}. The learning principle corresponding to this optimization problem is called \gls{erm}. 

\subsection{Gradient-Based Optimization}
There exist several methods to optimize the learning objective; in this thesis, we focus on \emph{gradient-based} methods, which use the information provided by the gradient of the loss function (or an approximation thereof) to minimize the training error. Clearly, to apply gradient-based methods, the loss function has to be differentiable. By far, the most widely used gradient-based method in modern \gls{ml} is \gls{sgd}. Briefly, \gls{sgd} reduces the value of the loss function by \quotes{moving} the parameters in the direction of its negative gradient. The process requires to initialize the parameters to some randomly chosen value $\omega(0)$, and to iterate over the dataset several times. At each iteration $t$, the parameters are updated according to the following update rule:
$$ \omega(t+1) = \omega(t) - \eta_t \grad J(\omega(t)),\; t= 0, 1, \ldots,$$
where $\eta_t$ is a per-iteration \emph{learning rate} that controls the magnitude of the update, and:
$$ \grad J(\omega(t)) = \frac{1}{m} \grad_{\Param}  \sum_{i=1}^{n_B} \Loss(\omega(t), z^{(i)})$$
is an estimate of the true gradient of the loss function, averaged over a \emph{mini-batch} of $m \ll n$ data points. \gls{sgd} has several desirable properties as regards its convergence: specifically, given a convex loss function, if the learning rate is decreased at an appropriate rate, \gls{sgd} converges to its global minimum almost surely as $t \shortrightarrow \infty$; if the loss function is not convex, under the same assumptions \gls{sgd} converges to one local minimum almost surel. Over the years, different variants of \gls{sgd} have been developed; a program that implements a specific variant is called \keyword{optimizer}.

\subsection{Overfitting}
One desired property of \gls{erm} is that the empirical risk is guaranteed to approximate arbitrarily well the true risk as $n \shortrightarrow \infty$, provided that the hypotheses space has enough \keyword{capacity}. The capacity of a hypotheses space is the scope of functions it is able to learn: the higher it is, the more \quotes{complex} functions can be learned. However, if the capacity of the hypotheses space is unconstrained, one might incur in \keyword{overfitting}, a phenomenon where the learned model perfectly predicts the examples in the dataset (achieves very low training error), but is unable to generalize to unseen examples (has high generalization error). Intuitively, an overly-expressive hypotheses space is likely to contain hypotheses so complex, that are able to fit not only the true relationships in the data, but also the sampling noise in the dataset. Such hypotheses would be then wrongly chosen as the best hypotheses according to the \gls{erm} principle. Overfitting can be observed by dividing the dataset in two disjoint partitions: a \keyword{training set} which is used for learning, and a \keyword{test set}, which is held out from the training procedure and is used to get an estimate of the true generalization error. To detect overfitting, one should monitor if the generalization error (estimated in the test set) diverges from the training error (computed on the training set) during the learning process.

\subsection{Regularization}
Besides using a separate test set for detection, overfitting can be prevented \apriori using \keyword{regularization} techniques. The general idea of regularization is to limit, directly or indirectly, the \keyword{complexity} of the hypoteses space used by the learning algorithm. A principled form of regularization of the learning process derives from the field of \gls{slt}, which studies, among other problems, the relationship between the training error and the generalization error.  One important result of \gls{slt} is the so-called \keyword{generalization bound}, which states that, if the complexity $\Fun{C}(\HypSpace)$ of a hypotheses space is known\footnote{The complexity of a hypotheses space can be measured, among other techniques, calculating its \emph{VC-Dimension}, whose precise definition is beyond the scope of this work.}, the following inequality:

$$R(h_{\omega}) \leq R_{\Data_n}(h_{\omega}) + \eps(n, \Fun{C}(\HypSpace), \delta)$$
holds with probability of at least $1 - \delta$ if $n > \Fun{C}(\Cal{H})$. In other words, the generalization bound tells us that the generalization error is bounded by above by the training error and a \emph{confidence term} $\eps$, which depends on the number of examples $n$ and the complexity of $\HypSpace$. These two terms are tightly related: the confidence term can be decreased by getting more data (increasing $n$) if possible, or by restricting the complexity of the hypothesis space using regularization. This second choice, however, leads to an increase of the training error. The bound induces a simple principle to learn effectively while avoiding overfitting, called \gls{srm}, which requires minimizing both the training error and the confidence term.

\section{Model Evaluation}\label{sec:model-selection}
With the goal of generalization in mind, a learning algorithm needs to be evaluated on unseen data. The name \keyword{model evaluation}, or \emph{model assessment}, refers to the task of getting a proper estimate of generalization capability of the model. In model evaluation, one is not necessarily interested to evaluating the generalization error; in most practical cases, another performance metric is used. For example, in classification tasks, the \emph{accuracy} of the model is evaluated instead. The evaluation is performed on the test set; different estimators are defined by the different ways in which we can split the dataset into training and test partitions. The simplest strategy is the \keyword{hold-out} strategy, which splits the data into one training set and one test set, according to some predefined proportion. The parameters of the model are found using the training set, and the performance estimate is obtained from the test set. According to the field of statistics, an estimator can be decomposed in two related quantities: \emph{bias} and \emph{variance} (see \eg \cite{?} for a formal treatment of the subject). Informally, bias is related to how much the estimation is close to the true value; variance is related to how much the estimation depends on the specific dataset on which it is obtained. \quotes{Good} estimators trade-off between the two. For small datasets, the hold-out estimator has high variance, since it is obtained on a single test set and over-estimate the true performance just by chance. To reduce the variance of the estimation, $k$-fold \gls{cv} can be used instead. $k$-fold \gls{cv} consists in splitting the data in $k$ disjoint partitions and repeat the evaluation $k$ times. Each repetition is a hold-out estimation where one partition in turn acts as test set, and the remaining $k-1$ form the training set. The final estimator is the average of the $k$ hold-out estimators. In practical cases, $k=5$ or $k=10$ are often used. 

\subsection{Model Selection}
Learning algorithms are such that finding good values of the parameters is usually not enough. In fact, the learning process is also influenced by other settings, not directly optimizable by gradient descent in parameter space. These extra settings are called \keyword{hyper-parameters}. Some examples are the learning rate $\eta$ and the number of iterations of \gls{sgd}; other hyper-parameters are specific to the particular learning algorithm used. The process of jointly choosing the parameters and the hyper-parameters of a model is called \keyword{model selection}, or \emph{hyper-parameter tuning}. Model selection requires some a set of hyper-parameter configurations to be evaluated. Given a configuration, a model is instantiated with the corresponding hyper-parameters, trained on the training set and evaluated on some held-out dataset. Usually, the hyper-parameters set is specified as a \emph{grid}, where each hyper-parameter is associated to a discrete set of possible choices. When this is the case, model selection is referred to as \emph{grid search}, and the algorithm is simply an exhaustive evaluation of all possible hyper-parameter combinations. Other strategies for defining the set of hyper-parameters and the model selection algorithm are also possible \cite{?}. Model selection requires a separate set of data than the test set. In fact, if done on the test set, the set of hyper-parameters found would be tailored on that specific test set, and the corresponding performance would be an over-optimistic (biased) estimate of the true performance. In general, the test set cannot be used to take decisions about the learning process, regardless of whether the decision concerns the parameters or hyper-parameters of the model. This problem is solved by using one or more parts of the training set as a \keyword{validation set}, where the effect of the different hyper-parameters on the performances is estimated. In this sense, model selection can be viewed as a nested assessment inside the more broad model evaluation task, and can be tackled using the same kinds of estimators such as $k$-fold \gls{cv}. Later on in this thesis, we shall see some strategies of how model selection and evaluation are performed jointly in practical settings.

\section{Neural Networks}\label{sec:nn}
This work revolves around artificial \glspl{nn}, or \gls{nn} in short,
a learning algorithm inspired by the mechanisms through which the neurons
in our brain learn. In brief, biological neurons acquire electrical signal
from neurons they are connected to, and send it over to other neurons if
the strength of such signal exceeds a given threshold. Learning in this
context can be intended as the process that adjusts the \quotes{strength}
of the neural connections according to the signal provided in input, such
that a specific behaviour is obtained. For this reason, the neural approach
to \gls{ml} is often called \emph{connectionist}, borrowing this terminology
from the computational neuroscience field. Mathematically speaking, given an input signal $\Vector{x} \in \Real^d$, the general function performed by a neuron can be abstracted by the following:

$$y = g\Paren{\sum_{i=1}^d \Vector{w}_{i}\Vector{x}_i + b},$$
where $\Vector{w} \in \Real^d$ are the weighted connections, $b \in \Real$ is called bias term,
$g$ is the threshold function (usually called \keyword{activation function}),
and $y \in \Real$ is the output. A visual representation of the
artificial neuron is shown in Figure \ref{fig:neuron}.
\begin{figure}[t!]
    \centering
    \resizebox{.4\textwidth}{!}{\input{Figures/Chapter2/01-neuron.tex}}
    \caption{Schematics of a computational neuron.}
    \label{fig:neuron}
\end{figure}
\glspl{nn} typically implement hypotheses spaces consisting of a hierarchy
of function compositions to map an input $\Vector{x}$ to the network output:

$$g_{\Param}(\Vector{x}) = (g_{\Param_\mathrm{out}} \circ g_{\Param_L} \circ ... \circ g_{\Param_{\mathrm{2}}} \circ g_{\Param_{\mathrm{1}}})(\Vector{x}),$$
where $g_{\Param_{\mathrm{1}}}$ is called \keyword{input layer}, $g_{\Param_\mathrm{out}}$
is called \keyword{output layer}, $g_{\Param_l}$ for $l = 2, \ldots, L$ are called \keyword{hidden layers} and $\Param = (\Param_{out}, \ldots, \Param_{1})$ are the parameters or \keyword{weights} of the network. Intuitively, a layer is a group of output neurons
(also called \keyword{units}) attached to the same input neurons, as in the example of Figure \ref{fig:layer}. Each layer of the network computes the following function:
$$ g_{\Param_{l}}(\Vector{x}) = g_l(\Matrix{W}_l^{\Transpose}\Vector{x} + \Vector{b}_l),$$ where $\Param_l = (\Matrix{W}_l, \Vector{b}_l)$ are its weights. More specifically, if $g_{\Param_{l-1}}$ and $g_{\Param_{l}}$ are two consecutive layers with $d_{l-1}$ and $d_{l}$ units respectively,  $\Matrix{W}_{l}$ is a weight matrix in $\Real^{d_{l-1} \times d_{l}}$, and $\Vector{b}_{l}$ is a
bias vector in $\Real^{d_l}$. 
\begin{figure*}[h!]
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \resizebox{.8\textwidth}{!}{\input{Figures/Chapter2/02a-layer.tex}}
        \caption{}
        \label{fig:layer}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \resizebox{.8\textwidth}{!}{\input{Figures/Chapter2/02b-network.tex}}
        \caption{}
        \label{fig:network}
    \end{subfigure}
    \caption{(A) An example of a neural network layer. The superscript above the units indicates the layer they belong to. (B): A Multi-Layer Perceptron
    with $L=3$ layers (biases not shown). The arrows indicate the flow of forward propagation.}
\end{figure*}
The activation function $g_l$ of a hidden layer is an element-wise non-linearity, and its output is often called \emph{activation}. The most common non-linear activation functions for hidden layers are shown in Figure \ref{fig:activations}. To work with \gls{sgd}, they are usually differentiable. Two very common examples are the \emph{sigmoid} function, defined as:
$$\sigma(x) = \frac{1}{1 + e^{-x}},$$
which maps its input in the range $(0, 1) \subset \Real$, and the \emph{hyperbolic tangent} function, defined as:
$$\Fun{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}},$$
whose output range is $(-1, 1) \subset \Real$. In modern \glspl{nn}, the activation function of the hidden layers is generally some variant of the \gls{relu} function, which is defined as:
$$\Fun{ReLU}(x) = \max(0, x).$$
Notice that, even though the ReLU function is not differentiable at $x = 0$, it can be implemented to work in practical settings.
The activation function of the output layer is task-dependent, and is tightly
coupled with the loss function used to train the network, as we shall see in
Section \ref{sec:loss}.
\begin{figure*}[h!]
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \resizebox{.8\textwidth}{!}{\input{Figures/Chapter2/03a-sigmoid.tex}}
        \caption{Sigmoid}
        \label{fig:sigmoid}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \resizebox{.8\textwidth}{!}{\input{Figures/Chapter2/03b-tanh.tex}}
        \caption{Hyperbolic Tangent}
        \label{fig:hyptan}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \resizebox{.8\textwidth}{!}{\input{Figures/Chapter2/03c-relu.tex}}
        \caption{Rectified Linear Unit}
        \label{fig:relu}
    \end{subfigure}
    \caption{Examples of activation functions for the hidden layers (in solid black) and their derivatives (in dashed black).}
    \label{fig:activations}
\end{figure*}
The \emph{architecture} of a network is a specification of the number of layers
$L$, the dimensions of the parameters $\Param$ for each layer, and the
corresponding activation functions. In practical settings, the architecture is a hyper-parameter and needs to be found with model selection. Clearly, the only part of the architecture that is not a hyper-parameter is the dimension of the input layer, which depends on the data the network uses. A \gls{nn} with $L \geq 2$ layers (including the output layer), where $g_{l}$ is a non-linear activation function is called \gls{mlp}, and is the most common form of \gls{nn}.
\glspl{mlp} are extremely versatile learning algorithms: in fact, it has been proved that they are \emph{universal function approximators}, meaning that they can approximate any computable function with arbitrary precision, given an appropriate architecture \cite{?}. Figure \ref{fig:network} shows an example of \gls{mlp} with two hidden layers.

One distinctive advantage of \glspl{nn} with respect to other learning algorithms is \emph{automatic feature engineering}: in fact, the hidden layers of a \gls{nn} extract features from the data without an explicit guidance from the end user, nor from the training procedure. This means that the network itself decides which \keyword{representation} of the data is relevant to solve a given task by tuning the parameters during learning; this decision is driven by the high-level goal of approximating the input-output relationship of interest. This ability is crucial in tasks where knowledge is difficult to represent, or where there is not enough domain expertise to manually devise features, and has been the key of the success of \glspl{nn} in \gls{ml}.

\subsection{Training}\label{sec:training}
Learning in a \gls{nn} requires several passes through the training set. Each pass is called an \emph{epoch}, and consists of two phases. In the first phase, training data points are fed to the network and passed to each of its layers sequentially, such that the output of a layer becomes the input of the following layer. The output layer of the network produces a \emph{prediction}
for each data point. This phase is called \emph{forward propagation}, which is why \glspl{nn} are often called \keyword{feed-forward}. Subsequently, the error between the prediction and the ground truth (the labels $y$) is calculated via the loss function. The second phase consists in propagating the error back to the hidden layers to calculate the gradient of the loss function at each layer. This is done via the well-known \keyword{backpropagation} algorithm, which is basically an application of the chain rule of derivation for function composition. Once the gradient of the loss function is available at each layer, the parameters are updated using \gls{sgd}.
Training \glspl{nn} requires particular care to ensure the convergence of the \gls{sgd} algorithm. In particular, the initial values of the weight matrices should be initialized with small random values around 0 for \emph{simmetry breaking}, and the learning rate must be $< 1$, to prevent too large update steps which would make the objective function diverge from the local minima.

\subsection{Loss Functions}\label{sec:loss}
As anticipated, the choice of the output layer of a \gls{nn} determines
the kinds of tasks it is able to learn. In turn, each task is associated to a specific loss function that must be minimized by the optimizer, in order to find a set of parameters that generalize. \glspl{nn}
are usually trained with \gls{mle} under a parameterized conditional distribution of the outputs of the network. More specifically, focusing on supervised learning for the moment, assume the unknown function $f$ is some conditional $p(y\given \Vector{x}; \Param)$, which we approximate with a neural network $p_{\Param}(y \given \Vector{x}) = g_{\Param}(\Vector{x})$. According to the \gls{mle} principle, this can be achieved by training the network to maximize the log-likelihood of the data. Given a dataset $\Data_n = \Set{(\VPattern{x}{i}, \Pattern{y}{i})}_{i=1}^n$, this is equivalent to the following objective function:
$$\argmax_{\Param} \frac{1}{n} \sum_{i=1}^n \log p_{\Param}(\Pattern{y}{i} \given \VPattern{x}{i}).$$
Since \gls{sgd} optimizers usually work by minimizing functions, it is more common to minimize the negative log-likelihood instead. Notice that \gls{mle} is tightly connected to the \gls{erm} principle. In particular, \gls{mle} is equivalent to \gls{erm} when the loss function between the true conditional and the conditional learned by the network is the \gls{kld}. Given a random variable $x$ and two arbitrary distributions $p$ and $q$, the \gls{kld} is defined as follows:
$$\Fun{KLD}(p \;\Vert\; q) = \int_x p(x) \log\Paren{\frac{p(x)}{q(x)}} dx.$$
In practice, the \gls{kld} is a measure of discrepancy between distributions: it is asymmetric, meaning that $\Fun{KLD}(p \;\Vert\; q) \neq \Fun{KLD}(q \;\Vert\; p)$, and it evaluates to 0 if and only if $p = q$. Thus, minimizing the \gls{kld} under the \gls{erm} principle is equivalent to maximizing the likelihood under the \gls{mle} principle. Below, we detail about three of the most common output layers, and the loss functions by which the corresponding networks can be trained.

\paragraph{Linear Output Layer}
A linear output layer is used to solve regression tasks, \ie tasks where the output space is continuous. Following, we consider $y \in \Real$ for simplicity. A linear output layer is defined as:

$$g_{\theta_\mathrm{out}}(\Vector{h}) = \Matrix{W}_{\mathrm{out}}^{\Transpose}\Vector{h} + \Vector{b}_{\mathrm{out}},$$
where $\Vector{h}$ is the output of the previous layers of the network. Notice that, in this case, the activation function $g_{\mathrm{out}}$ is just the identity function. The model assumed for the true conditional is the following: 
$$p(y \given \Vector{x}) \approx \Normal{g_{\theta}(\Vector{x})}{\Matrix{I}},$$
where $\Matrix{I}$ is a unit covariance matrix. In other words, the true conditional is a Gaussian distribution with unknown mean and unit variance, and the network outputs are an estimation of the true mean. A well known result of statistics tells us that applying the \gls{mle} principle to this problem is equivalent to minimizing the \gls{mse} loss function:
$$\Loss(\Param, (\Vector{x}, y)) =\Fun{MSE}(\Vector{x}, y) \defeq \norm{g_{\Param}(\Vector{x}) - y}_2.$$ 
Notice that minimizing the \gls{mse} corresponds to minimizing the Euclidean distance between the target $y$ and the prediction of the network $g_{\Param}(\Vector{x})$. 
A single linear output layer coupled with the \gls{mse} loss function is commonly known in \gls{ml} literature as the Linear Regression model.

\paragraph{Logistic Output Layer}
A logistic output layer is used to solve binary classification tasks, \ie tasks where the output space is the discrete set $\Set{0, 1}$. The label 0 is usually called the \emph{negative} class, while the label 1 is called \emph{positive}. The layer is defined as follows:

$$g_{\theta_\mathrm{out}}(\Vector{h}) = \sigma \Paren{\Matrix{W}_{\mathrm{out}}^{\Transpose}\Vector{h} + \Vector{b}_{\mathrm{out}}},$$
where $g_{\mathrm{out}} = \sigma$ is the sigmoid  function.
Since the codomain of the sigmoid function is the real-valued interval $(0, 1) \subset \Real$, it is suitable to express output probabilities. To apply \gls{mle} to a binary classification task, the following model for the true conditional is assumed: 
$$p(y \given \Vector{x}) \approx \Fun{Bernoulli}(g_{\Param}(\Vector{x})),$$
where. In other words, the true conditional is a Bernoulli distribution whose parameter is estimated by the neural network. According to statistics, applying \gls{mle} to the assumed model is equivalent to minimizing the \gls{bce} loss function:
$$\Loss(\Param, (\Vector{x}, y)) = \Fun{BCE}(\Vector{x},y) \defeq y\, \log(g_{\Param}(\Vector{x})) + ( 1 - y) \log(1 - g_{\Param}(\Vector{x})).$$
Training a single logistic output layer with the \gls{bce} loss function is equivalent to training a Logistic Regression model.

\paragraph{Softmax Output Layer}
The softmax output layer is used in multi-class classification tasks, \ie tasks where the output space is a discrete set $\Cal{C} = \Set{c_1, \ldots, c_k}$ of $k$ mutually exclusive classes. Following, we assume the classes are represented as the integers from 1 to $k$ for notational convenience. The layer is defined as:

$$g_{\mathrm{out}}(\Vector{h}) = \tau \Paren{\Matrix{W}_{\mathrm{out}}^{\Transpose}\Vector{h} + \Vector{b}_{\mathrm{out}}},$$
where $\tau$ is the \emph{softmax} activation function, defined element-wise over a generic vector $\Vector{s} \in \Real^k$ as:

$$\tau(\Vector{s}_i) = \frac{e^{\Vector{s}_i}}{\sum_{j=1}^k e^{\Vector{s}_j}}.$$
In practice, a softmax layer outputs a score for each possible class, and the vector of scores is normalized to be a probability distribution by the softmax function. The model for the true conditional assumed in this case is the following:
$$p(y \given \Vector{x}) \approx \Fun{Multinoulli}_k(g_{\Param}(\Vector{x})).$$
In other words, the true conditional is a Multinoulli (categorical) distibution for the $k$ classes, whose parameter is estimated by the \gls{nn}. Applying the \gls{mle} principle to this problem is equivalent to minimizing the \gls{ce} loss function: 
$$\Loss(\Param, (\Vector{x}, y)) = \mathrm{CE}(\Vector{x}, y) \defeq - \sum_{i=1}^k \mathbb{I}[i=y] \log \Vector{o}_i,$$
where $\Vector{o} = g_{\Param}(\Vector{x})$ is the distribution outputted by the network, and $\mathbb{I}$ is the indicator function. Alternatively to the indicator function, a $k$-dimensional \emph{one-hot encoded} vector can be used, where all positions are zero except the position of the true class $y$, which is one. In words, the one-hot vector encodes the discrete probability distribution over the possible classes, where all the mass is put on the class corresponding to the label $y$. Notice that the \gls{bce} loss function is just a special case of \gls{ce} where $k = 2$. A \gls{nn} with one single softmax output trained with \gls{ce} is known in \gls{ml} literature as the Softmax Regression model.

\subsection{Regularization}\label{sec:regularization}
As we have briefly described in Section \ref{sec:learning}, regularizing a \gls{ml} model requires to somehow limit the complexity of the hypotheses space, such that the learning algorithm is more likely to select hypotheses that do not overfit the training data. For \glspl{nn}, one straightforward approach to limit complexity is to reduce the number of hidden layers, and the number of units in each layer. Other strategies are discussed below.

\paragraph{Penalized Loss Function}
One very general regularization technique, which is not restricted to \glspl{nn}, is to impose a \emph{preference bias} to the possible values the weights might take, such that configurations that generalize achieve a lower training error than configurations that overfit. Focusing again on the supervised case, this can be achieved by augmenting the training ojective with a \emph{penalty term} as follows:
$$\argmin_{\Param} \frac{1}{n} \sum_{i=1}^n \Loss(\Param, (\Vector{x}, y)) + \lambda\norm{\Param}_p,$$
where $\norm{.}_p$ indicates a generic $p$-norm. The norm of the parameters implement the preference criteria used to avoid overfitting, whose influence on the training objective is controlled by a hyper-parameter $\lambda$. Depending on the value of $p$, we distinguish:
\begin{itemize}
    \item $L1$ regularization, where $p=1$. The bias of $L1$ regularization is to push the values of some weights to zero. This corresponds to training a \gls{nn} with less parameters (parameters set to 0 do not contribute to the loss function);
    \item $L2$ regularization, where $p=2$. The bias induced by the $L2$ norm is to penalize large values of the weights. Intuitively, restricting the range of the values that the weights can take limits the type of functions the network can approximate.
\end{itemize}
In \gls{ml} literature, there exist several other types of penalties: for example, ElasticNet regularization combines $L1$ and $L2$ regularization together.

\paragraph{Early Stopping}
Early stopping is another general regularization scheme, which is widely adopted in \glspl{nn} training. In short, it requires monitoring a performance metric (which can be the value of the loss function or other task-specific metrics) on the validation set, in order to stop the learning process as soon as overfitting is detected. Once learning has stopped, the parameters that yielded the best score according to the metric are chosen by the learning algorithm. Intuitively, early stopping implicitly biases the hypotheses space by limiting the number of training iterations, which in turn limits the number of hypotheses evaluated during training. 

\paragraph{Dropout}
Dropout is a regularization technique that is specific to \glspl{nn}. The idea behind dropout is to use a very expressive network for training, whose capacity is constrained stochastically at each iteration of the learning procedure. At each pass through a batch of training data, dropout \quotes{turns off} some of the units in a layer by multiplying their output with a binary vector mask, whose entries are samples from a Bernoulli distribution with hyper-parameter $p_{\mathrm{keep}}$, called \emph{dropout rate}. This has a double effect: first, a smaller number of parameters is used to compute a prediction (because some of them are made ineffective by the binary mask); second, the parameters used by the network are different at each pass (because of the stochasticity of the mask). According to this second implication, dropout implements a dynamic form of \emph{model ensembling}, a \gls{ml} technique in which one combines several low-capacity model to obtain a stronger predictor.

\section{Auto-Encoders}
An \gls{ae} is a \gls{nn} architecture for unsupervised learning. \glspl{ae} are trained to reconstruct their inputs by jointly learning two mappings, one from the input space to a \keyword{latent space}, and another from the latent space back to the input space. During training, the latent space learns general features about the input that help the reconstruction. Given a $d$-dimensional input vector $\Vector{x} \in \Real^d$, the general form of an \gls{ae} is defined as follows:
\begin{align*}
    \Vector{h} &= g_{\Param_{\Fun{enc}}}(\Vector{x})\\
    \Vector{r} &= g_{\Param_{\Fun{dec}}}(\Vector{h}),
\end{align*}
where $\Vector{h} \in \Real^h$ is an $h$-dimensional \emph{latent code}, $\Vector{r} \in \Real^d$ is a \emph{reconstruction} of the input, $g_{\Param_{\Fun{enc}}}$ is an encoding \gls{nn} or \keyword{encoder}, and $g_{\Param_{\Fun{dec}}}$ is a decoding \gls{nn} or \keyword{decoder}. Despite being an unsupervised model, \glspl{ae} can be trained in a supervised way, by implicitly using a dataset of the form $\Data_n = \Set{(\VPattern{x}{i}, \VPattern{x}{i})}_{i=1}^n$, \ie one where the input itself is the target. The loss function of an \gls{ae} is called \emph{reconstruction loss}, and measures some form of distance between the input and the reconstruction (the output of the decoder) as shown in Figure \ref{fig:autoencoder}; generally, it can be the \gls{mse} for continuous inputs, or the \gls{ce} for discrete inputs.

\begin{figure*}[h!]
    \centering
    \resizebox{.45\textwidth}{!}{\input{Figures/Chapter2/04-autoencoder}}
    \caption{An Auto-Encoder.}
    \label{fig:autoencoder}
\end{figure*}
If the latent space of the \gls{ae} is not constrained in any way, it learns to just copy its input to the output: in fact, one can always have $\Vector{x} = \Vector{r}$ everywhere by simply imposing that $g_{\Param_{\Fun{enc}}} \circ g_{\Param_{\Fun{dec}}}$ is the identity function. This is not very useful in practice, as a network structured in this way would not learn anything useful about the data distribution; thus, many forms of structuring or constraining the latent space have been studied. Perhaps the simplest form of \gls{ae} is the undercomplete \gls{ae}, \ie one where $h < d$. In this case, the latent space acts like a \emph{bottleneck} that is trained to retain relevant features of the input, while discarding irrelevant ones.
A useful byproduct of training an undercomplete \gls{ae} is that the latent space acts as a \emph{manifold}, \ie a lower-dimensional subspace with Euclidean properties where data points are projected onto. One limitation of this kind of architecture is that one has to choose the capacity of the encoder and decoder very carefully. As an extreme case, consider that an over-capacitated \gls{ae} with a 1-D latent space could learn to map each data point to a different integer. Clearly, this mapping would be uninformative with respect to the true data distribution. 

\subsection{Regularized Auto-Encoders}
Regularized \glspl{ae} generally use $h \geq d$, but impose constraints on the representations learned by the latent space through penalties on the loss function. Specifically, a regularized \gls{ae} optimizes the following general objective function:

$$\argmin_{\Param} \Loss(\Param, (\Vector{x}, \Vector{r})) + \lambda \Psi(\Vector{h}),$$
where different choices of $\Psi$ define different variants. For example, \emph{sparse} AEs  
are trained in such a way that the latent space produces sparse representations, meaning that only certain units are active for certain data inputs. This can be accomplished by constraining the mean activation of the units to be small. Specifically, in a sparse \gls{ae}, if $\rho$ is the average activation, the penalty is formulated as:
$$\Psi(\Vector{h}) = \sum_{i=1}^k \rho \log \frac{\rho}{\hat{\rho}_i} (1 - \rho) \log \frac{1-\rho}{1-\hat{\rho}_i},$$
where $\hat{\rho}_j$ is the activation of the $i$-th hidden unit. If $\rho$ is chosen to be small, the penalty constrains most hidden units to have zero activation.
Similarly, \emph{contractive} \glspl{ae} regularize the objective function by imposing a penalty on the gradients of the hidden units with respect to the input. More in detail, the penalty term of a contractive \gls{ae} is the following:
$$\Psi(\Vector{h}) = \sum_{i=1}^k\norm{\grad_{\Vector{x}}\Vector{h}_i}_2,$$
which intuitively corresponds to penalizing hidden activations that have large variation for small variations in the input. Thus, the local structure of the latent space is forced to be similar to that of the input space. Differently from the other variants, \emph{denoising} \glspl{ae} achieve regularization acting on the training procedure, rather than via a penalty term. In a denoising \gls{ae}, the input $\Vector{x}$ is corrupted before being passed to the network (usually through Gaussian noise addition). Thus, the network uses the corrupted version $\tilde{\Vector{x}}$ during forward propagation, while the loss is calculated on the original input. By being trained to remove the noise from the input, the network is forced to learn meaningful patterns. The forward propagation phase of a denoising \gls{ae} is shown in Figure \ref{fig:denoising-ae}.
\begin{figure*}[h!]
    \centering
    \resizebox{.6\textwidth}{!}{\input{Figures/Chapter2/05-denoising-ae}}
    \caption{A denoising Auto-Encoder. The dashed arrow indicates the corruption process which transforms the input $\Vector{x}$ into a noisy version $\tilde{\Vector{x}}$, which is not directly part of the forward propagation.}
    \label{fig:denoising-ae}
\end{figure*}

\section{Deep Learning}
In Section \ref{sec:nn}, we have talked about the representational power of \glspl{nn}. However, the way this representational power is attained is greatly influenced by the \emph{depth} of the network, \ie its number of layers. 
When a \gls{nn} is \quotes{deep}, the hidden features are organized during learning in a hierarchy, in which simpler features of the early layers are progressively combined into very sophisticated features in subsequent layers. Although \quotes{shallow} networks (networks with very few but large hidden layers) have their same approximating capabilities, deep networks combine features in a more computationally efficient way \cite{?}. This data-driven \emph{representational bias} has proven effective in many practical domains, such as computer vision \cite{?} and \gls{nlp} \cite{?}, establishing the success of deep \glspl{nn} in many \gls{ml} tasks. The term \gls{dl}, coined around 2006, is used to refer to \gls{nn} architectures composed of a large number of hidden layers, usually $\geq 3$. The representational power of deep networks comes in hand with a number of computational challenges that arise from their depth. Below, we summarize the most well-known:
\begin{itemize}
    \item the loss functions used for training deep networks are extremely complicated and non-convex, since the hidden layers have non-linear activation functions. This means that gradient-based methods may get stuck in bad local minima;
    \item very deep networks suffer from \emph{vanishing} or \emph{exploding gradient} issues, which can prevent the network from learning, or make the optimization numerically unstable, respectively;
    \item training deep \glspl{nn} requires large datasets and an extensive amount of computational power.
\end{itemize}
These three major issues have been the focus of basic research on deep networks in the last years. As regards the first challenge, these problems have been tackled by better characterizing the properties of the loss function surface \cite{?}. In parallel, improvements have been achieved by devising more effective optimizers \cite{?} and more easily optimizable activation functions \cite{?}. As regards the second challenge, several mechanisms to prevent the two undesirable phenomena from happening have been proposed and applied with success; the most known are gradient clipping \cite{?} to prevent gradient explosion, batch normalization \cite{?} and residual connections \cite{?} to prevent gradient vanishing. As for the third challenge, major contributions came from the advent of the big data revolution, which ensured large and progressively more curated data sources, and the use of \gls{gpu} vectorization, automatic differentiation and computational graphs to speed up the training and inference processes of deep networks.

\subsection{Convolutional Neural Networks}
A \gls{cnn} is a kind of \gls{nn} born in the field of computer vision. The development of \glspl{cnn} started long before the \quotes{deep learning renaissance} in 2006, but they only recently became popular, after demonstrating their effectiveness in several image recognition tasks. Given their historical importance in the landscape of \gls{dl}, we shall present them briefly even though they are not part of this thesis. The standard hidden layer of a \gls{cnn} is called \emph{convolutional layer}, and it is able to process 2-dimensional data such as images. It does so with parameterized \emph{filters} which are applied to an input through a convolution operation. The output of a filter is usually called \emph{feature map}. More in detail, a 2-D convolutional filter computes computes a feature map where each entry is defined as follows:
$$\Matrix{F}(i, j) = (\Matrix{M} * \Matrix{K})(i, j) = \sum_{i=1}^{n} \sum_{j=1}^{m} \Matrix{M}(i+s, j+t)\Matrix{K}(s,t).$$
In the above, $\Matrix{M} \in \Real^{n \times m}$ is a 2-D matrix (for example, an image with width $n$ and height $m$), $\Matrix{K} \in \Real^{s \times t}$ is a learnable filter, or \emph{kernel}, with width $s \ll n$ and height $t \ll m$, and $*$ is a convolutional operator. In practice, the filter is \quotes{slid} on the input row-wise. The values in the feature map are higher if parts of the input match the filter, or more intuitively, if the pattern of the filter is detected (to some extent) in the input image. In turn, this implies that each feature of the feature map shares the same weights (those of the kernel). This approach is called \emph{weight sharing}, and it implements \emph{translational invariance}, \ie features are detected regardless of their position in the input. Another essential hidden layer of a \gls{cnn} is a \emph{pooling layer}. Pooling works by dividing the input in non-overlapping regions, and computing an aggregation function (usually a max) for each region. Pooling serves a double purpose: firstly, it reduces the number of weights needed in subsequent layers, thus maintaining computational tractability as the depth of the network grows; secondly, it acts as a regularizer, as it discards the specific information of the region where it is applied, picking up only one representative pattern. Convolutional and pooling layers are applied sequentially to the input data, followed by a standard hidden layer activation function, usually a ReLU. The triple (Convolution-Pooling-ReLU) is the standard building block of convolutional architectures. The last layers of a \gls{cnn} usually consist of a standard \gls{mlp} (or a single output layer), which uses the final representation computed by the convolutional blocks as input, and compute the corresponding task-dependent output. Even though they were born within the computer vision field, \glspl{cnn} have been generalized to 1-D inputs (such as sound waves in \emph{speech recognition} tasks) and 3-D inputs (such as video frames in \emph{object tracking} tasks).

\section{Deep Generative Models}
Many \gls{ml} tasks require to learn the underlying probability distribution of the data. This is an inherently unsupervised problem, since to know this distribution, the model must capture the underlying structure of the data space, such as regions of data points with high probability. The two main operations that knowing the data distribution enables are:
\begin{itemize}
    \item \keyword{inference}, that is, computing the density of arbitrary data points;
    \item \keyword{sampling}, that is, generating new data by drawing samples from the learned distribution. 
\end{itemize}
Broadly speaking, a \gls{dgm} is a model that uses deep \glspl{nn} to approximate a probability distribution over random variables. In the case of learning the data distribution, the random variable is a training set $\Data_n = \Set{\VPattern{x}{i}}_{i=1}^n$. \glspl{dgm} are trained to minimize the \gls{kld} between the true data distribution $\PData$ and a model $p_{\Param}$, a distribution parameterized by a deep \gls{nn}:
$$\argmin_{\Param} \Fun{KLD}(\PData \;\Vert\; p_{\Param}) \defeq \Expect_{\Vector{x} \sim \PData}\Brack{\log \PData(\Vector{x}) -\log p_{\Param}(\Vector{x})}.$$
The \gls{kld} is a measure of discrepancy between distributions; it is asymmetric, meaning that $\Fun{KLD}(\PData \;\Vert\; p_{\Param}) \neq \Fun{KLD}(p_{\Param} \;\Vert\; \PData)$, and it evaluates to 0 if and ony if $\PData = p_{\Param}$. Since the term $\log \PData(\Vector{x})$ does not depend on the parameters $\Param$, we can equivalently optimize the following quantity, which has the same minimizer:
$$\argmin_{\Param} \Expect_{\Vector{x} \sim \PData}\Brack{-\log p_{\Param}(\Vector{x})}.$$
In practice, using \gls{mle} on the dataset $\Data_n$, \glspl{dgm} minimize the negative log-likelihood of the observed data using \gls{sgd}:
$$\argmin_{\Param} \frac{1}{n} \sum_{\Vector{x} \in \Data_n}-\log p_{\Param}(\Vector{x}).$$
The above objective can be minimized explicitly or implicitly. We distinguish two main categories of \glspl{dgm} based on this observation:
\begin{itemize}
    \item \emph{explicit} \glspl{dgm} minimize the log-likelihood directly. Thus, they can be used straightforwardly for inference and sampling;
    \item \emph{implicit} \glspl{dgm} do not minimize a log-likelihood, but a surrogate objective function that allows sampling from the data distribution as a byproduct.
\end{itemize}
In this thesis, we only deal with two specific kinds of explicit \glspl{dgm}, which are described in the following. Other explicit \glspl{dgm} include Sigmoid Belief Networks \cite{} and Generative Flows \cite{}; among implicit \glspl{dgm}, we mention Generative Adversarial Networks \cite{} and Generative Stochastic Networks \cite{}.

\subsection{Autoregressive Models}\label{sec:autoregressive}
An \gls{ar} model is an explicit \gls{dgm} where the training objective is decomposed in a product of conditionals according the chain rule of probability:
$$-\log p_{\Param}(\Vector{x}) = -\log\Paren{\prod_i p_{\Param_i}(x_i \given x_{<i})} = \sum_i -\log p_{\Param_i}(x_i \given x_{<i}),$$
where $x_i$ are random variables, and $x_{<i} = \Seq{x_1, x_2, \ldots, x_{i-1}}$. Basically, each conditional is approximated by a deep \gls{nn} $p_{\Param_i}$ that takes as input $x_{<i}$ (which for now we assume to be a fixed-size vector for simplicity) and outputs a distribution over $x_i$. Inference in \gls{ar} models is achieved by forward propagating the input through each network in the order established by the decomposition, and summing up the intermediate probabilities. To generate a data point according to the learned distribution, it is sufficient to sample each conditional in the same order. One problem with the \gls{ar} formulation is that it requires one deep network for each conditional, which is very likely to overfit the training data. This problem can be dealt with in two ways. The first is to use one single network  $p_{\Param}$ with shared parameters to learn all the conditionals; these approach is used by models such as recurrent neural networks, which we shall describe in detail in Section \ref{sec:rnns}. Another strategy, which we do not cover this work, is to use \emph{masking} strategies, either coupled with \glspl{ae} \cite{?} or with convolutional layers \cite{?}, to constrain the outputs of the network to follow the order of the conditional decomposition.

\subsection{Variational Auto-Encoders}
A \gls{vae} is a \emph{latent variable model} where a set of latent variables $\Vector{z} \in \Real^z$, also called explaining factors, is incorporated to the data distribution by marginalization as follows:
$$\PData(\Vector{x}) = \int p(\Vector{x} \given \Vector{z})\,p(\Vector{z})\, d\Vector{z}.$$
In the above formula, $p(\Vector{x} \given \Vector{z})$ is a \emph{decoding distribution} and $p(\Vector{z})$ is a prior over the latent variables, usually chosen to be a tractable distribution such as a Gaussian. The generative process expressed by a latent variable model consists in sampling from the prior a \quotes{specification} of the data, provided by the latent variables, which is used to condition the decoding distribution. Since the latent variables are not known in general, \glspl{vae} introduce an \emph{encoding distribution} $q(\Vector{z} \given \Vector{x})$ to produce latent variables given a data point. Instead of the data log-likelihood, \glspl{vae} work with a related quantity, called \gls{elbo} of the true log-likelihood, defined as follows:

$$\Fun{ELBO}(\Vector{x}) = \Expect_{\Vector{z} \sim q(\Vector{z} \given \Vector{x})} \log p(\Vector{x} \given \Vector{z}) - \Fun{KLD}(q(\Vector{z} \given \Vector{x}) \;\Vert\; p(\Vector{z})),$$
and such that $\log \PData(\Vector{x}) \geq \Fun{ELBO}(\Vector{x})$. By maximizing the \gls{elbo}, the true log-likelihood of the data can be recovered. Intuitively, this can be achieved by maximizing the expected log-likelihood of the decoding distribution $p(\Vector{x} \given \Vector{z})$ under the encoding distribution (the first term), while making the encoding distribution $q(\Vector{z} \given \Vector{x})$ close to the prior distribution $p(\Vector{z})$ at the same time (as specified by the \gls{kld} term). In practice, $p(\Vector{z})$ is chosen to be a standard Gaussian $\Normal{0}{\Matrix{i}}$ with unit covariance matrix $\Matrix{i}$, and $q(\Vector{z} \given \Vector{x})$ is a Gaussian distribution
$q_{\Param_{\mathrm{enc}}}(\Vector{z} \given \Vector{x}) = \Normal{\mu_{\Param_{\mathrm{enc}}}(\Vector{x})}{\sigma^2_{\Param_{\mathrm{enc}}}(\Vector{x})}$, whose parameters are approximated by a deep \gls{nn} with parameters $\Param_{\mathrm{enc}}$. This choice of distributions ensures that the \gls{kld} term of the \gls{elbo} can be calculated in closed form. The decoding distribution $p(\Vector{x} \given \Vector{z})$ is implemented as another deep \gls{nn} with parameters $\Param_{\mathrm{dec}}$. The loss function minimized by a \gls{vae} is thus the following:
$$\Loss(\Param, \Vector{x}) = \Expect_{\Vector{z} \sim q_{\Param_{\mathrm{enc}}}(\Vector{z} \given \Vector{x})} - \log p_{\Param_{\mathrm{dec}}}(\Vector{x} \given \Vector{z}) + \Fun{KLD}(q_{\Param_{\mathrm{enc}}}(\Vector{z} \given \Vector{x}) \;\Vert\; \Normal{0}{\Matrix{i}}),$$
where the first term is a reconstruction loss, and the second term regularizes the encoding distribution by making it similar to the prior. The forward propagation of a \gls{vae} is specified as follows: first, the input $\Vector{x}$ is mapped to the mean and variance of the encoding distribution by the encoder network. The two parameters are used to sample a latent vector $\Vector{z}$. This is turn is given to the decoder network, which outputs a reconstruction $\Vector{r}$. One major issue with this formulation is that this model cannot be trained with \gls{sgd}, since the gradient of a stochastic operation (the sampling from the encoder distribution) is not defined. Thus, the sampling process is reparameterized as $\Normal{\mu_{\Param_{\mathrm{enc}}}(\Vector{x})}{\sigma^2_{\Param_{\mathrm{enc}}}(\Vector{x})} = \mu_{\Param_{\mathrm{enc}}}(\Vector{x}) + \boldsymbol{\eps}\sigma^2_{\Param_{\mathrm{enc}}}(\Vector{x})$, with $\boldsymbol{\eps} \sim \Normal{0}{\Matrix{i}}$. This way, the stochastic operation is independent of the input, and gradient backpropagation through the encoder becomes deterministic. The reparameterized model is shown in Figure \ref{fig:vae}.
\begin{figure*}[h!]
    \centering
    \resizebox{.75\textwidth}{!}{\input{Figures/Chapter2/06-vae}}
    \caption{A Variational Autoencoder.}
    \label{fig:vae}
\end{figure*}
The \gls{vae} has several interesting properties: firstly, the latent space of a trained \gls{vae} is approximately normally distributed with 0 mean and unit variance. This means that it is compact and smooth around the mean, which in turn enables the user to seamlessly interpolate between latent representations. Secondly, it allows two possible generative modalities: an unconstrained one, which can be achieved by discarding the encoder network and starting the generative process by sampling from the prior $\Normal{0}{\Matrix{I}}$; and a conditional one, which is obtained by running an input $\Vector{x}$ through the entire network. This last modality is generative in the sense that the network outputs a variation (not an identical copy) of the input, due to the stochasticity induced by sampling the learned encoding distribution.